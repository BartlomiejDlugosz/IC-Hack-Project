Title: Data science
Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. 
Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.
Data science is "a concept to unify statistics, data analysis, informatics, and their related methods" to "understand and analyze actual phenomena" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a "fourth paradigm" of science (empirical, theoretical, computational, and now data-driven) and asserted that "everything about science is changing because of the impact of information technology" and the data deluge.
A data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.

Foundations
Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.
Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science. Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.

Etymology
Early usage
In 1962, John Tukey described a field he called "data analysis", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term "data science" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.
The term "data science" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.

Modern usage
In 2012, technologists Thomas H. Davenport and DJ Patil declared "Data Scientist: The Sexiest Job of the 21st Century", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that "the job is more in demand than ever with employers".
The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.
The professional title of "data scientist" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report "Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century", it referred broadly to any key role in managing a digital data collection.

Data science and data analysis
Data analysis typically involves working with structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning and data visualization to summarize data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data.
Data science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models. Data science often uses statistical analysis, data preprocessing, and supervised learning.

Cloud computing for data science
Cloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.
Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.

Ethical consideration in data science
Data science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts 
Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.

See also
Python (programming language)
R (programming language)
Data engineering
Big data
Machine learning
Bioinformatics
Astroinformatics
Topological data analysis


== References ==

Title: Jeph Acheampong
Jeph Acheampong is a Ghanaian-American businessman.
He is an Expo Live Global Innovator, a World Economic Forum Global Shaper, and was named a Future of Ghana Pioneer.

Early life and education
Acheampong was born in Accra, Ghana and grew up in New York. He graduated high school ini 2012 from Forest Hills High School (New York). Acheampong went on to study economics at New York University. Later, he pursued a masters at Harvard University.

Career
While at New York University, Acheampong founded Anansi Global, a fashion company. In 2018, he founded Blossom Academy, Ghana's first Data science academy. He also cofounded Blossom Corporate Training to upskill working professionals.
In 2016, He was awarded the President Service Award. In September 2021, he was featured on CNN in What the Big idea program and was recognized by Nigeria newspapers as 5 Most Influential People making impacts.
In 2022, the Government of Ghana appointing Jeph Acheampong to support the development of the Ghana National Artificial Intelligence Strategy 2023-2033
. He was invited as a speaker by West Africa Business Forum for youths and young women entrepreneurs from 15 countries across Africa, and also at the 2024 World Youth Development Forum in Guangzhou, China.
He is an advocate for systemic change that addresses the root causes of poverty and unemployment and in 2025, He was listed among the top ten Under-40 African Ivy League Graduates making an impact in Africa.

Awards and recognition
He is a 2018 Princeton in Africa Fellow, a 2023 Acumen Fellow, and a 2024 Cheng Fellow at the Harvard Kennedy School.


== References ==

Title: Art Recognition
Art Recognition is a technology company headquartered in Adliswil, within the Zurich metropolitan area, Switzerland. Specializing in the application of artificial intelligence (AI) for the purposes of art authentication and the detection of art forgeries, Art Recognition integrates advanced algorithms and computer vision technology. The company's operations extend globally, with the primary aim as a joint-stock-company to increase profit for its shareholders.

History
Art Recognition was established in 2019 by Carina Popovici and Christiane Hoppe-Oehl. The foundation of the company was driven by the long-standing challenge in the art world of authenticating paintings, a process traditionally reliant on expert judgment, historical research, and scientific analysis. Recognizing the limitations of existing methods, the co-founders were motivated by technological advancements in digital imaging and pattern recognition algorithms in the field of art.
These technological advancements, particularly in the realm of high-resolution digital imagery, enable a more detailed examination of artworks. By analyzing brushstrokes, signature patterns, and other distinct characteristics, and comparing them with known works by the same artist, digital tools offer a new dimension in authentication. Popovici and Hoppe-Oehl aimed to develop an advanced algorithm that could further assist experts by identifying stylistic elements and patterns unique to individual artists, thus aiding in the art authentication process.

Technology and methodology
Art Recognition employs a combination of machine learning techniques, computer vision algorithms, and deep neural networks to assess the authenticity of artworks. The AI algorithm analyzes various visual characteristics, such as brushstrokes, color palette, texture, and composition, to identify patterns and similarities with known authentic artworks.
The company's technology undergoes a process of data collection, dataset preparation, and training. In the initial phase, datasets are compiled, and data selection is supervised by art historians to ensure the inclusion of genuine artworks by specific artists. This approach aims to avoid including artworks that may have been partially completed by apprentices or contain mixed authorship.
Upon the preparation of datasets, a segment of the image set is used for training the AI algorithm, while the remaining images are set aside for testing. This phase aims to ensure the algorithm's proficiency in distinguishing authentic artworks from forgeries. Post-training, the algorithm undergoes evaluation with the test data, assessing its accuracy and efficacy in authenticating artworks.
After the testing phase, the AI algorithm is applied to analyze new images, including submissions from clients. Additionally, the algorithm is designed to identify artworks generated by generative AI, mimicking the style of renowned artists. This capability equips the algorithm to withstand adversarial attacks, enhancing its reliability in differentiating between authentic and artificially generated fake art pieces.

Academic partnerships and grants
Art Recognition's collaboration with Tilburg University in the Netherlands has resulted in the acquisition of a research grant from Eurostars,Eureka (organisation) the Eureka's flagship small and medium-sized enterprises (SME) funding program. In addition, the company has formed a partnership with the University of Liverpool in the United Kingdom, which has been supported by the Science and Technology Facilities Council (STFC) Impact Acceleration Award. Furthermore, Art Recognition has established a relationship with Innosuisse, a Swiss innovation agency, to expand its research and development initiatives.

Recognition and impact
Art Recognition's AI algorithm has received attention from various media outlets and industry events. The company was featured on the front page of The Wall Street Journal for its involvement in the authentication case of the Flaget Madonna, believed to have been partly painted by Raphael.
A broadcast by the Swiss public television SRF showcased how the algorithm can be used to detect art forgeries with high accuracy. Additionally, the company's work was featured in a TEDx talk discussing the use of AI in art authentication.

Debates and discussions
The technology developed by Art Recognition has been recognized for its role in providing a technology-based art authentication solution, compared to traditional methods. This advancement is seen as significant in the field of art verification, offering a modern approach to a historically complex process.
The use of AI in art authentication, as pioneered by Art Recognition, has become a topic of professional discourse. Notably, this subject was the focus of a debate on Radio Télévision Suisse, where experts deliberated over the capabilities and limitations of AI in identifying art forgeries. Such discussions highlight the evolving landscape of art authentication in the age of digital technology.
Despite the advancements in AI-driven art authentication, the field continues to face unique challenges, particularly regarding the acceptance of such technologies. Experts in the field stress the necessity of using AI as a complementary tool alongside traditional methods, rather than as a stand-alone or definitive solution for authenticating art.

Controversial cases
Art Recognition's AI algorithm has been applied to several high-profile and controversial artworks, sparking significant interest and debate in the art world.

Samson and Delilah at the National Gallery in London: The National Gallery's "Samson and Delilah", traditionally attributed to the artist Rubens, has also been examined using Art Recognition's AI, which has assessed the painting as non-authentic. This analysis contributed to ongoing scholarly discussions regarding the work's authenticity.
De Brecy Tondo Madonna. A research team from Bradford University and Nottingham University initially attributed the painting to Raphael, employing an AI face recognition software, while the AI developed at Art Recognition returned a negative result.  ***Redacted copyright violation***
Lucian Freud Painting Controversy: Featured in The New Yorker, a painting attributed to Lucian Freud became a subject of dispute. Art Recognition's AI analysis played a pivotal role in examining the painting's authenticity, contributing to the broader discussion about the challenges in verifying modern artworks.
Titian at Kunsthaus Zürich: A painting attributed to Titian, housed at Kunsthaus Zürich, has been a topic of debate among art experts. The application of Art Recognition's technology offered a new perspective, utilizing AI to analyze the painting's stylistic elements in comparison with authenticated works of Titian. Following this debate, Kunsthaus Zürich has announced plans to initiate a comprehensive project aimed at resolving the authenticity questions surrounding the painting. This project is set to involve collaboration with scientists and technology companies, leveraging a multidisciplinary approach to authenticate the artwork.
In each of these instances, Art Recognition's involvement has provided additional perspectives through AI analysis while contributing to broader conversations about the role of technology in art authentication. These cases demonstrate the evolving nature of art verification, where traditional methods are being supplemented, and sometimes challenged, by new technological approaches. However, they also underline the ongoing debates about the acceptance of AI in the field of art history, especially in the authentication of works by renowned artists.

References
External links
Official website
Art magazine website

Title: Artificial intelligence
Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.
High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."
Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.
Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.

Goals
The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.

Reasoning and problem-solving
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.
Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.

Knowledge representation
Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining "interesting" and actionable inferences from large databases), and other areas.
A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.
Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.

Planning and decision-making
An "agent" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the "utility") that measures how much the agent prefers it. For each possible action, it can calculate the "expected utility": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.
In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is "unknown" or "unobservable") and it may not know for certain what will happen after each possible action (it is not "deterministic"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.
In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.
Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.

Learning
Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.

There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).
In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.

Natural language processing
Natural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.
Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called "micro-worlds" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or "GPT") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.

Perception
Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.
The field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception.

Social intelligence
Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.

General intelligence
A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.

Techniques
AI research uses a wide variety of techniques to accomplish the goals above.

Search and optimization
AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.

State space search
State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.
Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. "Heuristics" or "rules of thumb" can help prioritize choices that are more likely to reach a goal.
Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.

Local search
Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.
Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.
Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by "mutating" and "recombining" them, selecting only the fittest to survive each generation.
Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).

Logic
Formal logic is used for reasoning and knowledge representation.
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as "and", "or", "not" and "implies") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as "Every X is a Y" and "There are some Xs that are Ys").
Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.
Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.
Fuzzy logic assigns a "degree of truth" between 0 and 1. It can therefore handle propositions that are vague and partially true.
Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.

Probabilistic methods for uncertain reasoning
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.
Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).

Classifiers and statistical learning methods
The simplest AI applications can be divided into two types: classifiers (e.g., "if shiny then diamond"), on one hand, and controllers (e.g., "if diamond then pick up"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an "observation") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.
There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the "most widely used learner" at Google, due in part to its scalability.
Neural networks are also used as classifiers.

Artificial neural networks
An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.
Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.
In feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. Perceptrons use only a single layer of neurons; deep learning uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are "close" to each other—this is especially important in image processing, where a local set of neurons must identify an "edge" before the network can identify an object.

Deep learning
Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.
Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.

GPT
Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called "hallucinations", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.
Current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.

Hardware and software
In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.
The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.

Applications
AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).

Health and medicine
The application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.
For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.

Games
Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.

Mathematics
Large language models, such as GPT-4, Gemini, Claude, LLaMa or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.
Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry and AlphaProof all from Google DeepMind, Llemma from EleutherAI or Julius.
When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks.
Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.
Topological deep learning integrates various topological approaches.

Finance
Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated "robot advisers" have been in use for some years.
World Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: "the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation."

Military
Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.
AI has been used in military operations in Iraq, Syria, Israel and Ukraine.

Generative AI
Agents
Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.

Sexuality
Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.
AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.

Other industry-specific tasks
There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated "AI" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
AI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.
In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights." For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
During the 2024 Indian elections, US$50 millions was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.

Ethics
AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to "solve intelligence, and then use that to solve everything else". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.

Risks and harm
Privacy and copyright
Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.
Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.
AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted "from the question of 'what they know' to the question of 'what they're doing with it'."
Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of "fair use". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include "the purpose and character of the use of the copyrighted work" and "the effect upon the potential market for the copyrighted work". Website owners who do not wish to have their content scraped can indicate it in a "robots.txt" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.

Dominance by tech giants
The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.

Power needs and environmental impacts
In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.
Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and "intelligent", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.
A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found "US power demand (is) likely to experience growth not seen in a generation...." and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.
In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US). Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.
In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.
After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.
Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.
On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. 
According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.

Misinformation
YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .
In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling "authoritarian leaders to manipulate their electorates" on a large scale, among other risks.

Algorithmic bias and fairness
Machine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.
On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as "gorillas" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called "sample size disparity". Google "fixed" this problem by preventing the system from labelling anything as a "gorilla". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.
COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.
A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as "race" or "gender"). The feature will correlate with other features (like "address", "shopping history" or "first name"), and the program will make the same decisions based on these features as it would on "race" or "gender". Moritz Hardt said "the most robust fact in this research area is that fairness through blindness doesn't work."
Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.
Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.
There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.

Lack of transparency
Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.
It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.
People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.
DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems.
Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.

Bad actors and weaponized AI
Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.
AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.
There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.

Technological unemployment
Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.
From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.

Existential risk
It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, "spell the end of the human race". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like "self-awareness" (or "sentience" or "consciousness") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.
First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that "you can't fetch the coffee if you're dead." In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is "fundamentally on our side".
Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.
The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.
In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to "freely speak out about the risks of AI" without "considering how this impacts Google". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.
In 2023, many leading AI experts endorsed the joint statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war".
Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making "human lives longer and healthier and easier." While the tools that are now being used to improve lives can also be used by bad actors, "they can also be used against the bad actors." Andrew Ng also argued that "it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests." Yann LeCun "scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction." In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.

Ethical machines and alignment
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.
Other approaches include Wendell Wallach's "artificial moral agents" and Stuart J. Russell's three principles for developing provably beneficial machines.

Open source
Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the "weights") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.

Frameworks
Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:

Respect the dignity of individual people
Connect with other people sincerely, openly, and inclusively
Care for the wellbeing of everyone
Protect social values, justice, and the public interest
Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.
Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.
The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.

Regulation
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.
In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that "products and services using AI have more benefits than drawbacks". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it "very important", and an additional 41% thought it "somewhat important", for the federal government to regulate AI, versus 13% responding "not very important" and 8% responding "not at all important".
In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.

History
The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an "electronic brain". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for "artificial neurons" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that "machine intelligence" was plausible.
The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as "astonishing": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.
Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do". In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The "AI winter", a period when obtaining funding for AI projects was difficult, followed.
In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.
Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into "sub-symbolic" approaches. Rodney Brooks rejected "representation" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of "connectionism", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence" (a tendency known as the AI effect).
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.
Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.
For many specific tasks, other methods were abandoned.
Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.
In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.
In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI". About 800,000 "AI"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.

Philosophy
Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.

Defining artificial intelligence
Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?" He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people but "it is usual to have a polite convention that everyone thinks."

Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts", they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'" AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".
McCarthy defines intelligence as "the computational part of the ability to achieve goals in the world". Another AI founder, Marvin Minsky, similarly describes it as "the ability to solve hard problems". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the "intelligence" of the machine—and no other philosophical discussion is required, or may not even be possible.
Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way".

Evaluating approaches to AI
No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.

Symbolic AI and its limits
Symbolic AI (or "GOFAI") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."
However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.

Neat vs. scruffy
"Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.

Soft vs. hard computing
Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.

Narrow vs. general AI
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.

Machine consciousness, sentience, and mind
The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on." However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.

Consciousness
David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.

Computationalism and functionalism
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.
Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds." Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.

AI welfare and rights
It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.
In 2017, the European Union considered granting "electronic personhood" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.
Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.

Future
Superintelligence and the singularity
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity".
However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.

Transhumanism
Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.
Edward Fredkin argues that "artificial intelligence is the next step in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.

Decomputing
Arguments for decomputing have been raised by Dan McQuillan (Resisting AI: An Anti-fascist Approach to Artificial Intelligence, 2022), meaning an opposition to the sweeping application and expansion of artificial intelligence. Similar to degrowth the approach criticizes AI as an outgrowth of the systemic issues and capitalist world we live in. Arguing that a different future is possible, in which distance between people is reduced and not increased to AI intermediaries.

In fiction
Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.
Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.

See also
Artificial intelligence and elections – Use and impact of AI on political elections
Artificial intelligence content detection – Software to detect AI-generated content
Behavior selection algorithm – Algorithm that selects actions for intelligent agents
Business process automation – Automation of business processes
Case-based reasoning – Process of solving new problems based on the solutions of similar past problems
Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation
Digital immortality – Hypothetical concept of storing a personality in digital form
Emergent algorithm – Algorithm exhibiting emergent behavior
Female gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets
Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence
Intelligence amplification – Use of information technology to augment human intelligence
Intelligent agent – Software agent which acts autonomously
Mind uploading – Hypothetical process of digitally emulating a brain
Organoid intelligence – Use of brain cells and brain organoids for intelligent computing
Robotic process automation – Form of business process automation technology
Wetware computer – Computer composed of organic material

Explanatory notes
References
AI textbooks
The two most widely used textbooks in 2023 (see the Open Syllabus):

Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.
Rich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.
The four most widely used AI textbooks in 2008:

Other textbooks:

Ertel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.
Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.

History of AI
Other sources
Further reading
External links

"Artificial Intelligence". Internet Encyclopedia of Philosophy.

Title: Artificial intelligence content detection
Artificial intelligence detection software aims to determine whether some content (text, image, video or audio) was generated using artificial intelligence (AI).
However, the reliability of such software is a topic of debate, and there are concerns about the potential misapplication of AI detection software by educators.

Accuracy issues
Multiple AI detection tools have been demonstrated to be unreliable in terms of accurately and comprehensively detecting AI-generated text. In a study conducted by Weber-Wulff et al., and published in 2023, researchers evaluated 14 detection tools including Turnitin and GPT Zero, and found that "all scored below 80% of accuracy and only 5 over 70%."

Text detection
For text, this is usually done to prevent alleged plagiarism, often by detecting repetition of words as telltale signs that a text was AI-generated (including AI hallucinations). They are often used by teachers marking their students, usually on an ad hoc basis. Following the release of ChatGPT and similar AI text generative software, many educational establishments have issued policies against the use of AI by students. AI text detection software is also used by those assessing job applicants, as well as online search engines.
Current detectors may sometimes be unreliable and have incorrectly marked work by humans as originating from AI while failing to detect AI-generated work in other instances. MIT Technology Review said that the technology "struggled to pick up ChatGPT-generated text that had been slightly rearranged by humans and obfuscated by a paraphrasing tool". AI text detection software has also been shown to discriminate against non-native speakers of English.
Two students from the University of California, Davis, were referred to the university's Office of Student Success and Judicial Affairs (OSSJA) after their professors scanned their essays with positive results; the first with an AI detector called GPTZero, and the second with an AI detector integration in Turnitin. However, following media coverage, and a thorough investigation, the students were cleared of any wrongdoing. 
In April 2023, Cambridge University and other members of the Russell Group of universities in the United Kingdom opted out of Turnitin's AI text detection tool, after expressing concerns it was unreliable. The University of Texas at Austin opted out of the system six months later.
In May 2023, a professor at Texas A&M University–Commerce used ChatGPT to detect whether his students' content was written by it, which ChatGPT said was the case. As such, he threatened to fail the class despite ChatGPT not being able to detect AI-generated writing. No students were prevented from graduating because of the issue, and all but one student (who admitted to using the software) were exonerated from accusations of having used ChatGPT in their content.
An article by Thomas Germain, published on Gizmodo in June 2024, reported job losses among freelance writers and journalists due to AI text detection software mistakenly classifying their work as AI-generated.
To improve the reliability of AI text detection, researchers have explored digital watermarking techniques. A 2023 paper titled "A Watermark for Large Language Models"  presents a method to embed imperceptible watermarks into text generated by large language models (LLMs). This watermarking approach allows content to be flagged as AI-generated with a high level of accuracy, even when text is slightly paraphrased or modified. The technique is designed to be subtle and hard to detect for casual readers, thereby preserving readability, while providing a detectable signal for those employing specialized tools. However, while promising, watermarking faces challenges in remaining robust under adversarial transformations and ensuring compatibility across different LLMs.

Anti text detection
There is software available designed to bypass AI text detection. 
A study published in August 2023 analyzed 20 abstracts from papers published in the Eye Journal, which were then paraphrased using GPT-4.0. The AI-paraphrased abstracts were examined for plagiarism using QueText and for AI-generated content using Originality.AI. The texts were then re-processed through an adversarial software called Undetectable.ai in order to reduce the AI-detection scores. The study found that the AI detection tool, Originality.AI, identified text generated by GPT-4 with a mean accuracy of 91.3%. However, after reprocessing by Undetectable.ai, the detection accuracy of Originality.ai dropped to a mean accuracy of 27.8%.
Some experts also believe that techniques like digital watermarking are ineffective because they can be removed or added to trigger false positives. "A Watermark for Large Language Models" paper by Kirchenbauer et al.  also addresses potential vulnerabilities of watermarking techniques. The authors outline a range of adversarial tactics, including text insertion, deletion, and substitution attacks, that could be used to bypass watermark detection. These attacks vary in complexity, from simple paraphrasing to more sophisticated approaches involving tokenization and homoglyph alterations. The study highlights the challenge of maintaining watermark robustness against attackers who may employ automated paraphrasing tools or even specific language model replacements to alter text spans iteratively while retaining semantic similarity. Experimental results show that although such attacks can degrade watermark strength, they also come at the cost of text quality and increased computational resources.

Multilingual text detection
One shortcoming of most AI content detection software is their inability to identify AI-generated text in any language. Large language models (LLMs) like ChatGPT, Claude, and Gemini can write in different languages, but traditional AI text detection tools have primarily been trained in English and a few other widely spoken languages, such as French and Spanish. Fewer AI detection solutions can detect AI-generated text in languages like Farsi, Arabic, or Hindi.

Image, video, and audio detection
Several purported AI image detection software exist, to detect AI-generated images (for example, those originating from Midjourney or DALL-E). They are not completely reliable. 
Others claim to identify video and audio deepfakes, but this technology is also not fully reliable yet either. 
Despite debate around the efficacy of watermarking, Google DeepMind is actively developing a detection software called SynthID, which works by inserting a digital watermark that is invisible to the human eye into the pixels of an image.

See also
AI alignment
Artificial intelligence and elections
Content similarity detection
Hallucination (artificial intelligence)
Natural language processing


== References ==

Title: Biomedical data science
Biomedical data science is a multidisciplinary field which leverages large volumes of data to promote biomedical innovation and discovery. Biomedical data science draws from various fields including Biostatistics, Biomedical informatics, and machine learning, with the goal of understanding biological and medical data. It can be viewed as the study and application of data science to solve biomedical problems. Modern biomedical datasets often have specific features which make their analyses difficult, including:

Large numbers of feature (sometimes billions), typically far larger than the number of samples (typically tens or hundreds)
Noisy and missing data
Privacy concerns (e.g., electronic health record confidentiality)
Requirement of interpretability from decision makers and regulatory bodies
Many biomedical data science projects apply machine learning to such datasets. These characteristics, while also present in many data science applications more generally, make biomedical data science a specific field. Examples of biomedical data science research include: 

Computational genomics
Computational imaging
Electronic health records data mining
Biomedical network science

Training in Biomedical Data Science
The National Library of Medicine of the US National Institutes of Health (NIH) identified key biomedical data scientist attributes in an NIH-wide review: general biomedical subject matter knowledge; programming language expertise; predictive analytics, modeling, and machine learning; team science and communication; and responsible data stewardship.

University Departments and Programs
Johns Hopkins University’s Department of Biomedical Engineering offers biomedical data science training at the undergraduate, master's, and PhD levels. They were the first university to offer programs at both undergraduate and graduate levels.
Dartmouth College's Geisel School of Medicine houses the Department of Biomedical Data Science where Quantitative Biomedical Sciences programs are available at the master's and PhD levels.
Imperial College London’s Faculty of Medicine and Data Science Institute offer an MRes in Biomedical Research (Data Science).
Mount Sinai’s Icahn School of Medicine offers a Master of Science in Biomedical Data Science.
Stanford University’s Department of Biomedical Data Science offers multiple biomedical informatics graduate programs (MS, PhD, and MD/PhD).
The University of Exeter’s College of Healthcare and Medicine offers an MSc in Health Data Science.

Biomedical Data Science Research in Academia
Scholarly Journals
The first journal dedicated to biomedical data science appeared in 2018 – Annual Review of Biomedical Data Science. “The Annual Review of Biomedical Data Science provides comprehensive expert reviews in biomedical data science, focusing on advanced methods to store, retrieve, analyze, and organize biomedical data and knowledge. The scope of the journal encompasses informatics, computational, and statistical approaches to biomedical data, including the sub-fields of bioinformatics, computational biology, biomedical informatics, clinical and clinical research informatics, biostatistics, and imaging informatics. The mission of the journal is to identify both emerging and established areas of biomedical data science, and the leaders in these fields.” 
Other journals have a more general scope than biomedical data science, but regularly publish biomedical data science research such as Health Data Science and Nature Machine Intelligence. Data science would not exist without curated datasets and the field has seen the rise of journals that are dedicated to describing and validating such datasets, some of which are useful for biomedical applications, including Scientific Data, Biomedical Data, and Data.

Example
The Human Genome Project (HGP), which uncovered the DNA sequences that compose human genes, would not have been possible without biomedical data science. Significant computational resources were required to process the data in the HGP, as the human genome contains over 6 billion DNA base pairs. Scientists constructed the genome by piecing together small fragments of DNA, and computing overlaps between these sequences alone required over 10,000 CPU hours. At this massive data scale, scientists relied on advanced algorithms to perform data processing steps such as sequence assembly and sequence alignment for quality control. Some of these algorithms, such as BLAST, are still used in modern bioinformatics. Scientists in the HGP also had to address complexities often associated with biomedical data including noisy data, such as DNA read errors, and privacy rights of the research subjects. The HGP, completed in 2004, has had immense impact both biologically, shedding light on human evolution, and medically, launching the field of bioinformatics and leading to technologies such as genetic screening and gene therapy.


== References ==

Title: Data-driven astronomy
Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies. The field is closely related to astrostatistics.
Data-driven astronomy (DDA) refers to the use of data science in astronomy. Several outputs of telescopic observations and sky surveys are taken into consideration and approaches related to data mining and big data management are used to analyze, filter, and normalize the data set that are further used for making Classifications, Predictions, and Anomaly detections by advanced Statistical approaches, digital image processing and machine learning. The output of these processes is used by astronomers and space scientists to study and identify patterns, anomalies, and movements in outer space and conclude theories and discoveries in the cosmos.

Background
Astroinformatics is primarily focused on developing the tools, methods, and applications of computational science, data science, machine learning, and statistics for research and education in data-oriented astronomy. Early efforts in this direction included data discovery, metadata standards development, data modeling, astronomical data dictionary development, data access, information retrieval, data integration, and data mining in the astronomical Virtual Observatory initiatives. Further development of the field, along with astronomy community endorsement, was presented to the National Research Council (United States) in 2009 in the astroinformatics "state of the profession" position paper for the 2010 Astronomy and Astrophysics Decadal Survey. That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper Astroinformatics: Data-Oriented Astronomy Research and Education.
Astroinformatics as a distinct field of research was inspired by work in the fields of Geoinformatics, Cheminformatics, Bioinformatics, and through the eScience work of Jim Gray (computer scientist) at Microsoft Research, whose legacy was remembered and continued through the Jim Gray eScience Awards.
Although the primary focus of astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to digitize historical and recent astronomical observations and images in a large database for efficient retrieval through web-based interfaces. Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.
Astroinformatics is described as the "fourth paradigm" of astronomical research. There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science. Data mining and machine learning play significant roles in astroinformatics as a scientific research discipline due to their focus on "knowledge discovery from data" (KDD) and "learning from data".
The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the Large Synoptic Survey Telescope and into the exabytes with the Square Kilometre Array. This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part due to this, data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing information-intensive and data-intensive sub-disciplines to an extent that these sub-disciplines are now becoming (or have already become) standalone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, such programs most likely will be developed in the near future.
Informatics has been recently defined as "the use of digital data, information, and related services for research and knowledge generation". However the usual, or commonly used definition is "informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support." Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., taxonomies, ontologies, folksonomies, and/or collaborative tagging) plus Astrostatistics will also be heavily involved. Citizen science projects (such as Galaxy Zoo) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.
In 2007, the Galaxy Zoo project was launched for morphological classification of a large number of galaxies. In this project, 900,000 images were considered for classification that were taken from the Sloan Digital Sky Survey (SDSS) for the past 7 years. The task was to study each picture of a galaxy, classify it as elliptical or spiral, and determine whether it was spinning or not. The team of Astrophysicists led by Kevin Schawinski in Oxford University were in charge of this project and Kevin and his colleague Chris Linlott figured out that it would take a period of 3–5 years for such a team to complete the work. There they came up with the idea of using Machine Learning and Data Science techniques for analyzing the images and classifying them.
In 2012, two position papers were presented to the Council of the American Astronomical Society that led to the establishment of formal working groups in astroinformatics and Astrostatistics for the profession of astronomy within the US and elsewhere.
Astroinformatics provides a natural context for the integration of education and research. The experience of research can now be implemented within the classroom to establish and grow data literacy through the easy re-use of data. It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.

Methodology
The data retrieved from the sky surveys are first brought for data preprocessing. In this, redundancies are removed and filtrated. Further, feature extraction is performed on this filtered data set, which is further taken for processes. Some of the renowned sky surveys are listed below:

The Palomar Digital Sky Survey (DPOSS)
The Two-Micron All Sky Survey (2MASS)
Green Bank Telescope (GBT)
The Galaxy Evolution Explorer (GALEX)
The Sloan Digital Sky Survey (SDSS)
SkyMapper Southern Sky Survey (SMSS)
The Panoramic Survey Telescope and Rapid Response System (PanSTARRS)
The Large Synoptic Survey Telescope (LSST)
The Square Kilometer Array (SKA)
The size of data from the above-mentioned sky surveys ranges from 3 TB to almost 4.6 EB. Further, data mining tasks that are involved in the management and manipulation of the data involve methods like classification, regression, clustering, anomaly detection, and time-series analysis. Several approaches and applications for each of these methods are involved in the task accomplishments.

Classification
Classification is used for specific identifications and categorizations of astronomical data such as Spectral classification, Photometric classification, Morphological classification, and classification of solar activity. The approaches of classification techniques are listed below:

Artificial neural network (ANN)
Support vector machine (SVM)
Learning vector quantization (LVQ)
Decision tree
Random forest
k-nearest neighbors
Naïve Bayesian networks
Radial basis function network
Gaussian process
Decision table
Alternating decision tree (ADTree)

Regression
Regression is used to make predictions based on the retrieved data through statistical trends and statistical modeling. Different uses of this technique are used for fetching Photometric redshifts and measurements of physical parameters of stars. The approaches are listed below:

Artificial neural network (ANN)
Support vector regression (SVR)
Decision tree
Random forest
k-nearest neighbors regression
Kernel regression
Principal component regression (PCR)
Gaussian process
Least squared regression (LSR)
Partial least squares regression

Clustering
Clustering is classifying objects based on a similarity measure metric. It is used in Astronomy for Classification as well as Special/rare object detection. The approaches are listed below:

Principal component analysis (PCA)
DBSCAN
k-means clustering
OPTICS
Cobweb model
Self-organizing map (SOM)
Expectation Maximization
Hierarchical Clustering
AutoClass
Gaussian Mixture Modeling (GMM)

Anomaly detection
Anomaly detection is used for detecting irregularities in the dataset. However, this technique is used here to detect rare/special objects. The following approaches are used:

Principal Component Analysis (PCA)
k-means clustering
Expectation Maximization
Hierarchical clustering
One-class SVM

Time-series analysis
Time-Series analysis helps in analyzing trends and predicting outputs over time. It is used for trend prediction and novel detection (detection of unknown data). The approaches used here are:

Artificial neural network (ANN)
Support vector regression (SVR)
Decision tree

Conferences
Additional conferences and conference lists:

See also
Astronomy and Computing
Astrophysics Data System
Astrophysics Source Code Library
Astrostatistics
Committee on Data for Science and Technology
Data-driven astronomy
Galaxy Zoo
International Astrostatistics Association
International Virtual Observatory Alliance (IVOA)
MilkyWay@home
Virtual Observatory
WorldWide Telescope
Zooniverse

References
External links
International AstroInformatics Association (IAIA)
Astronomical Data Analysis Software and Systems (ADASS)
Astrostatistics and Astroinformatics Portal
Cosmostatistics Initiative (COIN)
Astroinformatics and Astrostatistics Commission of the International Astronomical Union

Title: How Data Happened
How Data Happened: A History from the Age of Reason to the Age of Algorithms is a 2023 non-fiction book written by Columbia University professors Chris Wiggins and Matthew L. Jones. The book explores the history of data and statistics from the end of the 18th century to the present day.

Content
The book starts at the end of the 18th century, when European states began tabulating physical resources, and ends at the present day, when algorithms manipulate our personal information as a commodity. It looks at the rise of data and statistics, and how early statistical methods were used to justify eugenics, quantify supposed racial differences, and develop military and industrial applications. The authors also discuss the impact of the internet and e-commerce on data collection, the rise of data science, and the consequences of government-run surveillance systems collecting vast amounts of personal data for customized, targeted advertising. They emphasize the importance of privacy and democracy and propose remedies to the problems caused by mass data collection, including stronger regulation of the tech industry and collective action by its employees. The book is a historical analysis that provides context for understanding the debates surrounding data and its control.
The book has 336 pages and was published in 2023 by W. W. Norton & Company.

References
External links
The wild evolution of data science and how to unpack it, book excerpt on Big Think
From Eugenics to Targeted Advertising: The Dark Role of Data in Sorting Humanity, book excerpt on Literary Hub

Title: List of important publications in data science
This is a list of important publications in data science, generally organized by order of use in a data analysis workflow.

See the list of important publications in statistics for more research-based and fundamental publications; while this list is more applied, business oriented, and cross-disciplinary.
General article inclusion criteria are:

Papers from notable practitioners or notable professors, either with a Wikipedia page or reference to their notability
Common knowledge all data professionals should know, with references validating this claim
Highly cited applied statistics and machine learning publications
Discussion-facilitating papers on the field of data science as a whole (for example, the Attention Is All You Need paper is arguably a landmark paper that can be added here, but it is specific to generative artificial intelligence, not for all practitioners of data)
Some reasons why a particular publication might be regarded as important:

Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of data science.
When possible, a reference is used to validate the inclusion of the publication in this list.

History
Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)

Author: Leo Breiman
Publication data: 
Online version: https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf
Description: Describes two cultures of statistics, one using a parsimonious and generative stochastic model, while the other is an algorithmic model with no known mechanism for how the data is generated. Breiman argues that while statistics has traditionally favored using the stochastic model, there is value in expanding the methods that statisticians can use to study phenomenon.
Importance: Influence on the philosophies of statisticians right before the increased use of machine learning and deep learning methods. In a 20-year retrospective on this article, "Breiman's words are perhaps more relevant than ever". Notable statisticians at the time wrote opinion pieces about the publication. Although overall critical of the publication, David Cox writes that the publication "contains enough truth and exposes enough weaknesses to be thought-provoking." Bradley Efron commented that this publication is a "stimulating paper". Emanuel Parzen also comments about this publication that "Breiman alerts us to systematic blunders (leading to wrong conclusions) that have been committed applying current statistical practice of data modeling".
50 Years of Data Science

Author: David Donoho
Publication data: 
Online version: https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1384734
Description: Retrospective discussion paper on the history and origins of data science, with a number of commentary from notable statisticians.
Importance: This has been described as "the first in the field to present such a comprehensive and in-depth survey and overview", and helps to define the field that has many definitions.
The Composable Data Management System Manifesto

Author: Pedro Pedreira, Orri Erling, Konstantinos Karanasos, Scott Schneider, Wes McKinney, Satya R Valluri, Mohamed Zait, Jacques Nadeau
Publication data: 
Online version: https://www.vldb.org/pvldb/vol16/p2679-pedreira.pdf
Description: The vision paper advocating for a paradigm shift in how data management systems are designed using standard, composable, interoperable tools rather than siloed software tools.
Importance: A paradigm shifting view on how future data science software tools should be designed for more efficient workflows, the principles of which "will be especially crucial for addressing fragmentation, improving interoperability, and promoting user-centricity as data ecosystems grow increasingly complex".

Data collection and organization
Tidy Data

Author: Hadley Wickham
Publication data: 
Online version: https://www.jstatsoft.org/article/view/v059i10/ https://vita.had.co.nz/papers/tidy-data.pdf
Description: Describes a framework for data cleaning that is summarized in the quote, "each variable is a column, each observation is a row, and each type of observational unit is a table". This allows a standard data structure for which data analysis tools can be consistently built around.
Importance: Cited over 1,500 times, this effort for tidy data has been described by David Donoho as having "more impact on today’s practice of data analysis than many highly regarded theoretical statistics articles". In the context of data visualization, this publication is said to support "efficient exploration and prototyping because variables can be assigned different roles in the plot without modifying anything about the original dataset".
Data Organization in Spreadsheets

Author: Karl W. Broman and Kara H. Woo
Publication data: 
Online version: https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989
Description: This article offers practical recommendations for organizing data in spreadsheets, like Microsoft Excel and Google Sheets, to reduce errors and lower the barrier for later analyses due to limitations in spreadsheets or quirks in the software.
Importance: Influences teaching both data and non-data practitioners to create more analysis-friendly spreadsheets, and has been described to outline "spreadsheet best practices".

Data visualizations
Quantitative Graphics in Statistics: A Brief History

Author: James R. Beniger and Dorothy L. Robyn
Publication data: 
Online version: https://www.jstor.org/stable/2683467
Description: Outlines history and evolution of quantitative graphics in statistics, going through spatial organization (17th and 18th centuries), discrete comparison (18th and 19th centuries), continuous distribution (19th century), and multivariate distribution and correlation (late 19th and 20th centuries).
Importance: Helps put into perspective for learning data practitioners the recency of graphics that are used. A later publication "Graphical Methods in Statistics" by Stephen Fienberg in 1979 writes that his publication "owes much to the work of Beniger and Robyn".

Tooling
Hidden Technical Debt in Machine Learning Systems

Author: D. Sculley, Gary Holy, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, Dan Dennison
Publication data: 
Online version: https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf
Description: This paper argues that it is "dangerous to think of [complex machine learning] quick wins as coming for free" and overviews risk factors to account for when implementing a machine learning system.
Importance: All authors worked for Google, article is cited over 1,000 times, and helped practitioners thinking about quickly implementing a machine learning tool without understanding the long-term maintenance of the tool.
A few useful things to know about machine learning

Author: Pedro Domingos
Publication data: 
Online version: https://dl.acm.org/doi/10.1145/2347736.2347755 https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf
Description: The purpose of this paper is to distill inaccessible "folk knowledge" to effectively implement machine learning projects because "machine learning projects take much longer than necessary or wind up producing less-than-ideal results".
Importance: Cited over 4,000 times to influence the common set of knowledge for data practitioners using machine learning.

Teaching data science
The Introductory Statistics Course: A Ptolemaic Curriculum

Author: George W. Cobb
Publication data: 
Online version: https://escholarship.org/uc/item/6hb3k0nz
Description: This paper argues for a rethinking of how teachers of statistics should structure their introductory statistics courses away from the technical machinery based on the normal distribution and towards simpler alternative methods based on permutations done on computers.
Importance: Cited over 300 times, this paper influenced teachers of statistics in the 21st century to reconsider teaching the mere mechanics of statistics, while the use of computers can be leveraged for doing more with less.

See also
Lists of important publications in science

References
External links
Papers and tech blogs by companies sharing their work on data science and machine learning in production.

Title: Model-based clustering
In statistics, cluster analysis is the algorithmic grouping of objects into homogeneous
groups based on numerical measurements. Model-based clustering based on a statistical model for the data, usually a mixture model. This has several advantages, including a principled statistical basis for clustering,
and ways to choose the number of clusters, to choose the best clustering model, to assess the uncertainty of the clustering, and to identify outliers that do not belong to any group.

Model-based clustering
Suppose that for each of 
  
    
      
        n
      
    
    {\displaystyle n}
  
 observations we have data on 

  
    
      
        d
      
    
    {\displaystyle d}
  
 variables, denoted by 
  
    
      
        
          y
          
            i
          
        
        =
        (
        
          y
          
            i
            ,
            1
          
        
        ,
        …
        ,
        
          y
          
            i
            ,
            d
          
        
        )
      
    
    {\displaystyle y_{i}=(y_{i,1},\ldots ,y_{i,d})}
  

for observation 
  
    
      
        i
      
    
    {\displaystyle i}
  
. Then
model-based clustering expresses the probability density function of

  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 as a finite mixture, or weighted average of 

  
    
      
        G
      
    
    {\displaystyle G}
  
 component probability density functions:

  
    
      
        p
        (
        
          y
          
            i
          
        
        )
        =
        
          ∑
          
            g
            =
            1
          
          
            G
          
        
        
          τ
          
            g
          
        
        
          f
          
            g
          
        
        (
        
          y
          
            i
          
        
        ∣
        
          θ
          
            g
          
        
        )
        ,
      
    
    {\displaystyle p(y_{i})=\sum _{g=1}^{G}\tau _{g}f_{g}(y_{i}\mid \theta _{g}),}
  

where 
  
    
      
        
          f
          
            g
          
        
      
    
    {\displaystyle f_{g}}
  
 is a probability density function with
parameter 
  
    
      
        
          θ
          
            g
          
        
      
    
    {\displaystyle \theta _{g}}
  
, 
  
    
      
        
          τ
          
            g
          
        
      
    
    {\displaystyle \tau _{g}}
  
 is the corresponding
mixture probability where 
  
    
      
        
          ∑
          
            g
            =
            1
          
          
            G
          
        
        
          τ
          
            g
          
        
        =
        1
      
    
    {\displaystyle \sum _{g=1}^{G}\tau _{g}=1}
  
.
Then in its simplest form, model-based clustering views each component
of the mixture model as a cluster, estimates the model parameters, and assigns
each observation to cluster corresponding to its most likely mixture component.

Gaussian mixture model
The most common model for continuous data is that 
  
    
      
        
          f
          
            g
          
        
      
    
    {\displaystyle f_{g}}
  
 is a multivariate normal distribution with mean vector 
  
    
      
        
          μ
          
            g
          
        
      
    
    {\displaystyle \mu _{g}}
  

and covariance matrix 
  
    
      
        
          Σ
          
            g
          
        
      
    
    {\displaystyle \Sigma _{g}}
  
, so that 

  
    
      
        
          θ
          
            g
          
        
        =
        (
        
          μ
          
            g
          
        
        ,
        
          Σ
          
            g
          
        
        )
      
    
    {\displaystyle \theta _{g}=(\mu _{g},\Sigma _{g})}
  
. 
This defines a Gaussian mixture model. The parameters of the model, 

  
    
      
        
          τ
          
            g
          
        
      
    
    {\displaystyle \tau _{g}}
  
 and 
  
    
      
        
          θ
          
            g
          
        
      
    
    {\displaystyle \theta _{g}}
  
 for 
  
    
      
        g
        =
        1
        ,
        …
        ,
        G
      
    
    {\displaystyle g=1,\ldots ,G}
  
, 
are typically estimated by maximum likelihood estimation using the
expectation-maximization algorithm (EM); see also 
EM algorithm and GMM model.
Bayesian inference is also often used for inference about finite
mixture models. The Bayesian approach also allows for the case where the number of components, 
  
    
      
        G
      
    
    {\displaystyle G}
  
, is infinite, using a Dirichlet process prior, yielding a Dirichlet process mixture model for clustering.

Choosing the number of clusters
An advantage of model-based clustering is that it provides statistically
principled ways to choose the number of clusters. Each different choice of the number of groups 
  
    
      
        G
      
    
    {\displaystyle G}
  
 corresponds to a different mixture model. Then standard statistical model selection criteria such as the 
Bayesian information criterion (BIC) can be used to choose 
  
    
      
        G
      
    
    {\displaystyle G}
  
. The integrated completed likelihood (ICL) is a different criterion designed to choose the number of clusters rather than the number of mixture components in the model; these will often be different if highly non-Gaussian clusters are present.

Parsimonious Gaussian mixture model
For data with high dimension, 
  
    
      
        d
      
    
    {\displaystyle d}
  
, using a full covariance matrix for each mixture component requires estimation of many parameters, which can result in a loss of precision, generalizabity and interpretability. Thus it is common to use more parsimonious component covariance matrices exploiting their geometric interpretation. Gaussian clusters are ellipsoidal, with their volume, shape and orientation determined by the covariance matrix. Consider the eigendecomposition of a matrix 

  
    
      
        
          Σ
          
            g
          
        
        =
        
          λ
          
            g
          
        
        
          D
          
            g
          
        
        
          A
          
            g
          
        
        
          D
          
            g
          
          
            T
          
        
        ,
      
    
    {\displaystyle \Sigma _{g}=\lambda _{g}D_{g}A_{g}D_{g}^{T},}
  

where 
  
    
      
        
          D
          
            g
          
        
      
    
    {\displaystyle D_{g}}
  
 is the matrix of eigenvectors of

  
    
      
        
          Σ
          
            g
          
        
      
    
    {\displaystyle \Sigma _{g}}
  
, 

  
    
      
        
          A
          
            g
          
        
        =
        
          
            diag
          
        
        {
        
          A
          
            1
            ,
            g
          
        
        ,
        …
        ,
        
          A
          
            d
            ,
            g
          
        
        }
      
    
    {\displaystyle A_{g}={\mbox{diag}}\{A_{1,g},\ldots ,A_{d,g}\}}
  

is a diagonal matrix whose elements are proportional to
the eigenvalues of 
  
    
      
        
          Σ
          
            g
          
        
      
    
    {\displaystyle \Sigma _{g}}
  
 in descending order,
and 
  
    
      
        
          λ
          
            g
          
        
      
    
    {\displaystyle \lambda _{g}}
  
 is the associated constant of proportionality.
Then 
  
    
      
        
          λ
          
            g
          
        
      
    
    {\displaystyle \lambda _{g}}
  
 controls the volume of the ellipsoid,

  
    
      
        
          A
          
            g
          
        
      
    
    {\displaystyle A_{g}}
  
 its shape, and 
  
    
      
        
          D
          
            g
          
        
      
    
    {\displaystyle D_{g}}
  
 its orientation.
Each of the volume, shape and orientation of the clusters can be 
constrained to be equal (E) or allowed to vary (V); the orientation can
also be spherical, with identical eigenvalues (I). This yields 14 possible clustering models, shown in this table:

It can be seen that many of these models are more parsimonious, with far fewer 
parameters than the unconstrained model that has 90 parameters when

  
    
      
        G
        =
        4
      
    
    {\displaystyle G=4}
  
 and 
  
    
      
        d
        =
        9
      
    
    {\displaystyle d=9}
  
.
Several of these models correspond to well-known heuristic clustering methods.
For example, k-means clustering is equivalent to estimation of the
EII clustering model using the classification EM algorithm. The Bayesian information criterion (BIC)
can be used to choose the best clustering model as well as the number of clusters. It can also be used as the basis for a method to choose the variables
in the clustering model, eliminating variables that are not useful for clustering.
Different Gaussian model-based clustering methods have been developed with
an eye to handling high-dimensional data. These include the pgmm method, which is based on the mixture of 
factor analyzers model, and the HDclassif method, based on the idea of subspace clustering.
The mixture-of-experts framework extends model-based clustering to include covariates.

Example
We illustrate the method with a dateset consisting of three measurements
(glucose, insulin, sspg) on 145 subjects for the purpose of diagnosing
diabetes and the type of diabetes present.
The subjects were clinically classified into three groups: normal,
chemical diabetes and overt diabetes, but we use this information only
for evaluating clustering methods, not for classifying subjects.

The BIC plot shows the BIC values for each combination of the number of
clusters, 
  
    
      
        G
      
    
    {\displaystyle G}
  
, and the clustering model from the Table.
Each curve corresponds to a different clustering model.
The BIC favors 3 groups, which corresponds to the clinical assessment.
It also favors the unconstrained covariance model, VVV.
This fits the data well, because the normal patients have low values of
both sspg and insulin, while the distributions of the chemical and
overt diabetes groups are elongated, but in different directions.
Thus the volumes, shapes and orientations of the three groups are clearly
different, and so the unconstrained model is appropriate, as selected
by the model-based clustering method.

The classification plot shows the classification of the subjects by model-based
clustering. The classification was quite accurate, with a 12% error rate
as defined by the clinical classificiation.
Other well-known clustering methods performed worse with higher
error rates, such as single-linkage clustering with 46%,
average link clustering with 30%, complete-linkage clustering
also with 30%, and k-means clustering with 28%.

Outliers in clustering
An outlier in clustering is a data point that does not belong to any of
the clusters. One way of modeling outliers in model-based clustering is
to include an additional mixture component that is very dispersed, with
for example a uniform distribution. Another approach is to replace the multivariate 
normal densities by 
  
    
      
        t
      
    
    {\displaystyle t}
  
-distributions, with the idea that the long tails of the

  
    
      
        t
      
    
    {\displaystyle t}
  
-distribution would ensure robustness to outliers.
However, this is not breakdown-robust.
A third approach is the "tclust" or data trimming approach
 which excludes observations identified as
outliers when estimating the model parameters.

Non-Gaussian clusters and merging
Sometimes one or more clusters deviate strongly from the Gaussian assumption.
If a Gaussian mixture is fitted to such data, a strongly non-Gaussian 
cluster will often be represented by several mixture components rather than
a single one. In that case, cluster merging can be used to find a better
clustering. A different approach is to use mixtures
of complex component densities to represent non-Gaussian clusters.

Non-continuous data
Categorical data
Clustering multivariate categorical data is most often done using the
latent class model. This assumes that the data arise from a finite
mixture model, where within each cluster the variables are independent.

Mixed data
These arise when variables are of different types, such
as continuous, categorical or ordinal data. A latent class model for
mixed data assumes local independence between the variable. The location model relaxes the local independence
assumption. The clustMD approach assumes that
the observed variables are manifestations of underlying continuous Gaussian
latent variables.

Count data
The simplest model-based clustering approach for multivariate
count data is based on finite mixtures with locally independent Poisson
distributions, similar to the latent class model. 
More realistic approaches allow for dependence and overdispersion in the 
counts. 
These include methods based on the multivariate Poisson distribution,
the multivarate Poisson-log normal distribution, the integer-valued
autoregressive (INAR) model and the Gaussian Cox model.

Sequence data
These consist of sequences of categorical values from a finite set of 
possibilities, such as life course trajectories. 
Model-based clustering approaches include group-based trajectory and
growth mixture models  and a distance-based
mixture model.

Rank data
These arise when individuals rank objects in order of preference. The data
are then ordered lists of objects, arising in voting, education, marketing
and other areas. Model-based clustering methods for rank data include
mixtures of Plackett-Luce models and mixtures of Benter models, 
and mixtures of Mallows models.

Network data
These consist of the presence, absence or strength of connections between 
individuals or nodes, and are widespread in the social sciences and biology.
The stochastic blockmodel carries out model-based clustering of the nodes
in a network by assuming that there is a latent clustering and that 
connections are formed independently given the clustering. The latent position cluster model
assumes that each node occupies a position in an unobserved latent space,
that these positions arise from a mixture of Gaussian distributions,
and that presence or absence of a connection is associated with distance
in the latent space.

Software
Much of the model-based clustering software is in the form of a publicly
and freely available R package. Many of these are listed in the
CRAN Task View on Cluster Analysis and Finite Mixture Models.
The most used such package is
mclust, 
which is used to cluster continuous data and has been downloaded over 
8 million times.
The poLCA package  clusters
categorical data using the latent class model. 
The clustMD package  clusters
mixed data, including continuous, binary, ordinal and nominal variables.
The flexmix package 
does model-based clustering for a range of component distributions.
The mixtools package  can cluster
different data types. Both flexmix and mixtools
implement model-based clustering with covariates.

History
Model-based clustering was first invented in 1950 by Paul Lazarsfeld
for clustering multivariate discrete data, in the form of the 
latent class model.
In 1959, Lazarsfeld gave a lecture on latent structure analysis 
at the University of California-Berkeley, where John H. Wolfe was an M.A. student.
This led Wolfe to think about how to do the same thing for continuous
data, and in 1965 he did so, proposing the Gaussian mixture model for 
clustering.
He also produced the first software for estimating it, called NORMIX.
Day (1969), working independently, was the first to publish a journal
article on the approach.
However, Wolfe deserves credit as the inventor of model-based clustering
for continuous data.
Murtagh and Raftery (1984) developed a model-based clustering method
based on the eigenvalue decomposition of the component covariance matrices. 
McLachlan and Basford (1988) was the first book on the approach, 
advancing methodology and sparking interest.
Banfield and Raftery (1993) coined the term "model-based clustering",
introduced the family of parsimonious models,  
described an information criterion for
choosing the number of clusters, proposed the uniform model for outliers,
and introduced the mclust software.
Celeux and Govaert (1995) showed how to perform maximum likelihood estimation 
for the models. 
Thus, by 1995 the core components of the methodology were in place, 
laying the groundwork for extensive development since then.

Further reading
Scrucca, L.; Fraley, C.; Murphy, T.B.; Raftery, A.E. (2023). Model-Based Clustering, Classification and Density Estimation using mclust in R. Chapman and Hall/CRC Press. ISBN 9781032234953.
Bouveyron, C.; Celeux, G.; Murphy, T.B.; Raftery, A.E. (2019). Model-Based Clustering and Classification for Data Science: With Applications in R. Cambridge University Press. ISBN 9781108494205.
Free download: https://math.univ-cotedazur.fr/~cbouveyr/MBCbook/

Celeux, G; Fruhwirth-Schnatter, S.; Robert, C.P. (2018). Handbook of Mixture Analysis. Chapman and Hall/CRC Press. ISBN 9780367732066.
McNicholas, P.D. (2016). Mixture Model-Based Clustering. Chapman and Hall/CRC Press. ISBN 9780367736958.
Hennig, C.; Melia, M.; Murtagh, F.; Rocci, R. (2015). Handbook of Cluster Analysis. Chapman and Hall/CRC Press. ISBN 9781466551886.
Mengersen, K.L.; Robert, C.P.; Titterington, D.M. (2011). Mixtures: Estimation and Applications. Wiley. ISBN 9781119993896.
McLachlan, G.J.; Peel, D. (2000). Finite Mixture Models. Wiley-Interscience. ISBN 9780471006268.


== References ==

Title: Observational Health Data Sciences and Informatics
The  Observational Health Data Sciences and Informatics, or OHDSI (pronounced "Odyssey") is an international collaborative effort aimed at improving health outcomes through large-scale analytics of health data. The OHDSI effort includes diverse researchers and health databases worldwide, with its central coordinating center located at Columbia University.
The group was derived from the Observational Medical Outcomes Partnership (OMOP), a public-private consortium based in the United States of America, created with the goal of improving the state of observational health data for better drug development, which started in response to the U.S. Food and Drug Administration (FDA) Amendments Act of 2007. OMOP developed a Common Data Model (CDM), standardizing the way observational data is represented. After OMOP ended, this standard started being maintained and updated by OHDSI.
As of February 2024, the most recent CDM is at version 6.0, while version 5.4 is the stable version used by most tools in the OMOP ecosystem.

See also
Health informatics
Open science
Big data
Legacy OMOP methods

References
External links
OHDSI official website
Legacy OMOP methods

Title: Persistence barcode
In topological data analysis, a persistence barcode, sometimes shortened to barcode, is an algebraic invariant associated with a filtered chain complex or a persistence module that characterizes the stability of topological features throughout a growing family of spaces. Formally, a persistence barcode consists of a multiset of intervals in the extended real line, where the length of each interval corresponds to the lifetime of a topological feature in a filtration, usually built on a point cloud, a  graph, a  function, or, more generally, a simplicial complex or a chain complex. Generally, longer intervals in a barcode correspond to more robust features, whereas shorter intervals are more likely to be noise in the data. A persistence barcode is a complete invariant that captures all the topological information in a filtration. In algebraic topology, the persistence barcodes were first introduced by Sergey Barannikov in 1994 as the "canonical forms" invariants consisting of a multiset of line segments with ends on two parallel lines, and later, in geometry processing, by Gunnar Carlsson et al. in 2004.

Definition
Let 
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
  
 be a fixed field. Consider a real-valued function on a chain complex 
  
    
      
        f
        :
        K
        →
        
          R
        
      
    
    {\displaystyle f:K\rightarrow \mathbb {R} }
  
 compatible with the differential, so that 
  
    
      
        f
        (
        
          σ
          
            i
          
        
        )
        ≤
        f
        (
        τ
        )
      
    
    {\displaystyle f(\sigma _{i})\leq f(\tau )}
  
 whenever 
  
    
      
        ∂
        τ
        =
        
          ∑
          
            i
          
        
        
          σ
          
            i
          
        
      
    
    {\displaystyle \partial \tau =\sum _{i}\sigma _{i}}
  
  in 
  
    
      
        K
      
    
    {\displaystyle K}
  
. Then for every 
  
    
      
        a
        ∈
        
          R
        
      
    
    {\displaystyle a\in \mathbb {R} }
  
 the sublevel set 
  
    
      
        
          K
          
            a
          
        
        =
        
          f
          
            −
            1
          
        
        (
        (
        −
        ∞
        ,
        a
        ]
        )
      
    
    {\displaystyle K_{a}=f^{-1}((-\infty ,a])}
  
 is a subcomplex of K, and  the values of 
  
    
      
        f
      
    
    {\displaystyle f}
  
 on the generators in 
  
    
      
        K
      
    
    {\displaystyle K}
  
  define a filtration (which is in practice always finite): 

  
    
      
        ∅
        =
        
          K
          
            0
          
        
        ⊆
        
          K
          
            1
          
        
        ⊆
        ⋯
        ⊆
        
          K
          
            n
          
        
        =
        K
      
    
    {\displaystyle \emptyset =K_{0}\subseteq K_{1}\subseteq \cdots \subseteq K_{n}=K}
  
.
Then, the filtered complexes classification theorem states that for any filtered chain complex over  
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
  
, there exists a linear transformation that preserves the filtration and brings the filtered complex into so called canonical form, a canonically defined direct sum of filtered complexes of two types: two-dimensional complexes with trivial homology 
  
    
      
        d
        (
        
          e
          
            
              a
              
                j
              
            
          
        
        )
        =
        
          e
          
            
              a
              
                i
              
            
          
        
      
    
    {\displaystyle d(e_{a_{j}})=e_{a_{i}}}
  
 and one-dimensional complexes with trivial differential 
  
    
      
        d
        (
        
          e
          
            
              a
              
                i
              
              ′
            
          
        
        )
        =
        0
      
    
    {\displaystyle d(e_{a'_{i}})=0}
  
. The multiset 
  
    
      
        
          
            
              B
            
          
          
            f
          
        
      
    
    {\displaystyle {\mathcal {B}}_{f}}
  
 of the intervals  
  
    
      
        [
        
          a
          
            i
          
        
        ,
        
          a
          
            j
          
        
        )
      
    
    {\displaystyle [a_{i},a_{j})}
  
 or 
  
    
      
        [
        
          a
          
            i
          
          ′
        
        ,
        ∞
        )
      
    
    {\displaystyle [a_{i}',\infty )}
  
 describing the canonical form,  is called the barcode, and it is the complete invariant of the filtered chain complex. 
The concept of a persistence module is intimately linked to the notion of a filtered chain complex. A persistence module 
  
    
      
        M
      
    
    {\displaystyle M}
  
 indexed over 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  
 consists of a family of 
  
    
      
        
          F
        
      
    
    {\displaystyle \mathbb {F} }
  
-vector spaces 
  
    
      
        {
        
          M
          
            t
          
        
        
          }
          
            t
            ∈
            
              R
            
          
        
      
    
    {\displaystyle \{M_{t}\}_{t\in \mathbb {R} }}
  
 and linear maps 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        :
        
          M
          
            s
          
        
        →
        
          M
          
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}:M_{s}\to M_{t}}
  
 for each 
  
    
      
        s
        ≤
        t
      
    
    {\displaystyle s\leq t}
  
 such that 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        ∘
        
          φ
          
            r
            ,
            s
          
        
        =
        
          φ
          
            r
            ,
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}\circ \varphi _{r,s}=\varphi _{r,t}}
  
 for all 
  
    
      
        r
        ≤
        s
        ≤
        t
      
    
    {\displaystyle r\leq s\leq t}
  
. This construction is not specific to 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  
; indeed, it works identically with any totally-ordered set.

A persistence module 
  
    
      
        M
      
    
    {\displaystyle M}
  
 is said to be of finite type if it contains a finite number of unique finite-dimensional vector spaces. The latter condition is sometimes referred to as pointwise finite-dimensional.
Let 
  
    
      
        I
      
    
    {\displaystyle I}
  
 be an interval in 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  
. Define a persistence module 
  
    
      
        Q
        (
        I
        )
      
    
    {\displaystyle Q(I)}
  
 via 
  
    
      
        Q
        (
        
          I
          
            s
          
        
        )
        =
        
          
            {
            
              
                
                  0
                  ,
                
                
                  
                    if 
                  
                  s
                  ∉
                  I
                  ;
                
              
              
                
                  
                    F
                  
                  ,
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle Q(I_{s})={\begin{cases}0,&{\text{if }}s\notin I;\\\mathbb {F} ,&{\text{otherwise}}\end{cases}}}
  
, where the linear maps are the identity map inside the interval. The module 
  
    
      
        Q
        (
        I
        )
      
    
    {\displaystyle Q(I)}
  
 is sometimes referred to as an interval module.
Then for any 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  
-indexed persistence module 
  
    
      
        M
      
    
    {\displaystyle M}
  
 of finite type, there exists a multiset 
  
    
      
        
          
            
              B
            
          
          
            M
          
        
      
    
    {\displaystyle {\mathcal {B}}_{M}}
  
 of intervals such that 
  
    
      
        M
        ≅
        
          ⨁
          
            I
            ∈
            
              
                
                  B
                
              
              
                M
              
            
          
        
        Q
        (
        I
        )
      
    
    {\displaystyle M\cong \bigoplus _{I\in {\mathcal {B}}_{M}}Q(I)}
  
, where the direct sum of persistence modules is carried out index-wise. The multiset 
  
    
      
        
          
            
              B
            
          
          
            M
          
        
      
    
    {\displaystyle {\mathcal {B}}_{M}}
  
 is called the barcode of 
  
    
      
        M
      
    
    {\displaystyle M}
  
, and it is unique up to a reordering of the intervals.
This result was extended to the case of pointwise finite-dimensional persistence modules indexed over an arbitrary totally-ordered set by William Crawley-Boevey and Magnus Botnan in 2020, building upon known results from the structure theorem for finitely generated modules over a PID, as well as the work of Cary Webb for the case of the integers.


== References ==

Title: Data physicalization
A data physicalization  (or simply physicalization) is a physical artefact whose geometry or material properties encode data. It has the main goals to engage people and to communicate data using computer-supported physical data representations.

History
Before the invention of computers and digital devices, the application of data physicalization already existed in ancient artifacts as a medium to represent abstract information. One example is Blombo ocher plaque which is estimated to be 70000 – 80000 years old. The geometric and iconographic shapes engraved at the surface of the artifact demonstrated the cognitive complexity of ancient humans. Moreover, since such representations were deliberately made and crafted, the evidences suggest that the geometric presentation of information is a popular methodology in the context of society. Although researchers still cannot decipher the specific type of information encoded in the artifact, there are several proposed interpretations. For example, the potential functions of the artifact are divided into four categories, categorized as "numerical", "functional", "cognitive", and "social". Later, at around 35,000 B.C, another artifact, the Lebombo bone, emerged and the encoded information became easier to read. There are around 29 distinct notches carved on the baboon fibula. It is estimated that the number of notches is closely related to the number of lunar cycles. Moreover, this early counting system was also regarded as the birth of calculation.
Right before the invention of writing, the clay token system was spread across ancient Mesopotamia. When the buyers and sellers want to make a trade, they prepare a set of tokens and seal them inside the clay envelope after impressing the shape on the surface. Such physical entity was widely used in trading, administrative documents, and agricultural settlement. Moreover, the token system is evidence of the early counting system. Each shape corresponds to a physical meaning such as the representation of "sheep", forming a one-to-one mapping relationship. The significance of the token is it uses physical shape to encode numerical information and it is regarded as the precursor of the early writing system. The logical reason is the two-dimension symbol would record the same information as the impression created by the clay token.
From 3000 BCE to the 17th century, a more complex visual encoding, Quipus, was developed and widely used in Andean South America. Knotted strings unrelated to quipu have also been used to record information by the ancient Chinese, Tibetans and Japanese. The ancient Inca empire used it for military and taxation purposes. The Base-10 logical-numerical system can record information based on the relative distance of knots, the color of the knots, and the type of knots. Due to the texture (cotton) of Quipus, very few of them survive. By analyzing those remaining artifacts, Erland Nordenskiöld proposed that Quipus is the only writing system used by Inca, and the information encoding technique is sophisticated and distinctive.
The idea of data physicalization become popular since the 17th century in which architects and engineers widely used such methods in civil engineering and city management. For example, from 1663 to 1867, Plan-relief model was used to visualize French territorial structure and important military units such as citadels and walled cities. Therefore, one of the functions of the Plan-relief model was to plan defense or offense. It is worth noting that the model can be categorized as a military technology and it did not encode any abstract information. The tradition of using tangible models to represent buildings and architectures still remains today.
One of the contemporary examples of data physicalization is the Galton board designed by Francis Galton who promoted the concept of Regression toward the mean. The Galton board, a very useful tool in approximating the Gaussian law of errors, consists of evenly spaced nails and vertical slats at the bottom of the board. After a large number of marbles are released, they will settle down at the bottom, forming the contour of a Bell Curve. Most marbles will agglomerate at the center (smaller deviation) with few on the edge of the board.
In 1935, three different electricity companies (e.g. Pacific Gas and Electric Company, Commonwealth Edison Company) created an electricity data physicalization model to visualize the power consumption of their customers so that the company can better forecast the upcoming power demand. The model has one short axis and one long axis. The short axis indicates "day", whereas the long axis spans the whole year. The viewers can gain perspective on when customers consume electricity the most during the day and how does the consumption change across different seasons. The model was built manually by cutting wooden sheets and stacked all pieces together.
Researchers began to realize that data physicalization models can not only help agents manage/plan certain tasks, but also can greatly simplify very complex problems by letting users manipulate data in the real world. Therefore, from an epistemic perspective, physical manipulation enables users to uncover hidden patterns that cannot be easily detected. Max Perutz received Nobel Prize in Chemistry in 1962 for his distinguished work in discovering the structure of the globular protein. When a narrow X-ray passes through the haemoglobin molecule, the diffraction pattern can review the inner structure of the atomic arrangements. One of Perutz's works within this research involved creating a physicalized haemoglobin molecule which enables him to manipulate and inspect the structure in a tangible way.
In the book, Bertin designed a matrices visualization device called Domino which let users manipulate row and column data. The combination of row and column can be considered as a two-dimensional data space. In Semiology of Graphics, Bertain defined what variables can be reordered and what variables cannot. For example, time can be considered as a one direction variable. We should keep it in a natural order. Compared with the aforementioned work, this model emphasized the visual thinking aspect of data physicalization and supports a variety of data types such as maps, matrices, and timelines. By adjusting the data entries, an analyst can find patterns inside the datasets and repeatedly use Domino on different datasets.
More recent physicalization examples include using LEGO bricks to keep track of project progress. For example, people used LEGO to record their thesis writing progress. Users can use the LEGO board to set concrete steps before pushing to real publications such as data analysis, data collection, development, etc. Another application involves using LEGO in bug tracking. For software engineers, keeping track of the issue of the code base is a crucial task and LEGO simplify this progress by physicalize the issues.
A specific application of data physicalization involves building tactile maps for visually impaired people. Past example include using microcapsule paper to build tactile maps. With the help of digital fabrication tool such as laser cutter, researchers in Fab Lab at RWTH Aachen University has used it to produce relief-based a tactile map to support visually impaired users. Some tangible user interface researchers combined TUI with tactile maps to render dynamic rendering and enhance collaboration between vision impaired people (e.g. FluxMarkers).


== References ==

Title: Social data science
Social data science is an interdisciplinary field that addresses social science problems by applying or designing computational and digital methods. As the name implies, Social Data Science is located primarily within the social science, but it relies on technical advances in fields like data science, network science, and computer science. The data in Social Data Science is always about human beings and derives from social phenomena, and it could be structured data (e.g. surveys) or unstructured data (e.g. digital footprints). The goal of Social Data Science is to yield new knowledge about social networks, human behavior, cultural ideas and political ideologies.
A social data scientist combines domain knowledge and specialized theories from the social sciences with programming, statistical and other data analysis skills.

Methods
Social data science employs a wide range of quantitative - both established methods in social science as well as new methods developed in computer science and interdisciplinary data science fields such as natural language processing (NLP) and network science.
Social Data Science is closely related to Computational Social Science, but also sometimes includes qualitative data, and mixed digital methods.
Common social data science methods include:
Quantitative methods:

Machine learning
Deep learning
Social network analysis
Randomized controlled trials
Natural language processing (NLP), especially through text as data.
surveys
Qualitative methods:

Interviewing
Observation
Ethnography
Content analysis
Discourse analysis
Mixed digital methods:

Controversy mapping
Spatial analysis
Quali-quantitative methods 
Computational ethnography
One of the pillars of social data science is in the combination of qualitative and quantitative data to analyze social phenomena and develop computationally grounded theories. For example, by using mixed methods to digitize qualitative data and analyzing it via computational methods, or by qualitatively analyzing and interpreting quantitative data.

Data
Social data scientists use both digitized data  (e.g. old books that have been digitized) and natively digital data (e.g. social media posts). Since such data often take the form of found data that were originally produced for other purposes (commercial, governance, etc.) than research, data scraping, cleaning and other forms of preprocessing and data mining occupy a substantial part of a social data scientist's job.
Sources of SDS data include:

Text data
Sensor data
Register data
Survey data
Geo-location data
Observational data

Relations to other fields
Social sciences
Social data science is part of the social sciences along with established disciplines (anthropology, economics, political science, psychology, and sociology) and newer interdisciplinary fields like behavioral science, criminology, international relations, and cognitive science. As such, its fundamental unit of study is social relations, human behavior and cultural ideas, which it investigates by using quantitative and/or qualitative data and methods to develop, test and improve fundamental theories concerning the nature of the human condition. SDS also differs from traditional social science in two ways.

First, its primary object is digitized phenomena and data in the widest sense of this word, ranging from digitized text corpora to the footprints gathered by digital platforms and sensors.
Secondly, more than simply applying existing quantitative and qualitative social science methods, social data science seeks to develop and disrupt these via the import and integration of state of the art of data science techniques

Data Science
Social data science is a form of data science in that it applies advanced computational methods and statistics to gain information and insights from data. Social data science researchers often make use of methods developed by data scientists, such as data mining and machine learning, which includes but is not limited to the extraction and processing of information from big data sources. Unlike the broader field of data science, which involves any application and study involving the combination of computational and statistical methods, social data science mainly concerns the scientific study of digital social data and/or digital footprints from human behavior.

Computational Social Science
Like computational social science, social data science uses data science methods to solve social science problems. This includes the reappropriation and refinement of methods developed by data scientists to better fit the questions and data of the social sciences as well as their specialized domain knowledge and theories. Unlike computational social science, social data science also includes critical studies of how digital platforms and computational processes affect wider society and of how computational and non-computational approaches integrate and combine.

Digital Methods
While most social data science researchers are closely affiliated with or part of computational social science, some qualitative oriented social data scientists are influenced by the fields of digital humanities and digital methods that emerged from science and technology studies (STS). Like digital methods, the aim is here to repurpose the 'methods of the medium' to study digitally-mediated society and to engage in an ongoing discussions about bias in science and society by bringing computational social science and Digital Methods into dialogue. SDS is also related to digital sociology and digital anthropology, but to a higher degree aspires to augment qualitative data and digital methods with state of the art data science techniques.

History of the field
The origin of term "social data science" coincided with the emergence of a number of research centers and degree programs. In 2016, the Copenhagen Center for Social Data Science (SODAS) - the first academic institution using the SDS name - was launched at the University of Copenhagen. The plan for an interdisciplinary center working at the intersection of the social and computational sciences was rooted in the Copenhagen Networks Study  from 2011 to 2016 by researchers from the Technical University of Denmark (DTU) and the University of Copenhagen. The University of Oxford and the University of Copenhagen were among the first research institutions to offer degree programmes in SDS. In 2018, the University of Oxford launched the one-year MSc in Social Data Science,  which was followed by the two-year master's programme at the University of Copenhagen in 2020. Since then, an increasing number of universities have begun to offer graduate programs or specializations in social data science
Social data science has emerged after the increasing availability of digitized social data, sometimes referred to as Big Data, and the ability to apply computational methods to this data at a low cost, which has offered novel opportunities to  address questions about social phenomena and human behavior. While the origin of social data can be traced back to 1890s (when some 15 million individual records were processed by the US Census in the form of punch cards), the social data boom in the 21th century is a direct consequence of the increasing availability of consumer data resulting from the advent of e-commerce Subsequent waves of availability of unstructured social data include Amazon.com review system and Wikipedia, and more recently, social media, which has played a key role in the emergence of the digital attention economy and big tech.

Criticism and debates
Data scientists have played a vital role in the data revolution, both during the original tech-optimist phase where big data and the Internet was seen as the solution to many societal and scientific problems, and as participants  in the tech-lash that followed in its wake as result of, among other things, the Facebook–Cambridge Analytica data scandal. Social data science researchers and research projects have been especially impactful in debates and criticism revolving around:

Surveillance capitalism
Digital disinformation
Algorithmic bias
The replication and validity crisis on the social sciences 
Ethics and privacy
Data governance

Impact and examples
Social data science research is typically published in multidisciplinary journals, including top general journals Science, Nature, and PNAS, as well as notable specialized journals such as:

Nature Human Behaviour
Nature Computational Science
The Journal of Computational Social Science
Big Data and Society
Science Advances
Nature Communications
Scientific Reports
PLOS ONE
In addition, social data science research is published in the top social science field journals including American Sociological Review, Psychological Science, American Economic Review, Current Anthropology.

Education and Research Institutions
There are multiple specific definitions of social data science, but several institutions around the world currently offer degree and research programs under the rubric of Social Data Science.

Education
M.Sc. in Social Data Science  - University of Copenhagen
MSc in Social Data Science  - University of Oxford
MSc in Social and Economic Data Science (SEDS)  - University of Konstanz
BSc in Social Data Science  - University of Hong Kong
P.Grad.Dip in Social Data Science  - University of Dublin
MSc Applied Social Data Science  - The London School of Economics
Master of Science in Social Data Science  - Central European University
MSc Social Data Science  - University of Essex
MSc in Techno-Anthropology  - University of Aalborg
MSc Social Data Science  - University College Dublin
BSc in Social Data Science  - Witten/Herdecke University
Quantitative Analysis and Social data Science (QASS)  - KU Leuven
Human and Social Data Science MSc  - University of Sussex

Research
Copenhagen Center for Social Data Science (SODAS)  - University of Copenhagen
Center for Social Data Science  - University of Helsinki
Social Data Science Lab  - Cardiff University
SoDa Laboratories  - Monash University
Mannheim Center for Data Science  - University of Mannheim
Social & Behavioral Data Science Centre (SoBe DSC)  - University of Amsterdam
Social Data Science  - Alan Turing Institute
Social Data Science Center  - University of Maryland
Centre for Social Data Analytics  - Auckland University of Technology
MASSHINE  – Aalborg University

Professions and industry
Social data scientists are in high demand across society, specifically for employers valuing interdisciplinary skills, and can be found working as:

1. Industry Researchers: Typical workplaces: governments, companies and corporations, independent research institutes, foundations, NGOs. Typical titles: researcher, data manager, data steward, data scientist, data engineer, consultant, manager, director, partner, politicians, data analyst, software developer, BI, UX, UI.Researcher
2. Academic Researchers: Ph.D. Students, Researchers, Postdocs, Professors
3. Entrepreneurs: Start your own business using social data science methods to solve real-world social problems. Typical titles: CTO, CEO, Chief Data Scientist

Sub Branches
Social data science is still a new field, with developing branches. Broadly speaking the field can be divided into a range of method-based sub-fields:

Method-based sub-fields
Network Science: Network analysis is often utilized to visualize or study network dynamics in social data science studies. This includes for instance social media networks.
Mixed Digital Methods: In computer-assisted qualitative analysis, the researcher often utilizes computational methods such as natural language processing techniques or topic modelling to explore a corpus of text, such as parliamentary speeches or Twitter data.
Machine Learning for Causal Inference: The social sciences are often interested in finding causal relationships between variables. This is of special interest to social data science, where multiple fields of research try to establish appropriate policy responses to contemporary societal issues. Often, drawing from research from Judea Pearls directed acyclical graph approach and the Neymann-Rubin Causal model to inform whether there exists a causal relationship between two (or more) variables. Furthermore, incorporating machine learning into causal inference is of great interest.
Natural Language Processing: Applied natural language processing is the adaptation and repurposing of methods from natural language processing and the application of these methods to questions of social behavior.

Themes
Algorithmic Bias and Fairness: Considering how algorithms play a still larger role in humans everyday life, the study of fairness in this context has grown as a field. Especially whether miniorieties are negatively or positively impacted by these algorithms.
Polarization and Misinformation: Many scholars use enormous amounts of granular data generated by social media and political agents to study social contagion, the spread of misinformation and disinformation. These studies often use text or social media interactions to explore how politicians and/or the public behave and interact in the digital and physical arena.
Climate Social Data Science: The intersection between climate science, and digital (behavioral) data. This includes climate activism on social media and using digital trace data to investigate how people and societies are impacted by rising temperatures (CITE: Rising Temperature Erode Human Sleep Globally).


== References ==

Title: Statistical thinking
Statistical thinking is a tool for process analysis of phenomena in relatively simple terms, while also providing a level of uncertainty surrounding it. It is worth nothing that "statistical thinking" is not the same as "quantitative literacy", although there is overlap in interpreting numbers and data visualizations.
Statistical thinking relates processes and statistics, and is based on the following principles:

All work occurs in a system of interconnected processes.
Variation exists in all processes
Understanding and reducing variation are keys to success.

History
W. Edwards Deming promoted the concepts of statistical thinking, using two powerful experiments:
1. The Red Bead experiment, in which workers are tasked with running a more or less random procedure, yet the lowest "performing" workers are fired. The experiment demonstrates how the natural variability in a process can dwarf the contribution of individual workers' talent.
2. The Funnel experiment, again demonstrating that natural variability in a process can loom larger than it ought to.
The take home message from the experiments is that before management adjusts a process—such as by firing seemingly underperforming employees, or by making physical changes to an apparatus—they should consider all sources of variation in the process that led to the performance outcome.
Nigel Marriott breaks down the evolution of statistical thinking.

Benchmarks
Statistical thinking is thought to help in different contexts, such as the courtroom, biology labs, and children growing up surrounded by data.
The American Statistical Association (ASA) has laid out what it means to be "statistically educated". Here is a subset of concepts for students to know, that:

data beat anecdotes
data is natural, predictable, and quantifiable
random sampling allows results of surveys and experiments to be extrapolated to the population
random assignment in comparative experiments allows cause-and-effect conclusions to be drawn
to know association is not causation
significance does not necessarily imply practical importance, especially for studies with large sample sizes
no statistically significant difference or relationship does not necessarily mean there is no difference or no relationship in the population, especially for studies with small sample sizes
Statistical thinking is a recognized method used as part of Six Sigma methodologies.

See also
Systems thinking
Evidence-based practice
Analytical thinking
Critical thinking
Computational thinking
Data thinking


== References ==

Title: Toy problem
In scientific disciplines, a toy problem or a puzzlelike problem is a problem that is not of immediate scientific interest, yet is used as an expository device to illustrate a trait that may be shared by other, more complicated, instances of the problem, or as a way to explain a particular, more general, problem solving technique. A toy problem is useful to test and demonstrate methodologies. Researchers can use toy problems to compare the performance of different algorithms. They are also good for game designing.
For instance, while engineering a large system, the large problem is often broken down into many smaller toy problems which have been well understood in detail. Often these problems distill a few important aspects of complicated problems so that they can be studied in isolation. Toy problems are thus often very useful in providing intuition about specific phenomena in more complicated problems.
As an example, in the field of artificial intelligence, classical puzzles, games and problems are often used as toy problems. These include sliding-block puzzles, N-Queens problem, missionaries and cannibals problem, tic-tac-toe, chess, Tower of Hanoi and others.

See also
Blocks world
Firing squad synchronization problem
Monkey and banana problem
Secretary problem

References
External links
"toy problem". The Jargon Lexicon.

Title: Toyota Connected North America
Toyota Connected North America is an information technology and data science company, founded in 2016 and based in Plano, Texas. Toyota Connected is a subsidiary of Toyota Motor Corporation, founded in partnership with Microsoft, and is involved in research and the development of technology for in-vehicle services, telematics and other forms of artificial intelligence.

History
Toyota Connected North America was officially established in April 2016 in Plano, Texas, where the U.S. headquarters of Toyota Motor North America were also hosted; the company's foundation was intended as an expansion of Toyota’s partnership with Microsoft, which had originally begun in 2011 with the goal of building a platform for telematics services. As part of the venture, Microsoft had a 5 percent stake in Toyota Connected, and provided the company with access to their cloud-based platform Azure, in order to develop or expand wireless services suited to track driving patterns, share information on traffic and weather conditions and monitoring personal health data, among other functions. Toyota Connected also announced it would support research projects on artificial intelligence, while simultaneously focusing on the analysis of data from vehicle sensors and cameras to develop algorithms for self-driving cars.
At inception, Toyota invested an estimated $5.5 million in Toyota Connected; Zach Hicks, who had previously served as the chief digital officer at Toyota Motor North America, was appointed as the company’s first CEO.
In July 2022, Hicks stepped down from his role as CEO and president of Toyota Connected following his retirement, being subsequently replaced by Steve Basra. In August 2023, former Toyota Connected Executive Vice President and Chief Operating Officer Christopher Yang was appointed as the company's new CEO, after Basra had left both the role and the company in June of the same year.
At the 2023 New York International Auto Show, Toyota Connected debuted its proprietary generative artificial intelligence tool to produce photorealistic images through text input.


== References ==

Title: Data universalism
Data universalism is an epistemological framework that assumes a single universal narrative of any dataset without any consideration of geographical borders and social contexts. This assumption is enabled by a generalized approach in data collection. Data are used in universal endeavours across social, political, and physical sciences unrestricted from their local source and people. Data are gathered and transformed into a mutual understanding of knowing the world which forms theories of knowledge. One of many fields of critical data studies explores the geologies and histories of data by investigating data assemblages and tracing data lineage which unfolds data histories and geographies (p. 35). This reveals intersections of data politics, praxes, and powers at play which challenges data universalism as a misguided concept.

Theoretical framework
Data are mainly sampled in rich Western countries which are considered the leaders and voices of technological developments while ignoring cultures, communities, and geographies despite its application being widespread. As the data lifecycle grows, processed small data that are grounded within big data are compiled and formed from heterogeneous sources extracted from mainstream places, forming what has come to be understood as knowledge.
As of 2022, research has not shown the origin behind universalism as a practice due to a lack of controlled data. According to cultural psychologists, democracy and universalism have a positive correlation but there are no studies that show how universalism is shaped by people's experiences and environments (p. 1). A push toward datafication has been spurred by democratically advanced Western voices and diffused across fragile democracies in the Global South with no consideration to the geopolitical context and influence powers of the data landscape in countries outside the West.  As mentioned by Cappelen et. al, obscure information is found on the epistemology of universalism, although it is argued that a lack of representative data are problematic for broad global analyses (p. 1).

Criticism
Data universalism has been critiqued by many scholars concerned about data privacy and data justice, claiming that it conceals cultural specifications and diversity. Datafication ought to be viewed through the lens of epistemic diversity and justice to achieve data obedience. So, people are encouraged to critically examine the impacts of datafication by reimagining people and places.

De-westernization
Milan and Treré have contended that datafication as a privileged practice carried by dominant Western democracies that fail to see the richness of worldviews and meanings of the South. As promoted by Global South and Indigenous scholars, data universalism mistakenly assumes data to be universal when it ought to be treated differently (p. 323). If data are extracted without an ethical foundation grounded by attitude and method, the data becomes pervasive and incompetent. Moreover, a shift from a technocentric perspective that emphasizes the human agency behind data to a data method that stimulate discussion about the repercussions of datafication requires relocating the agency. This reinforces the notions of ethics and responsibilities around the data (p. 327). A push for bottom-up data practices shifts the focus of datafication to data justice to encourage citizens of the South to participate in political agency and repel an oppressed and inequal datafication process. Negligence in abstracting knowledge from others through diversifying social and historical contexts will result in biased sampling techniques and methods in data generation. This causes skewed data to be generated leading to unequal datafication.
Also, an aggregate technique used for data processing misrepresents data in a way that shapes an aggregate value as representing an aggregated individual. This adheres to the bias factor in making the subject appear as a collective by reducing variance and limiting space of contributions (p. 192). Using an aggregate technique submits to universal normativity which situates what is considered to be universally right as a practice that does not take responsibility for the context of a specific situation nor the interpretation of norms. Similarly, the interpretation of norms are also subject to contextual interpretations. While humans operate in unique cases where information can be incomplete, agency empowers humans to assess the situation and make radical decisions in complex situations where information is obscure. Thus, assuming universal normativity will not only incapacitate one's ethical validity when making choices but may lead to questionable decisions (p. 284).

Global South
Historical processes of global capitalism and colonialism have majorly impacted the supply of knowledge from Western modernity and subordinated knowledge from the Global South. Colonialism, in this perspective, is understood as data colonialism which pressures but also exploits datafication on communities. Global South and Indigenous scholars claim that the decolonial lens which transcends to a Eurocentric perspective adds value to critical data studies by questioning the geopolitics of knowledge, the depth of knowledge regeneration, and the power constructions of past injustices. Notions of data politics and data justice are more interested in giving a voice to the underprivileged and acquiring decolonial practices rather than issues concerning the blueprints of the political and social contexts of liberal democracies and social orders. Still, achieving decolonial critical data studies comes with a unique set of challenges that confronts the knowledge that has been produced and the knowings of the world, which is at the center of epistemology. The wave of the big data revolution feeds on insights into the production of data, how knowledge is produced, and how it is conducted and governed while using new epistemologies to make sense of the world.

See also
Big data
Colonialism
Critical data studies
Data
Global South


== References ==

Title: Ilkay Altintas
Ilkay Altintas is a Turkish-American data and computer scientist, and researcher in the domain of supercomputing and high-performance computing applications. Since 2015, Altintas has served as chief data science officer of the San Diego Supercomputer Center (SDSC), at the University of California, San Diego (UCSD), where she has also served as founder and director of the Workflows for Data Science Center of Excellence (WorDS) since 2014, as well as founder and director of the WIFIRE lab. Altintas is also the co-initiator of the Kepler scientific workflow system, an open-source platform that endows research scientists with the ability to readily collaborate, share, and design scientific workflows.
Born in Aydın, Turkey, Altintas attended Middle East Technical University before embarking on a research career. While pursuing her career as a research scientist, she completed her PhD at the University of Amsterdam in 2011. In addition to her work as a research scientist, she is a computer science lecturer at the University of California, San Diego. Altintas is also a co-founder and board member of the Data Science Alliance nonprofit. She serves on the advisory board of several national agencies and companies, and as an editor of peer-reviewed scientific journals.

Education
Altintas graduated from Middle East Technical University in Ankara, Turkey with a bachelor's degree in computer engineering in 1999, and a master's degree in computer engineering in 2001. In 2011, she received a PhD in computational science from the University of Amsterdam in the Netherlands, for her work and contributions toward workflow-driven collaborative science.

Career and research
After graduating from Middle East Technical University in 2001, Altintas was hired as a research programmer at the San Diego Supercomputer Center (SDSC). She went on to serve SDSC as the assistant director of the National Laboratory for Advanced Data Research (NLADR), the founder and director of the Scientific Workflow Automation Technologies Laboratory, and deputy coordinator for research. Currently, she is the chief data science officer of SDSC, where she is also the founder and director of the Workflows for Data Science Center of Excellence (WorDS); the founder and director of the WIFIRE lab; and the division director for Cyberinfrastructure Research, Education and Development. The WorDS hub at SDSC serves to promote the utilization and distribution of workflow services in the context of a variety of domains and projects, ranging from the WIFIRE lab (which Altintas founded and currently directs) to the Kepler scientific workflow system (which Altintas serves as the co-initiator of).

WIFIRE Lab
On October 26, 2003, Altintas experienced California wildfires for the first time, after witnessing the Cedar Fire in San Diego County. The environmental and economic toll that this natural disaster had on the residents of San Diego County inspired Altintas to contribute to improving the then-current systems of managing and predicting wildfires. She allotted much of her time thereafter to learning about California's fire-adapted ecosystem, and the two factors, weather and fuel, that contribute significantly to the spread and dynamic behavior of wildfires. She noted that the behavior of wildfires can be computationally modeled, with data for modeling purposes being pulled from the physical environment (e.g., landscape data, real-time weather information and camera imagery, and remote sensor data). This spurred the creation of Altintas' brainchild, the WIFIRE lab, which received National Science Foundation (NSF) funding from 2013 to 2018. Using scalable computing and dynamic, automated, scientific workflows, data are collected and processed, and can be retrieved and accessed by a diverse array of audiences. The main accomplishments of the WIFIRE lab till date include the deployment of data integration services at the intersection of fire science, data management, and machine learning; real-time data driven fire modeling services (with the help of the High Performance Wireless Research and Education Network (HPWREN)); and automated fire modeling workflows and machine learning pipelines. The WIFIRE lab currently serves a community of 130+ organizations, and works to combine data science and fire science to aid in dynamic fire modeling at scale. Altintas has presented on past and ongoing WIFIRE research to a variety of audiences, ranging from stakeholders and policymakers at the California State Capitol to UC San Diego School of Global Policy and Strategy (GPS) Science Policy Fellows Program graduate students.
The real-time fire modeling services provided by the WIFIRE lab have aided in the identification and traceability of wildfire outbreaks in local California communities. In addition to aiding in the identification and traceability of wildfire outbreaks, the framework of the services deployed by WIFIRE is being used toward aiding in the identification and traceability of the COVID-19 outbreak. The application of the real-time map (used initially for wildfire identification) in the space of COVID-19 research serves the purpose of alerting individuals in real-time regarding information surrounding the spread and severity of the virus in certain areas.
For her pioneering WIFIRE work, Altintas has been featured on various US-based radio broadcasts, newspapers, and magazines. She has been featured on TIME magazine, the National Public Radio (NPR), The New York Times, and Voice of America, along with research scientists whom she has collaborated with. She has also been featured on the University of California Television (UCTV) channel for her WIFIRE research.

The Kepler Project
Altintas is the co-initiator of the Kepler scientific workflow system, an open source platform that allows its users to share and collaborate on workflows for a variety of applications. The Kepler Project website provides its visitors with the opportunity to download the Kepler application. It also serves as a home for information regarding upcoming Kepler-related workshops, references on how to use Kepler for a varied set of subjects and applications (such as bioinformatics), and tips regarding how to maneuver the application in an effective manner.

Teaching
Altintas currently serves as the faculty co-director of the Master of Advanced Studies in Data Science and Engineering, and as a lecturer in the Department of Computer Science and Engineering, at UC San Diego. In addition, she is a big data MOOC instructor through Coursera and edX, having taught over a million learners till date.

Awards and honors
White House Office of Science and Technology Policy Invited Workshop Speaker (2016)
Halıcıoğlu Data Science Institute (HDSI) Fellow, Inducted by invitation, University of California, San Diego (2018)
Pi Person of the Year, Inaugural Recipient, San Diego Supercomputer Center (SDSC) (2014)
Innovations in Networking Award for Experimental Applications, Corporation for Education Network Initiatives in California (CENIC) (2018)
Emerging Woman Leader in Technical Computing Award, Association for Computing Machinery's Special Interest Group on High Performance Computing (ACM SIGHPC) (2017)
Peter Chen Big Data Young Researcher Award, The Services Society (2017)
Award for Excellence for Early Career Researchers, Institute of Electrical and Electronics Engineers (IEEE), Technical Committee on Scalable Computing (TCSC) (2015)
Best Workshop Paper Award for "Towards an Integrated Cyberinfrastructure for Scalable Data-Driven Monitoring, Dynamic Prediction and Resilience of Wildfires," International Conference on Computational Science (ICCS), Reykjavík, Iceland (2015)
Editor's Choice Awards, HPCwire (2014)
Best Application of Big Data in HPC
Best Data-Intensive System (End User focused)
Reader's Choice Award for Best Application of Big Data in HPC, HPCwire (2014)

See also
List of female scientists in the 21st century
List of Middle East Technical University alumni
List of University of Amsterdam alumni

References
External links
Ilkay Altintas at San Diego Supercomputer Center

Title: Mariola Grażyna Antczak
Mariola Grażyna Antczak (born Mariola Grażyna Birecka; 14 January 1970 in Gdańsk) is a Polish bibliologist and informatologist, associate professor and head of the Department of Information, Library and Book Studies at University of Łódź.

Education
Mariola Antczak received an MA in library and information science at the Faculty of Philology of University of Łódź in 1994, Ph.D. in bibliology in 1999. Her Habilitation (D.Sc.) in library and information science was completed at the Faculty of Philology of the University of Silesia in 2012, the thesis title being Rola bibliotek i bibliotekarzy szkolnych w edukacji społeczeństwa informacyjnego na tle przeobrażeń oświatowych w Polsce w latach 1989-2007.

Career and research
Since 2015, she serves as head of the Department of Information, Library and Book Studies at University of Łódź.
In 1994–2000, she worked as a teacher-librarian at Primary School No. 99 in Łódź and then, in 2000–2001, as a teacher-librarian at Junior High School No. 31 in Łódź. Her academic career started in 2001 when she became an assistant professor at the Department of Information, Library and Book Studies of the University of Łódź. Currently, Mariola Antczak is an associate professor there.
Antczak is a specialist in school libraries with particular emphasis on the didactic work of teacher-librarians and library organization and management. She is also interested in readership, media, and information education, as well as library marketing, information society, information literacy, reading culture of children and youth, libraries in Finland, and drama as a didactic method.

Membership and service
Polskie Towarzystwo Komunikacji Społecznej: Wrocław, dolnośląskie voivodeship, Poland; 2022 to present, since 25 February 2022 head of section "Book and information culture"
Łódzkie Towarzystwo Naukowe: Łódź, łódzkie voivodeship, Poland; 2012 to present

Awards and honours
Antczak received the "Librarian of the Year" award from the Polish School Librarian Association (1998), two awards from the president of the University of Łódź (2004, 2005), and a Voivodeship marshal award (2019).

Selected publications
Antczak, Mariola, Czapnik, Grzegorz, ed. (2022). Organizacja konferencji naukowych. Łódź: Wydawnictwo Uniwersytetu Łódzkiego. ISBN 978-83-822-0902-0.
Antczak, Mariola, Wachowicz, Monika (2022). Assessment of the COVID-19 Pandemic Impact on the Changes in the Operation and Structure of Polish Voivodeship Pedagogical Libraries. Polish Libraries 10, page 201–226.
Antczak, Mariola, Gruszka, Zbigniew (2022). Academic Library Research Development in the Context of if of Journals: Bibliometric Analysis of Articles Based on the Lista Database (2000-2019). Przegląd Biblioteczny 4, page 467–486.
Antczak, Mariola, Gruszka, Zbigniew, ed. (2020). Interdyscyplinarium nauk o mediach i kulturze. Łódź: Wydawnictwo Uniwersytetu Łódzkiego. ISBN 978-83-8142-727-2.
Antczak, Mariola, Gruszka, Zbigniew, Czapnik, Grzegorz, ed. (2020). Łódzkie biblioteki publiczne. Czas zmian, czas wyzwań. Łódź: Wydawnictwo Uniwersytetu Łódzkiego. ISBN 978-83-8142-668-8.
Antczak, Mariola, Kalińska-Kula, Magdalena (2020). Biblioteczny marketing wewnętrzny w teorii i praktyce. Łódź: Wydawnictwo Uniwersytetu Łódzkiego. ISBN 978-83-8142-335-9.
Antczak, Mariola (2019). Marketing biblioteczny. In: Zarządzanie biblioteką, Wojciechowska, Maja, ed., Warszawa: Wydawnictwo Naukowe i Edukacyjne SBP, page 521-540. ISBN 978-83-65741-26-4.
Antczak, Mariola, Przybysz-Stawska, Magdalena, ed. (2018). Słownik członków ŁTN 2010-2015. Łódź: Wydawnictwo Uniwersytetu Łódzkiego, Wydawnictwo Łódzkiego Towarzystwa Naukowego. ISBN 978-83-8142-249-9.
Antczak, Mariola (2016). Współczesne trendy w edukacji przyszłych nauczycieli bibliotekarzy na przykładzie Katedry Bibliotekoznawstwa i Informacji Naukowej Uniwersytetu Łódzkiego. Folia Librorum. Acta Universitatis Lodziensis 1–2, page 81–106.
Antczak, Mariola, Walczak-Niewiadomska, Agata, ed. (2015). Biblioteki i książki w życiu nastolatków. Łódź, Warszawa: Wydawnictwo Uniwersytetu Łódzkiego, Wydawnictwo SBP. ISBN 978-83-64203-45-9.
Antczak, Mariola, Walczak-Niewiadomska, Agata, ed. (2015). Książki w życiu najmłodszych. Łódź, Warszawa: Wydawnictwo Uniwersytetu Łódzkiego, Wydawnictwo SBP. ISBN 978-83-64203-47-3.
Antczak, Mariola, Walczak-Niewiadomska, Agata, ed. (2015). W kręgu kultury czytelniczej dzieci i młodzieży. Łódź, Warszawa: Wydawnictwo Uniwersytetu Łódzkiego, Wydawnictwo SBP. ISBN 978-83-64203-46-6.
Antczak, Mariola (2014). A comparison of selected aspects of Finnish and Polish public libraries. Folia Scandynavica Poznaniensia 16, page 114–130.
Walczak-Niewiadomska, Agata, Brzuska-Kępa, Alina, Antczak, Mariola, ed. (2013). Media a czytelnicy. Studia o popularyzacji czytelnictwa i uczestnictwie kulturowym młodego pokolenia. Łódź: Wydawnictwo Uniwersytetu Łódzkiego. ISBN 978-83-7525-990-2.
Antczak, Mariola (2010). Rola bibliotek i bibliotekarzy szkolnych w edukacji społeczeństwa informacyjnego na tle przeobrażeń oświatowych w Polsce w latach 1989-2007. Łódź: Wydawnictwo Uniwersytetu Łódzkiego. ISBN 978-83-7525-381-8.
Antczak, Mariola (2010). Rola bibliotekarzy w nauczaniu umiejętności informacyjnych gimnazjalistów: wybrane zagadnienia. Przegląd Biblioteczny 78 (1), page 58–71.
Antczak, Mariola (2010). Biblioteka szkolna a przygotowywanie uczniów do samokształcenia w świetle aktów legislacyjnych z lat 1945-1982. Folia Librorum 15, page 207–237.
Antczak, Mariola (2004). Techniki dramy w teorii i praktyce nie tylko dla nauczycieli bibliotekarzy. Warszawa: "Sukurs". ISBN 978-83-904579-8-7.

References
External links
Mariola Antczak on ORCID
Mariola Antczak on ResearchGate
Mariola Antczak on Google Scholar

Title: Shlomo Argamon
Shlomo Argamon (born 1967) is an American/Israeli computer scientist and forensic linguist. He is the associate provost for artificial intelligence and professor of computer science at Touro University.

Education
Shlomo Argamon received his B.S. in applied mathematics from Carnegie-Mellon University and his MPhil and Ph.D. in computer science from Yale University, supervised by Drew McDermott. He spent two years doing postdoctoral research under a Fulbright Foundation fellowship with Sarit Kraus at Bar-Ilan University in Ramat Gan, Israel.

Research
Since the late 1990s, Argamon has worked primarily on computational linguistics and machine learning, focusing on the analysis of non-denotational meaning, including computational analysis of language stylistics, sentiment analysis, and metaphor analysis. He has also published well-cited research on active learning (machine learning), metalearning, and robotic mapping.

Computational stylistics
Argamon is best known for his work on computational stylistics, particularly author profiling. Together with Moshe Koppel and others, he has shown how statistical analysis of word usage can determine an author's age, sex, native language, and personality type with high accuracy in English-language texts. His work also has shown how textual features indicating differences between male and female authorship are consistent between languages and across time.
Argamon also developed computational stylistic methods that provide insights into the meaning of stylistic differences. One of his key innovations for this purpose is the development of computational stylistic analysis using systemic functional linguistics. For example, together with Jeff Dodick and Paul Chase, he examined whether there are clear and consistent differences between scientific method in experimental sciences and historical sciences. Their work showed how using systemic functional features in computational stylistic analysis provides evidence for multiple scientific methodologies of the sorts posited previously by philosophers of science.

Linguistics for cybersecurity
Argamon has pushed for the increased use of linguistic analysis for attribution of cybersecurity attacks. He has pointed out how linguistic attribution techniques can be used to good effect on natural language texts that arise in different attack scenarios, and has provided analyses for high-profile cases such as the Sony Pictures hack, the Democratic National Committee cyber attacks, and the Shadow Brokers NSA leak.

Data science
In 2013, Argamon founded the Illinois Institute of Technology Master of Data Science program, which he directed until 2019. The program seeks to teach students "to think about the real problems that need to be solved, not to simply find technical solutions." Argamon views data scientists as "sensemakers", whose job is not merely to produce analytic results, but to help their clients make sense of a complex, uncertain, and fast-changing world through rigorous analysis and explanation of the data.

Honors
Fellow of the British Computer Society.
Distinguished Lecturer in Forensic Linguistics, Centre for Forensic Linguistics, Aston University, 2014.
Fulbright Foundation Postdoctoral Fellow, 1994–1996.
Hertz Foundation Doctoral Fellow, 1990–1994.


== References ==

Title: Shaku Atre
Shakuntala "Shaku" Atre (1940 - 2024) is an Indian data scientist and an American business woman. After a fourteen-year career with IBM, she began her own firm and became widely regarded as an expert on business technology and database use. Atre is best known for her books Database: Structured Techniques for Design, Performance and Management: With Case Studies (1980), one of the first books written on managing databases, and her co-authored book Business Intelligence Roadmap, written with Larissa Moss. She has served as an adjunct professor of data science at University of Pune and at several institutions in the United States. Her works have been used as university textbooks.

Early life and education
Atre was born in 1940 in India into a Marathi family, and grew up in Panvel, a village near Mumbai. She earned a Master of Science from the University of Poona, studying mathematics and statistics. During her education, she also studied languages, gaining proficiency in speaking five different languages. Atre went on to begin her graduate studies in mathematics and physics at the University of Heidelberg, writing her thesis on astronomy. Because she needed to make large scale calculations to develop her thesis, she began studying computer programming, which led to an interest in the field of computer technology. During her studies, she was hired by IBM in 1967. When her student visa expired three and a half years later, Atre and her husband immigrated to the United States.

Career
When they arrived in New York City in 1971, IBM had implemented a hiring freeze, but Atre was able to convince them she was not a new hire because she had worked for the company in Germany. They put her on staff as a systems programmer and she worked for the company for the next ten years, moving up the ladder from installation and technical support, to trainer in IBM's Systems Institute, to branch office systems engineer and finally, as a program manager for international product releases. During her tenure at IBM, from 1977 to 1981, she served as one of the referees for the company’s Systems Journal, selecting which articles would be peer-reviewed for the publication. In 1980, she published Data Base: Structured Techniques for Design, Performance, and Management with Case Studies, which would be used as a university textbook. It was translated into several different languages and had sold more than 150,000 copies within its first two decades of publication. During this period, she also began teaching as an adjunct professor at various universities, including the Polytechnic Institute of New York.
In the last quarter of 1981, Atre left IBM and began operating her own business consulting firm and working part-time managing long-distance telephone databases for AT&T. The following year, she began writing articles for Computerworld, which in 2003 would become a regular column for the magazine. Her book Data Base Management Systems for the Eighties was published in 1983, and provided a discussion of eight different data systems, comparing features and business applications to assist readers in analyzing which systems best met their needs. From 1984, her expert opinion of various data systems was regularly sought by trade magazines, such as Infosystems, MIS Week, Network World, and Software News, among others.
In 1988, Atre sold her firm, Atre International Consultants, of Rye, New York to Computer Assistance, Inc. of Hartford, Connecticut. She was retained as president of Atre International, which operated as a branch of Computer Assistance, and became a consultant to the fifteen other national branches of the firm. The firm was acquired by Coopers Lybrand in 1989 and the following year, she negotiated a joint-venture with Intec, Inc. a software firm from Mumbai to develop utilities and training programs for use with Windows 3. In 1991, Atre published Distributed Databases, Cooperative Processing, and Networking, which became the basis for a series of seminars of the same name, which she presented at the Technology Transfer Institute in 1992. She became a contributing editor to the journal DBMS in 1994.
In 1998, Atre relocated to Santa Cruz, California, where her son had begun a web-based business, using her name-recognition in the industry to start his firm. From her new location, Atre operated both Atre Group, Inc. and Atre Associates. In 2003, she co-authored with Larissa Moss the book, Business Intelligence Roadmap: The Complete Project Lifecycle for Decision-Support Applications, which became frequently cited for its practical advice to business managers on how to integrate strategic company goals and varied business applications through database management. That same year, Atre and Robert Blumberg, president of Blumberg Consulting, were hired by DM Review magazine to publish a five-part series of articles on business intelligence. The articles were designed to give information to business managers on how unstructured data could be transformed into usable content for their industries.

Selected works
Atre, S. (1980). Data Base: Structured Techniques for Design, Performance, and Management with Case Studies. New York, New York: Wiley. ISBN 978-0-471-05267-8.
Atre, Shaku (1983). Data Base Management Systems for the Eighties. Wellesley, Massachusetts: QED Information Sciences. ISBN 978-0-89435-053-5.
Atre, Shaku (1991). Distributed Databases, Cooperative Processing, and Networking (1st ed.). New York, New York: McGraw-Hill. ISBN 978-0-8306-2554-3.
Moss, Larissa Terpeluk; Atre, Shaku (2003). Business Intelligence Roadmap: The Complete Project Lifecycle for Decision-Support Applications. Boston, Massachusetts: Addison-Wesley. ISBN 978-0-201-78420-6.

Notes


== References ==

Title: Bridget Boakye
Bridget Boakye is a Ghanaian entrepreneur, data scientist and writer. She co-founded TalentsinAfrica, one of Africa's fastest-growing skills accelerator and recruitment platforms. Her company was among the top 20 companies selected in October 2019 for the Harambe Entrepreneur Alliance at Bretton Woods, New Hampshire. Her company also emerged as one of the top three start-up companies at the Oxford University Africa Innovation Fair.

Early years and education
Bridget was born and raised in Ghana. She moved to the United States to live with her parents when she was ten years old and completed her tertiary education at Swarthmore College, graduating with a bachelor's degree in economics.

Works
After completing her tertiary education at Swarthmore College, she worked in development and education before moving to Ghana.
She was an editor at She Leads Africa where she mostly debated on African history, women, economics and entrepreneurship. In Ghana, she co-founded TalentsinAfrica and ChaleKasa. TalentsinAfrica is an AI-backed recruitment platform democratizing access to opportunity for young Africans, while ChaleKasa is a bespoke events company curating experiences to connect Diasporans and Africans. She is also the co-founder of the Women's Corner GH and the strategist for Africans on China.

Recognitions
She is an Amplify Africa Fellow
She is a member of the Global Shapers Accra Hub.
She was named a Frank 5 Fellow of Swarthmore College, 2018–2019
Hamambe Alliance Entrepreneur Associate, 2019

Philanthropy
During her 27th birthday, Bridget collaborated with Crowdfrica.org to create Bridget Gives @ 27, a fund raising project which she used to help raise $2,000 from her friends and family in support of providing healthcare to the people of Ohua Ghana.


== References ==

Title: Thomas Borwick
The Hon. Thomas James Robert Borwick (born 1987), is a British SEO strategist, CEO and digital influencer. A member of the Conservative Party, Borwick provided tech support for the successful Vote Leave campaign.

Life
The second son of Lord and Lady Borwick, his father (Jamie) sits in the House of Lords and his mother (Victoria), was an MP (2015–17). 
Borwick read economics and finance at the University of Richmond, Virginia, graduating BA, BS, before pursuing further studies at London Business School (MBA). 
He first joined Killik & Co, before founding his own company, Kanto Systems, in 2012. Borwick worked for Cambridge Analytica and AggregateIQ, and was chief technology officer for Vote Leave before creating the campaigning group 3rd Party Ltd for influencing the outcome of the 2019 United Kingdom general election. Borwick ran targeted Facebook advertisements calling for the electorate to vote Green, possibly to divide the anti-Conservative vote. However, given that these ads were run both in swing (Plymouth) and non-swing constituencies (Oxford), the campaign may also have been intended for harvesting data on Green Party supporters. 
Borwick has also operated numerous other tech companies, including Disruptive Communications Ltd, which he started in partnership with Douglas Carswell, former UKIP MP; and Voter Consultancy Ltd, whose micro-targeting of Facebook Ads allegedly precipitated unforeseen death threats against Remainer MPs and led to a subsequent investigation by the Information Commissioner's Office.
Assaulted by three women at a London KFC outlet in 2015, Borwick was critical of KFC's door security, who could assist only by escorting him and his girlfriend from the premises.

See also
Baron Borwick


== References ==

Title: Nadieh Bremer
Nadieh Bremer is a data scientist and data visualization designer. She is based out of a small town outside of Amsterdam.

Education
Bremer graduated cum laude from Leiden University with Masters of Science in Astronomy. She also attended University of California, Berkeley.

Career
Early in her career, Bremer worked as a Senior Consultant of Advanced Analytics and Data Visualizations at Deloitte. She is currently a freelance data designer under the name "Visual Cinnamon," working with small startups to provide custom visualizations of their data.

Publications
Her work has been published in The Washington Post, Bloomberg CityLab, Scientific American, The Week, and DigitalArts, among others.
She co-authored Data Sketches with Shirley Wu in February 2021.

Awards
"Politics & Global", Gold - Information is Beautiful Awards (2018)
Best Data Visualization - North American Digital Media Awards (2018)
Best Investigative Data Journalism - Online Journalism Awards (2018)
Best Individual - Information is Beautiful Awards (2017)
"Unusual", Gold - Information is Beautiful Awards (2017)
"Science & Tech", Silver - Information is Beautiful Awards (2017)
Rising Star - Information is Beautiful Awards (2016)

References
External links
Nadieh Bremer

Title: Iain Buchan
Iain Edward Buchan is a public health physician, data scientist and academic. He holds the W.H. Duncan Chair of Public Health Systems and is Associate Pro Vice Chancellor for Innovation at  the University of Liverpool.
Buchan's research focuses on health data science and informatics to enable better prevention, early intervention, and value of care for patients and populations. He has written 337 articles and his work has been cited of 26000 times according to Google Scholar. He is most known for leading the world's first evaluation of mass rapid antigen testing, and the first realistic risk-mitigated reopening of mass events during the UK's response to the COVID-19 pandemic. He also developed the Civic Data Cooperative, which resulted in the Combined Intelligence for Population Health Action (CIPHA) system during the pandemic. He is the recipient of HTN Health Tech Award, Alwyn-Smith Medal, and Florence Nightingale Award.
Buchan is a Fellow of the Faculty of Public Health, the American College of Medical Informatics, British Computer Society and the Faculty of Clinical Informatics. He has also been an advisor to UK, European and international health policy groups, AstraZeneca) and research organizations including UKRI, Wellcome Trust and the UK National Institute for Health and Care Research (NIHR), for which he is a Senior Investigator.

Education and early career
In the 1980s, Buchan pursued medical training alongside studies in pharmacology and statistical software development. As an undergraduate, he published the first version of a statistical package called "StatsDirect." During the 1990s, as a junior doctor, he researched care pathways, health system dynamics, and care inequities. Later, he trained as a public health consultant while conducting research in medical informatics and pursuing doctoral studies in computational statistics.

Career
Buchan began his academic career in 1992 as an Honorary Clinical Lecturer at the University of Liverpool. He then served as a Research Associate in Medical Informatics at the University of Cambridge in 1996 and Senior Research Fellow in Medical Informatics at Wolfson College, Cambridge in 1997, before training as a Consultant in Public Health. In 2003, he joined the University of Manchester as a Clinical Senior Lecturer in Public Health Intelligence and was promoted in 2008 to Clinical Professor in Public Health Informatics. There, from 2003 to 2017, he founded Health eResearch Centre and co-directed the Farr Institute. In the E-Science movement of the early 2000s he conceived e-Labs and Research Objects, leading to today's Trusted Research Environments and applications in healthcare. At Manchester, he also invented the FARSITE system, helping spin out NW eHealth, and started the #DataSavesLives movement and the Connected Health Cities project.
Subsequently, Buchan served as Director of Healthcare Research at Microsoft Research Cambridge in 2017–2018, producing two patents and furthering the health avatar framework he had conceived eight years earlier.
In 2018, Buchan returned to Liverpool as the University of Liverpool's first chair in Public Health and Clinical Informatics. From 2019 to 2022, he was the founding Executive Dean of the Institute of Population Health at Liverpool, whilst leading research responses to the COVID-19 pandemic. Since 2022, he has been conducting multidisciplinary research partnerships, especially in health technology as Associate Pro Vice Chancellor for Innovation.

Research
Buchan's research areas encompass public health, data science, clinical informatics, epidemiology, and biostatistics. In particular, he has published in areas related to public health challenges, such as inequalities, obesity, mental health and pandemic resilience, and in methodology, including machine learning in epidemiology, research objects in e-science, learning health systems, and the concept of a digital twin/health avatar for healthcare.

COVID-19 response and data-intensive public health research
Buchan led the world's first evaluation of voluntary mass testing for the SARS-CoV2 antigen with lateral flow devices, working with the British Army, local and national government, public health agencies and the UK's National Health Service. This work provided quick proof that lateral flow devices worked as expected to detect people infected with the COVID-19 virus whether they had symptoms or not. Responding to media debate over the reliability of lateral flow devices, he clarified the evidence regarding a public health test versus a clinical test for COVID-19. The impact of this testing was that COVID-19 hospital admissions fell by 43% initially and 25% overall. The BMJ asked him and colleagues for an accompanying methodology paper on the data analysis as a blueprint of best practice. The UK's universal access community testing policy was shaped by this work, including its demonstration of inequalities in testing uptake and barriers such as digital poverty. He had also formulated a test-to-release daily testing alternative to quarantine for close contacts of cases, which resulted in the Daily Contact Testing policy. He also researched COVID-19 and informed policies in other contexts including care homes, hospitals, schools, and vaccination.
In Spring 2021, Buchan applied previous testing and other COVID-19 risk mitigation research to address the issue of young people being vaccinated last and missing out on social development opportunities due to the continued lockdown of significant cultural events. So, he led a city-scale reopening (after COVID-19 lockdowns) of a cluster of business, nightclub and a music festival events – resulting in minimal SARS-CoV-2 transmission, high levels of enjoyment, low levels of fear over risks, and demonstrated the effectiveness of collaborative strategies for health security at mass cultural gatherings.

Public health and data science
Buchan's research has underscored the importance of trust in health data utilization, highlighting transparency, consent, and public involvement, with a specific focus on the role of national governments in the reuse of health data. Building on earlier work in civic data linkage and public health intelligence, he established the first Civic Data Cooperative in Liverpool in late 2019, and put a National Grid of Civic Data Cooperatives forward to the UK Government as means of improving health system innovation and resilience.
Buchan engaged machine learning researchers from Microsoft Research in the field of epidemiology, leading to discoveries pertaining to asthma and allergies. Most recently, he formed the Mental Health Research for Innovation Centre of the UK Government's Mental Health Mission.
Buchan conducted research on other health data science directions including Trusted/Trustworthy Research Environments with Research Objects and eLab networks to improve research reproducibility and tackle the widespread problem of calibration drift in clinical prediction models. He drew attention to the problem of multimorbidity and the need for a unified modelling approach, not only for discovery science but also for personalized care via interactive Health Avatars.
Some of Buchan's most highly cited papers arose from applications of his statistical software to public health problems. He has worked to make better use of routine health record data with combined biostatistics and machine learning approaches to predicting clinical outcomes.
Buchan's data science research has focused on addressing public health challenges, including obesity, inequalities, mental health, and pandemics. He raised a warning over obesity among pre-school children using routinely collected data, then alerted to the high burden of cancer attributable to obesity, then highlighted the challenges of using consumer technology data to understand weight control. He drew attention to the excess of premature deaths in North compared with South England and the need for regional growth incentives.

Awards and honors
2012 – Fellow, American College of Medical Informatics
2014 – Manchester Ambassador, Greater Manchester Combined Authority
2017 – Fellow, British Computer Society
2017 (renewed 2023) – Senior Investigator, National Institute for Health and Care Research
2021 – Best Use of Health Data, HTN Health Tech Awards
2022 – Healthcare Project of the Year, BioNoW
2022 – Alwyn-Smith Medal, The Faculty of Public Health
2023 – Florence Nightingale Award, Royal Statistical Society

Selected articles
Bundred, P.; Kitchiner, D; Buchan, I (10 February 2001). "Prevalence of overweight and obese children between 1989 and 1998: population based series of cross sectional studies". BMJ. 322 (7282): 326–328. doi:10.1136/bmj.322.7282.326. PMC 26573. PMID 11159654.
Simpson, Angela; Tan, Vincent Y. F.; Winn, John; Svensén, Markus; Bishop, Christopher M.; Heckerman, David E.; Buchan, Iain; Custovic, Adnan (1 June 2010). "Beyond Atopy: Multiple Patterns of Sensitization in Relation to Asthma in a Birth Cohort Study". American Journal of Respiratory and Critical Care Medicine. 181 (11): 1200–1206. doi:10.1164/rccm.200907-1101OC. PMID 20167852.
Hacking, J. M.; Muller, S.; Buchan, I. E. (15 February 2011). "Trends in mortality from 1965 to 2008 across the English north-south divide: comparative observational study". BMJ. 342 (feb15 2): d508. doi:10.1136/bmj.d508. PMC 3039695. PMID 21325004.
Bechhofer, Sean; Buchan, Iain; De Roure, David; Missier, Paolo; Ainsworth, John; Bhagat, Jiten; Couch, Philip; Cruickshank, Don; Delderfield, Mark; Dunlop, Ian; Gamble, Matthew; Michaelides, Danius; Owen, Stuart; Newman, David; Sufi, Shoaib; Goble, Carole (February 2013). "Why linked data is not enough for scientists". Future Generation Computer Systems. 29 (2): 599–611. doi:10.1016/j.future.2011.08.004.
Belgrave, Danielle C. M.; Buchan, Iain; Bishop, Christopher; Lowe, Lesley; Simpson, Angela; Custovic, Adnan (1 May 2014). "Trajectories of Lung Function during Childhood". American Journal of Respiratory and Critical Care Medicine. 189 (9): 1101–1109. doi:10.1164/rccm.201309-1700OC. PMC 4098108. PMID 24606581.
Ainsworth, J.; Buchan, I. (2015). "Combining Health Data Uses to Ignite Health System Learning". Methods of Information in Medicine. 54 (6): 479–487. doi:10.3414/ME15-01-0064. PMID 26395036.
Sperrin, Matthew; Candlish, Jane; Badrick, Ellena; Renehan, Andrew; Buchan, Iain (July 2016). "Collider Bias Is Only a Partial Explanation for the Obesity Paradox". Epidemiology. 27 (4): 525–530. doi:10.1097/EDE.0000000000000493. PMC 4890843. PMID 27075676.
García-Fiñana, Marta; Hughes, David M; Cheyne, Christopher P; Burnside, Girvan; Stockbridge, Mark; Fowler, Tom A; Fowler, Veronica L; Wilcox, Mark H; Semple, Malcolm G; Buchan, Iain (6 July 2021). "Performance of the Innova SARS-CoV-2 antigen rapid lateral flow test in the Liverpool asymptomatic testing pilot: population based cohort study". BMJ. 374: n1637. doi:10.1136/bmj.n1637. PMC 8259455. PMID 34230058.
Burnside, Girvan; Cheyne, Christopher P; Leeming, Gary; Humann, Michael; Darby, Alistair; Green, Mark A; Crozier, Alexander; Maskell, Simon; O’Halloran, Kay; Musi, Elena; Carmi, Elinor; Khan, Naila; Fisher, Debra; Corcoran, Rhiannon; Dunning, Jake; Edmunds, W John; Tharmaratnam, Kukatharmini; Hughes, David M; Malki-Epshtein, Liora; Cook, Malcolm; Roberts, Ben M; Gallagher, Eileen; Howell, Kate; Chand, Meera; Kemp, Robin; Boulter, Matthew; Fowler, Tom; Semple, Malcolm G; Coffey, Emer; Ashton, Matt; García-Fiñana, Marta; Buchan, Iain E; Buchan, IE (January 2024). "COVID-19 risk mitigation in reopening mass cultural events: population-based observational study for the UK Events Research Programme in Liverpool City Region". Journal of the Royal Society of Medicine. 117 (1): 11–23. doi:10.1177/01410768231182389. PMC 10858718. PMID 37351911.


== References ==

Title: Mine Çetinkaya-Rundel
Mine Çetinkaya-Rundel (born 1982) is a Turkish-American statistician and professor of the practice at Duke University, and a professional educator at RStudio. She is the author of several open source statistics textbooks and is an instructor for Coursera. She is the chair-elect of the Statistical Education Section of the American Statistical Association. Previously, she was a senior lecturer at University of Edinburgh.

Education
Çetinkaya-Rundel grew up in Turkey and graduated from Robert College,  before coming to the United States for her undergraduate studies to study actuarial science at New York University. After working as an actuary at Buck Consultants, Çetinkaya-Rundel enrolled in graduate school at the University of California, Los Angeles. She earned her master's degree and PhD in statistics at UCLA where her dissertation on estimating the impact of air pollution was supervised of Jan de Leeuw.

Research and career
Çetinkaya-Rundel coauthored three open-access textbooks, OpenIntro Statistics, Introductory Statistics with Randomization and Simulation, and OpenIntro: Advanced High School Statistics. She is an author on the associated R package, openintro. Çetinkaya-Rundel is also a proponent of reproducible analysis in the context of statistics education.

Awards and honours
Çetinkaya-Rundel was elected as a Fellow of the American Statistical Association (ASA) in 2020. She was elected chair of the ASA Statistical Computing program in 2021.

Selected publications
R for Data Science
Introductory statistics with randomization and simulation
OpenIntro Statistics
Advanced High School Statistics


== References ==

Title: Jaya Chakrabarti
Jaya Chakrabarti (born April 1973) MBE is a data scientist, academic and business owner.

Biography
Chakrabarti is originally from Watford before moving to Bristol. Chakrabarti runs the digital agency Nameless, President of the Bristol Chamber of Commerce, board member of Home Office Transparency in Supply Chains MSSIG (Modern Slavery Strategy & Implementation Group), and ran the successful campaign to appoint a Mayor of Bristol in 2012.
Chakrabarti chaired a local democracy organisation, Bristol Manifesto. She is also a Research Fellow with the University of Northampton Business School. Her digital agency, Nameless, campaigns against modern slavery. As CEO of Transparency in the Supply Chains (TISC), a public database of company compliance with anti-slavery laws, Chakrabarti has called for companies to evidence their actions to combat modern slavery.
Chakrabarti served on the Board of Bristol's Watershed Media Centre from 2003 to 2015, was a member of OFCOM's Consumer Panel from 2012 to 2018, and is a Business Fellow with the University of the West of England.
Chakrabarti was made a Member of the Order of the British Empire (MBE) in the 2014 Birthday Honours, for "services to creative digital industries and the community in Bristol".
In 2017 she and her husband Stuart Gallemore helped to save the life of a man who fell into Bristol Harbour while drunk by alerting others who came to his aid.
In 2021 she became President of the Bristol Chamber of Commerce.


== References ==

Title: Meghan Chayka
Meghan Chayka (born c. 1984) is a Canadian data scientist and co-founder of the ice hockey analytics firm Stathletes.

Career
Chayka worked as a financial analyst at the Ministry of Transportation of Ontario and as an analyst at John Deere before shifting to focus entirely on sport analysis in the mid-2010s.
She is a Data Scientist in Residence at the Rotman School of Management at the University of Toronto, a position she has held since 2017.

Stathletes
Chayka's first venture into ice hockey statistics came in 2009 when she and friend Neil Lane began filming her younger brother John's games while he was playing junior ice hockey in the Ontario Junior Hockey League (OJHL), with the aim of analysing the collected data to help him improve his game. They developed a method of video analysis that produced dramatically more points of statistical data than contemporary methodologies.
After John's hockey career was ended by injury, he identified the potential marketability of Chayka and Lane's method of video analysis, which eventually grew into the founding of Stathletes by the Chaykas and Lane in 2010. The company grew quickly and, in 2012, the Vancouver Canucks became the first National Hockey League (NHL) team to employ their services.
Chayka has used her position within the company to champion the growth of women's ice hockey and Stathletes has become an industry leader in analytics for the women's game. Statistical data and analysis generated by Stathletes were used by the CBC's for its broadcasts of the women's ice hockey tournament at the 2018 Winter Olympics and the women's ice hockey tournament at the 2022 Winter Olympics. Chayka spearheaded the landmark partnership between Stathletes and the National Women's Hockey League (PHF since 2021) to provide analytics for the 2020–21 NWHL season.
A regular participant in academic conferences, she has spoken at a number of analytics and sports conferences, including at the 2020 MIT Sloan Sports Analytics Conference and the inaugural Women's Hockey Analytics Conference.
During the first year of the COVID-19 pandemic in Canada, Chayka joined forces with Alison Lukan to present Hockey Analytics Night in Canada (HANIC), a regular online conference intended to bring together and spark discussion within the ice hockey analytics community via diverse topics and speakers.

Awards and honours
Chayka was named the Ontario Chamber of Commerce's Top Young Entrepreneur of the Year in 2018. In 2019, she was named to both the George Brown College's The 5 to Watch for Sports Business Executives and The Hockey News' Top 100 People of Power and Influence.
Chayka appeared on The Athletic's Top Forty Under 40 in both 2019 and 2020, and Sportsnet's 25 Most Powerful Women in Hockey 2022. She was identified by The Hockey News as one of twenty candidates with the credentials to become the first woman to serve as a general manager in the NHL.
Alongside Melody Davidson and Sami Jo Small, Chayka was selected as an honorary coach for the 2022 PHF All-Star Showcase.

Personal life
Chayka holds both an Honours in Business Administration (HBA) and a master's degree in economics from McMaster University and a BBA in finance from the Goodman School of Business at Brock University.
Her younger brother, John Chayka, served as general manager of the Arizona Coyotes during 2016 to 2020.


== References ==

Title: Lynn Cherny
Lynn Cherny (born 1967) is a Boston-based data analysis consultant specialized in data mining and analysis, customer research, and interface design. She is currently a faculty member at EMLYON Business School. She has worked as a consultant for companies including TiVo, Adobe, AT&T Labs, Autodesk, and Internet startups, doing work such as statistical programming in R and Python, text clustering, data analysis on survey data and software usage logs, design for bioinformatics tools, dashboard mockups, and soup-to-nuts interaction design.

Education
Cherny received a Ph.D. from Stanford University in Linguistics; an M.Phil. from Cambridge University in Computer Speech and Language Processing; and a B.A. from University of Maryland in Linguistics.

Career
Cherny showcased her interest in data analysis and design while she was a Ph.D student at Stanford. Her career began in research in an HCI group at Bell Labs (later AT&T Labs), but she left research to work in industry as a UI designer. She also serves as a self-employed consultant for data analysis and data visualization at Ghostweather Research & Design, LLC. In recent years, she received an academic fellowship teaching Interactive Data Visualization from University of Miami, and currently serves as associate professor in Emlyon Business School at Lyon, France. She is also the author of two books "Wired Women: Gender and New Realities in Cyberspace," and "Conversation and Community: Chat in a Virtual World."

Data-driven design
Cherny spent 20 years in various UX, UI, and usability roles in Silicon Valley, Paris, Seattle, and Boston. Starting from 1998, she served as one of the first UI designer at Excite, Inc in 2000, she was hired as a Senior UI Designer and became manager of the UI and Usability group at TiVo, Inc. Leaving for a job opportunity offered by Axance, a web usability consulting company in Paris, France in 2001, Cherny was responsible for international projects and project oversight, as well as extending business into design consulting. From 2002 to 2004, she served as senior Interaction Designer for Adobe Systems, Inc, where she designed for interoperability and UI consistency in the Creative Suite 2 bundle, which consisted of Photoshop, Illustrator, InDesign, Acrobat. She shared a patent for some of her work on color management synchronization, settings and PDF export. Cherny worked for the MathWorks from 2004–2005, where she focused on bioinformatics, biological simulation and developing data analysis/visualization tools. She shared a patent for a data visualization tool designed and developed at the MathWorks. From 2005 to 2007, she serves as the manager of interaction design and visual design team at Autodesk, working on an architectural design desktop application and responsible for user research and design management. From 2007 to 2010, she worked as a user experience research and design Consultant for SolidWorks.

Data Consulting
Serving as a self-employed consultant for data analysis and data visualization at Ghostweather Research & Design, LLC, Cherny has worked on data visualization and analysis problems including creating customer personas from survey data and interviews, visualizing tax brackets, analyzing bug reports and stack traces, clustering pharmaceutical drug reports, network analysis for topic modeling, entity recognition of company names in news articles, and dashboard design.

Teaching
Cherny is currently an associate professor at Emlyon Business School, where she teaches business analytics, Python, text mining, SQL, visualization. From 2015 to 2016, she served as visiting Knight Chair for the Center for Communication, Culture and Change at University of Miami. She taught data visualization and data analysis and help launching Data Visualization and Journalism track in the School of Communication's Interactive Media department.

Publications
Cherny published a book titled Wired Women: Gender and New Realities in Cyberspace, which looks at women and the new Internet technology and culture features fourteen essays that discuss such issues as gender attitudes, courtship via e-mail, censorship, hacker culture, online harassment.
Cherny's dissertation from Stanford University was an early study on online user-extensible chat community, later published as Conversation and Community: Chat in a Virtual World (CSLI, 1999). She revised this dissertation into a book of the same title Conversation and Community: Chat in a Virtual World.

References
External links
Lynn Cherny's personal website
Lynn Cherny's twitter
Lynn Cherny's LinkedIn

Title: Rumman Chowdhury
Rumman Chowdhury (b. 1980) is a Bangladeshi American data scientist, a business founder, and former responsible artificial intelligence lead at Accenture. She was born in Rockland County, New York. She is recognized for her contributions to the field of data science.
Chowdhury's journey into the world of science was inspired by her love for science fiction, a passion that ignited her curiosity, often attributed to the "Dana Scully Effect." This fascination with the intersection of science and fiction laid the foundation for her future endeavors.

Education
Chowdhury completed her undergraduate study in Management Science and Political Science at Massachusetts Institute of Technology. She received a Master of Science from Columbia University in Statistics and Quantitative methods. She holds a Doctorate Degree in Political Science from University of California, San Diego. She finished her PhD whilst working in Silicon Valley. Her main interest for her career and higher educational studies was how data can be used to understand people's bias and ways to evaluate the impact of technology on humanity.

Career
Early
Chowdhury taught data science at the boot camp Metis and worked at Quotient before joining Accenture in 2017. She led their work on responsible artificial intelligence. She was concerned about algorithmic bias and the AI workforce; particularly on retaining researchers. She has spoken openly about the need to define what ethical AI actually means and was responsible for coining the term "moral outsourcing". She works with companies on developing ethical governance and algorithms that explain their decisions transparently. She is determined to use AI to improve diversity in recruitment. 
Chowdhury, alongside a team of early career researchers at the Alan Turing Institute, developed a Fairness Tool which scrutinises the data that is input to an algorithm and identifies whether certain categories (such as race or gender) may influence the outcome. The tool both identifies and tries to fix bias, enabling organisations to make fairer decisions.

All.ai, Parity and X Institute
Chowdhury designed All.ai, a language analysis tool that can monitor and improve the gender balance of speakers in meetings.
In 2020, she founded Parity to bridge the translation gap between risk, legal, and data teams.
She launched the X Institute, a program which teaches refugees about data science and marketing.
She has given a keynote speech at Slush, talking about augmenting human capabilities. She delivered a TED talk about humanity in the age of artificial intelligence.

Twitter
From February 2021 to November 2022 Chowdhury was a director for the Machine Learning Ethics, Transparency and Accountability (META) team with Twitter. META's goal was to study and improve the machine learning systems used within Twitter, this included biased algorithms which may cause harm to the user. Biased algorithms have been an issue for a long time in the tech industry; traits such as gender, sex, race, or social class hold potential segregation that may result in unfair decisions. META strived to avoid this by making Twitter better, fair, accountable, and more transparent for its users. Most projects that META teams worked on involved research and data analysis, and the team was largely made up of professors, researchers, and engineers. In 2021,  Chowdhury and the META team published an analysis titled Examining algorithmic amplification of political content on Twitter. In November 2022, Chowdhury was one of many Twitter employees who were laid off at short notice after Elon Musk's takeover of the company.

Awards
In 2017, she was included in the 100 Women (BBC) in the "Glass Ceiling Team" category. In 2018, she was named one of five people who are shaping AI by Forbes. She was acknowledged by The Business Journals as one of the Bay Area's top 40 Under 40. She has also been inducted into the British Royal Society of the Arts (RSA) to celebrate people who have made progress in social challenges.


== References ==

Title: Maxime Cohen
Maxime C. Cohen is a professor of Retail and Operations Management and the Academic Director of the Bensadoun School of Retail Management at McGill University in Montreal, Quebec, Canada. Previously, he was the Director of Research at the Bensadoun School of Retail Management at McGill University. He holds the title of Scale AI Chair in Data Science for Retail.

Education and career
Cohen did his Ph.D. in Operations Research from the Massachusetts Institute of Technology. He completed his Master of Science in Electrical Engineering and a Bachelor of Science in Aerospace Engineering from the Technion – Israel Institute of Technology. His research interests are in data science, AI technologies, empirical and behavioral operations management, field experiments, online marketplaces, pricing and revenue management, and retail.
Cohen was appointed as Chief of AI Strategy at the CIUSSS West-Central Montreal in December 2024. Prior to this, he was appointed as the first Chief Artificial Intelligence Officer (CAIO) at ELNA Medical, Canada’s largest integrated network of medical clinics. He was also a visiting professor and Shubik Fellow at Yale School of Management in 2023-2024. Before joining the faculty at McGill University, Cohen was an assistant professor of technology, operations, and statistics at the NYU Stern School of Business. He also worked as a research scientist at Google AI.
He has co-authored several books and published numerous academic papers in leading journals, including Harvard Business Review, Information Systems Research, Management Science, Marketing Science, MIT Sloan Management Review, etc. He worked extensively on pricing optimization and on demand prediction. In 2022, he co-authored a book titled Demand Prediction in Retail that was published by Springer. In September 2024, he authored the AI is Good for Us white paper highlighting the benefits and positive impacts of AI on humanity and society.

Honours
Top Retail Influencer, RETHINK Retail (2024).
MSOM Young Scholar Prize (2022).
Named to Poets&Quants' annual Best 40-Under-40 MBA Professors list (2022).
CIST Best Conference Paper Nomination Award (2021).
POMS Wickham Skinner Early-Career Research Accomplishments Award (2020).
First Place in the Best OM Paper in Management Science Award (2019).
UPS PhD Fellowship (2014-2015).
Best Application of Theory award from the 2015 NEDSI Conference.
MIT Energy Initiative Fellowship (2011-2012).

References
External links
Maxime Cohen publications indexed by Google Scholar
Website
Academic homepage

Title: David Cournapeau
David Cournapeau is a data scientist. He is the original author of the  scikit-learn package, an open source machine learning library in the Python programming language.

Early life and education
Cournapeau graduated with a MSc in Electrical Engineering from Telecom Paristech, Paris in 2004, and obtained his PhD in Computer Science at Kyoto University, Japan, in the domain of speech recognition.

Career
The scikit-learn project started as scikits.learn, a Google Summer of Code project by David Cournapeau.
After having worked for Silveregg, a SaaS Japanese company delivering recommendation systems for Japanese online retailers, he worked for 6 years at Enthought, a scientific consulting company. He joined Cogent Labs, a Japanese Deep Learning/AI company, in 2017. He is a Machine Learning Engineering Manager at Mercari, Inc.
Cournapeau has also been involved in the development of other central numerical Python libraries:
NumPy and SciPy.


== References ==

Title: Ivo D. Dinov
Ivaylo (Ivo) D. Dinov (Bulgarian: Ивайло Д. Динов) is a mathematical statistician, data scientist, and computational neuroscientist, who is the Henry Philip Tappan collegiate professor at the University of Michigan. He is a co-developer of the 5D spacekime model, a new technique for complex time (kime) representation, modeling, and analysis of repeated measurement longitudinal processes. Dinov is the author of the Data Science and Predictive Analytics (DSPA) book and has published significantly on a wide range of topics, including mathematical modeling, computational statistics, data science, neuroscience, applied statistics, and generative artificial intelligence models (GAIMs).

Education
In 1991, Dinov completed his undergraduate study in mathematics and informatics from Sofia University in Bulgaria. Under the supervision of Kenneth L. Kuttler, he wrote a master's thesis entitled "Bochner Integrals and vector measures" and received his MS in pure mathematics in 1993 from Michigan Technological University Advised by De Witt Sumners and Fred William Huffer, Dinov completed his MS in statistics and PhD in mathematics degrees at the Florida State University. His doctoral dissertation was on "Mathematical and statistical techniques for modeling and analysis of medical data".
Subsequently, Dinov was a postdoctoral fellow at the University of California, Los Angeles (UCLA). During his postdoctoral training at UCLA he received a T32 fellowship in neuroimaging and brain mapping, as part of the UCLA Institutional National Research Service Award.

Academic career
Between 2001 and 2013, Dinov was a faculty in the departments of statistics and neurology at UCLA. In 2002, he founded the Statistics Online Computational Resource (SOCR).
In 2013, Dinov joined the faculty at the University of Michigan, where he has been serving in multiple academic leadership roles, including department chair, associate director of the Michigan Institute for Data Science, and chair of various University of Michigan faculty senate committees.
Dinov is an active member in several professional societies, including the American Statistical Association (ASA), International Association for Statistical Education (IASE), American Mathematical Society (AMS), American Physical Society (APS), and an honorary member (fellow) of the  Sigma Theta Tau International Society and American Association for the Advancement of Science (AAAS). He is an elected member of the International Statistical Institute (ISI).

Publications
Some of Dinov's selected publications include the following:

Data Science and Predictive Analytics book, which has received  multiple expert reviews and its editions in various digital, paperback, and hardcover formats have been read by over six million readers.
The Data Science: Time Complexity, Inferential Uncertainty, and Spacekime Analytics book covers a wide range of topics including motivation for doing data science, characteristics of big datasets, artificial intelligence concepts and tools, neuroimaging genetics, climate change, problems with time, definition of Kime, economic forecasting, wave functions, definition and properties of operators, Fourier transforms, Kaluza-Klein theory, contravariance, velocity, Schrodinger equation, Lorenz transformation, Heisenberg's uncertainty principles, spacekime analytics, linear modelling using tensors, inferential uncertainty, and information theoretic versus data science formulations.
The article Methodological challenges and analytic opportunities for modeling and interpreting big healthcare data identified multiple scientific gaps and proposed novel modeling methods for complex healthcare data including heterogeneous imaging, genetic, clinical and phenotypic data elements.

Personal life
In 2022, as coach of the University of Michigan Men's Water Polo, Dinov led the Michigan Wolverines to a Big Ten Division title and was recognized as the "2022 Big Ten Division Men's Water Polo Coach of the Year".

Citations
External links
Ivo D. Dinov publications indexed by Google Scholar
Michigan Experts Profile
SOCR Profile
Dinov's Math Genealogy at the Mathematics Genealogy Project

Title: Dirk Eddelbuettel
Dirk Eddelbuettel is a Canadian statistician, data scientist and researcher. He is the author of the open-source software package Rcpp, written in the R programming language, and has also written the textbook Seamless R and C++ Integration with Rcpp on the topic. He is co-founder of the R In Finance Conference. In addition, he has contributed to many packages in R as well as the Debian project. He is also a co-creator of the Rocker Project bringing Docker to R.

Early life and education
Eddelbuettel has an M.Sc in Industrial Engineering (Comp.Sci./OR) from Karlsruhe Institute of Technology in Germany. He received an M.A. and a PhD in Financial Econometrics from the School for Advanced Studies in the Social Sciences (EHESS) in France.

Career
Eddelbuettel has been a contributor to CRAN for over a decade. He is the co-author/maintainer of more than sixty packages.  He is also the Debian/Ubuntu maintainer for R and other quantitative software, editor of the CRAN Task Views for Finance and High-Performance Computing, co-founder of the annual R/Finance conference, and an editor of the Journal of Statistical Software. He is a member of the R Foundation and contributed to R-Hub.
Eddelbuettel has been interviewed by Data Science Los Angeles and others. He frequently gives talks to the R community.
He worked as a senior quantitative analyst from 2008 to 2012, and as a senior financial engineer from 2012 to 2018. Currently he is working as a principal software engineer in Chicago.
Eddelbuettel joined University of Illinois at Urbana-Champaign in 2018. He works as an adjunct Clinical Professor in Statistics where he created and teaches STAT 447: Data Science Programming Methods.

References
External links
Official website
R In Finance Conference

Title: Jessica Flack
Jessica C. Flack is a data scientist, evolutionary biologist, and professor at the Santa Fe Institute.

Education and career
Jessica Flack attended Cornell University for her undergraduate studies and graduated in 1996 with a Bachelor of the Arts (Honors Degree). She received her PhD from Emory University in 2003, where she studied cognitive science, animal behavior and evolutionary theory. Following her Ph.D. she moved to the Santa Fe Institute as a postdoctoral fellow, and studied complexity science, collective behavior, and robustness from 2004 to 2007.
In 2011 she moved to the  University of Wisconsin, Madison to help found and direct the Center of Complexity & Collective Computation in the Wisconsin Institute for Discovery. Following her work in Wisconsin, she went back to the Santa Fe Institute and, as of 2022, works there as a professor. Flack also acts as the director for the Collective Computation Group at SFI, and serves as the chair of public events. She performs much of her research in collaboration with co-director David Krakauer.

Research
Flack is known for her work connecting the behavior of individuals to group activity. She has used animals, particularly macaques monkeys, to examine group behavior. Flack's early work examined social rules within chimpanzees. Her work with macaque revealed that fights within a group improve group's ability to make decisions, a process Flack calls collective computation. She has also used macaque to examine conflict resolution and social structure.

Selected publications
Flack, Jessica C.; Girvan, Michelle; de Waal, Frans B. M.; Krakauer, David C. (2006). "Policing stabilizes construction of social niches in primates". Nature. 439 (7075): 426–429. Bibcode:2006Natur.439..426F. doi:10.1038/nature04326. ISSN 0028-0836. PMID 16437106. S2CID 4416675.
Flack, J.C.; de Waal, F.B.M. (2000-01-01). "'Any animal whatever'. Darwinian building blocks of morality in monkeys and apes". Journal of Consciousness Studies. 7 (1–2): 1–29.
Flack, Jessica C. (2012-07-05). "Multiple time-scales and the developmental dynamics of social systems". Philosophical Transactions of the Royal Society B: Biological Sciences. 367 (1597): 1802–1810. doi:10.1098/rstb.2011.0214. PMC 3367696. PMID 22641819.


== References ==

Title: Guillermo Gallego
Guillermo Gallego is an American data scientist, academic and author. He is the Liu Family Emeritus professor at Columbia University, the Crown Worldwide Professor Emeritus at The Hong Kong University of Science and Technology and is the X.Q. Deng Presidential Chair Professorship at The Chinese University of Hong Kong, Shenzhen.
Gallego is most known for his works on discrete choice models, dynamic pricing, pricing analytics, assortment optimization and dynamic programming. Among his authored works are his publications in academic journals, including  Management Science, Operations Research, Mathematics of Operations Research, and MSOM, as well as a book titled Revenue Management and Pricing Analytics. He is the recipient of two best-papers awards from Management Science and Operations Research, the 2011 INFORMS Historical Award of the Revenue Management & Pricing Section, the 2012 INFORMS Practice Award, the 2016 INFORMS Impact Prize, the 2021 INFORMS Revenue Management and Pricing Section Prize among others.
Guillermo has been elected as an INFORMS fellow in the class of 2012 and an MSOM fellow in the class of 2013. He is a Hong Kong Institution of Engineers (HKIE) Fellow since 2016, and has been listed in Stanford's List of World’s top 2% scientists.

Education
Gallego completed his B.A. in Mathematics from the University of California in 1980. Later in 1987, he obtained his M.S. from Cornell University, followed by a PHD from the same institution in 1988.

Career
Gallego began his academic career at Columbia University in 1988, holding various positions within the Department of Industrial Engineering and Operations Research, promoted to full professor in 2000, and to the Liu Family Professorship in 2012. Additionally, from 2016 to 2022, he served as the Crown Worldwide Professor of Engineering at The Hong Kong University of Science and Technology. Since 2022, he is the holder of the X.Q. Deng Presidential Chair Professorship at The Chinese University of Hong Kong, Shenzhen.
From 2002 to 2008, he held an appointment as the department chairman of Industrial Engineering and Operations Research (IEOR) at Columbia University. Gallego served as the department head of the Department of Industrial Engineering and Decision Analytics at Hong Kong University of Science and Technology from 2016 to 2021. He currently serves as the Operations Research Area Coordinator in the School of Data Science at the Chinese University of Hong Kong, Shenzhen.

Research
Gallego, in his early research, explored how companies could smoothly recover from production schedule disruptions, and on integrating inventory control and vehicle routing to optimize distribution systems. His 1993 collaborative work presented a new proof of Scarf's ordering rule for the newsboy problem and extended its analysis to scenarios in inventory management, including the recourse case, fixed ordering costs, random yields, and multi-item competition for scarce resources. In the following year, he proposed dynamic pricing strategies for inventory management in industries with time-constrained sales, stochastic and price-sensitive demand, using intensity control, obtaining structural monotonicity, closed-form solutions, and extensions to more complex scenarios. Moreover, in 1995, he addressed the optimization of pricing strategies for industries facing fixed stock and finite horizon challenges. The study determined the optimal timing of mark-ups or mark-downs based on stochastic demand and price sensitivity to maximize revenues.
Gallego's 1997 work proposed stochastic pricing strategies to optimize network revenue management problems, deriving asymptotic optimality results for simple policies. In 2001, he explored optimal inventory control policies, such as state-dependent (s, S) and base-stock policies, considering advance demand information, aiming to improve revenue, capacity use, and operational performance in stochastic inventory systems. In 2008, he explored the strategic use of callable products, analyzing their role in generating riskless additional revenue in the context of revenue management problems. Through this research, he also provided optimal pricing and booking strategies, with potential extensions to multifare structures, network models, and other industries. While investigating the assortment optimization problems under the nested logit model in 2014, he demonstrated that the problem is polynomial solvability for the standard model, and proposed heuristic strategies for NP-hard cases. Later in 2016, he introduced a Markov chain choice model for assortment planning, approximating discrete choice models, optimizing assortments, and offering insights into substitution behavior.
In 2019, Gallego authored a book titled Revenue Management and Pricing Analytics with H. Topaloglu. Through this book, he bridged the disparity between current research findings and well-established literature within the field. Georgia Perakis from Massachusetts Institute of Technology, in her review of the book said "The book by Gallego and Topaloglu provides a fresh, up-to-date and in depth treatment of revenue management and pricing. It fills an important gap as it covers not only traditional revenue management topics but also new and important topics such as revenue management under customer choice as well as pricing under competition and online learning." She also lauded the book's ability to cater to a diverse range of readers, from advanced undergraduates to those pursuing masters and PhD degrees. In 2022, he presented a learning algorithm for a firm to optimize revenue through personalized pricing for various consumer types over a finite selling period, using a primal-dual formulation to explicitly learn the dual optimal solution and overcoming the curse of dimensionality. His more recent work has focused on personalized pricing and personalized assortment optimization.

Awards and honors
2005-2006 – IBM Faculty award
2011 – INFORMS Revenue Management and Pricing Historical Prize, INFORMS
2012 – Fellow, INFORMS
2017 – INFORMS, Best OM paper award in Management Science
2019 – INFORMS, Best OM paper in Operations Research
2021 – INFORMS Revenue Management and Pricing Section Prize, INFORMS

Bibliography
Books
Revenue Management and Pricing Analytics (2019) ISBN 9781493996049

Selected articles
Gallego, G., & Van Ryzin, G. (1994). Optimal dynamic pricing of inventories with stochastic demand over finite horizons. Management science, 40(8), 999–1020.
Feng, Y., & Gallego, G. (1995). Optimal starting times for end-of-season sales and optimal stopping times for promotional fares. Management science, 41(8), 1371–1391.
Gallego, G., & Özer, Ö. (2001). Integrating replenishment decisions with advance demand information. Management science, 47(10), 1344–1360.
Gallego, G. & O. Sahin. (2010). Revenue Management with Partially Refundable Fares. Operations Research, 58, 817 – 833.
Gallego, G. & R. Wang. (2014). Multiproduct Price Optimization and Competition Under the Nested Logit Model with Product-Differentiated Price Sensitivities. Operation Research, 62  (2), 450–461.
Gallego, G. & M. Hu. (2014). Dynamic Pricing of Perishable Assets under Competition. Management Science, 60  (5), 1241–1259.
Blanchet, J., Gallego, G., & Goyal, V. (2016). A Markov chain approximation to choice modeling. Operations Research, 64(4), 886–905.
Gallego, G., Li, A., Truong, V. A., & Wang, X. (2020). Approximation algorithms for product framing and pricing. Operations Research, 68(1), 134–160.
Gao, P., Ma, Y., Chen, N., Gallego, G., Li, A., Rusmevichientong, P., & Topaloglu, H. (2021). Assortment optimization and pricing under the multinomial logit model with impatient customers: Sequential recommendation and selection. Operations research, 69(5), 1509–1532.
Chen, N., & Gallego, G. (2022). A primal–dual learning algorithm for personalized dynamic pricing with an inventory constraint. Mathematics of Operations Research, 47(4), 2585–2613.


== References ==

Title: Siddharth Garg
Siddharth Garg is a cybersecurity researcher and associate professor at New York University Tandon School of Engineering. He is also a member of NYU WIRELESS. Garg is known for his research leveraging machine learning to securely manufacture computer chips so they are less prone to hacking. In 2016, he was named one of Popular Science magazine's "Brilliant 10."

Education
Garg attended Indian Institute of Technology, Madras where he received his Bachelor of Technology degree in 2004. He then attended Stanford University for his Master of Science degree in electrical engineering 2005. For his doctoral research, he attended Carnegie Mellon University, where he received his PhD in 2009. His doctoral advisor was Diana Marculescu and his dissertation, entitled System-level modeling and mitigation of the impact of process variations on digital integrated circuits, received Carnegie Mellon's Angel G. Jordan Award for outstanding thesis contribution.

Career
Following Garg's postdoctoral work, he became an assistant professor at University of Waterloo from 2010 to 2014, before moving to New York University Tandon School of Engineering, where he is currently an associate professor. His research interests bridge machine learning and cybersecurity. His research group has investigated how artificial intelligence can be exploited by malicious actors. They found that it is possible to embed behavior in artificial intelligence algorithms, for example those used for speech recognition, that can emerge in response to certain signals. Garg and his team showed that they could train an image recognition algorithm to interpret a stop sign as a speed limit signal by placing a post-it note over it. When such behavior is programmed by malicious actors, it's known as a "backdoor." They are working to understand different backdoors in order to develop ways to proactively detect them. Garg has also worked to develop manufacturing protocols for computer chips to make them resistant to hacking attempts.

Awards and honors
Brilliant 10, Popular Science, 2016
National Science Foundation CAREER Awards, 2015


== References ==

Title: Karina Gibert
Karina Gibert is a data scientist and researcher at the Technical University of Catalonia. She co-founded the Intelligent Data Science and Artificial Intelligence Research Center (IDEAI) in 2018 and was made director in 2021.

Education and career
In 1990, Gibert was awarded an undergraduate degree in computer science from the Barcelona School of Informatics. She followed this with a master's in computer science in 1991. Gibert then gained her PhD from the Technical University of Catalonia in 1995, researching computational statistics and Artificial Intelligence.
Gibert's research encompasses a variety of topics in statistics, data science and intelligent decision support systems, and she has been a member of the Knowledge Engineering and Machine Learning group (KEMLg) since 1986.
Alongside heading up IDEAI, Gibert became dean of the Official College of Computer Engineering of Catalonia in 2023. She is a member of the International Advisory Board of the Joint Research Center journal Environmental Modeling and Software, and an expert on the Catalonia.AI strategic plan.
In 2022, Gibert was a guest editor for the Women in Artificial Intelligence special issue of the Applied Sciences journal. This provided a collection of AI research led by women, to bridge the gender gap in the field.

Recognition
Karina Gilbert has been recognised for her work, winning various awards such as the Ada Byron Award 2022, the WomenTech Award 2023, and the National Award for Computer Engineering 2023.


== References ==

Title: Helen Glaves
Helen Glaves is the Senior Data Scientist at the British Geological Survey. She serves as Editor for the American Geophysical Union Earth and Space Science journal and was awarded the European Geosciences Union Ian McHarg medal. Glaves will serve as the President of the European Geosciences Union from 2021 to 2023.

Early life and education
Glaves earned bachelor's degrees in geology and information technology. Her early research focussed on database design.

Research and career
Glaves has contributed to novel ways to store and share marine research data. She is the programme manager of the Research Data Alliance, which she has been involved with since its inception. Glaves co-ordinates the Ocean Data Interoperability Platform (ODIP), which looks to share ocean data across scientific domains and international borders. She led the expansion of ODIP (ODIP-II) that supported transferring data in different formats between research centres. ODIP-II makes use of the Natural Environment Research Council (NERC) vocabulary server to transfer between different data formats. The vocabulary server was developed by the British Oceanographic Data Centre and National Oceanography Centre.
In 2016 Glaves was awarded the  European Geosciences Union (EGU) Ian McHarg medal. She serves as the President of the EGU Earth and Space Science Informatics section. She was elected the President of the European Geosciences Union from 2021 to 2023. She is an Editor of the American Geophysical Union journal Earth and Space Science.


== References ==

Title: Jarek Gryz
Jarek Gryz is a computer scientist, data analyst, author, and academic. He is a professor in the Department of Electrical Engineering and Computer Science and a member of the Cognitive Science Program in the Department of Philosophy at York University in Toronto, Canada.
Gryz's research spans the field of data analysis, query optimization, artificial intelligence, and privacy in IT systems. He authored the book Database Query Optimization with Soft Constraints and has published over 100 peer-reviewed articles, He is the recipient of the Outstanding Contribution to Internationalization Award and the IBM Faculty Award as well as a Senior Member of the Institute of Electrical and Electronics Engineers (IEEE).

Education and early career
Gryz earned his MA degree in Philosophy from Warsaw University in 1989 and another MA degree in philosophy from the University of Maryland, College Park in 1992. He then shifted his subject of study and started a graduate program in the Department of Computer Science in 1993 and received a master's degree in computer science from the same institution in 1995. Subsequently, in 1996, he completed a brief internship at the Stanford Research Institute in Menlo Park, followed by a Ph.D. in computer science in 1997 under the supervision of Jack Minker, from the University of Maryland, College Park.

Career
Following his Ph.D., Gryz started his academic career as an Assistant Professor and became a professor at York University in Toronto. He has also held teaching appointments at Warsaw University in Poland, Reykjavik University in Iceland, and College of William and Marry in the United States In addition, he served as the Faculty Fellow of the IBM Centre of Advanced Studies in Canada from 1998 to 2013. He holds an appointment as a professor in the Department of Electrical Engineering and Computer Science and is a member of the Cognitive Science Program in the Department of Philosophy at York University in Toronto, Canada.

Research
Gryz's research has primarily focused on the field of database systems and has made contributions in various areas, including query optimization, data visualization, and data mining. Furthermore, he has explored the philosophical foundations of Artificial Intelligence.

Computer science
Gryz has worked primarily in the area of database query optimization. He proposed several techniques to improve query performance in relational databases using sampling, integrity constraints, views or a novel concept of soft constraints. Many of these algorithms have been implemented in the commercial version of DB2, IBM's relational database system. He further extended this work into other types of databases such as object-oriented, XML, and graph.
In the area of data mining, Gryz designed new algorithms for discovery of homogeneous regions in a binary matrix. He also worked on improving existing methods of frequent itemset mining. Furthermore, he branched into research on data visualization, which serves as a crucial element of analysis of massive data sets.[17].

Philosophy
Gryz's early research in philosophy focused on ethics. He examined the distinction between deontic and aretaic terms in moral vocabulary and offered a new interpretation of Stoic ethics. His interest in philosophy led to interdisciplinary research into philosophical foundations of AI, privacy in IT systems, and ethical issues, such as bias and interpretability of algorithms.

Awards and honors
2006 – Faculty Award, International Business Machines Corporation
2010 – Outstanding Contribution to Internationalization Award, York International
2016 – Senior Member, Institute of Electrical and Electronics Engineers (IEEE)

Bibliography
Books
Database Query Optimization with Soft Constraints (2008)

Selected articles
Gryz, J. (1999). Query rewriting using views in the presence of functional and inclusion dependencies. Information Systems, 24(7), 597–612.
Edmonds, J., Gryz, J., Liang, D., & Miller, R. J. (2003). Mining for empty spaces in large data sets. Theoretical Computer Science, 296(3), 435–452.
Chomicki, J., Godfrey, P., Gryz, J., & Liang, D. (2003). Skyline with presorting. In Proceedings of ICDE, 717–719.
Godfrey, P., Shipley, R., & Gryz, J. (2007). Algorithms and analyses for maximal vector computation. The VLDB Journal, 16(1), 5–28.
Yakovets, N., Godfrey, P., & Gryz, J. (2016). Query planning for evaluating SPARQL property paths. In Proceedings of the 2016 International Conference on Management of Data (pp. 1875–1889).
Godfrey, P., Gryz, J., & Lasek, P. (2016). Interactive visualization of large data sets. IEEE transactions on knowledge and data engineering, 28(8), 2142–2157.
Gryz, J., & Rojszczak, M. (2021). Black box algorithms and the rights of individuals: No easy solution to the" explainability" problem. Internet Policy Review, 10(2), 1–24.


== References ==

Title: Youyang Gu
The Youyang Gu COVID-19 model (sometimes abbreviated YYG) is a computer software disease model for COVID-19 produced by independent data scientist Youyang Gu.
The model is unique in applying machine learning to derive the basic reproduction number (R0) from data published by Johns Hopkins University's Center for Systems Science and Engineering (CSSE), and it seeks to minimize the error between its projections and CSSE data on the number of United States COVID-19 deaths.

Use and endorsements
Gu's model was one of seven featured in The New York Times' survey of models and one of nine in FiveThirtyEight's survey, was cited by the Centers for Disease Control (CDC) in its estimates for U.S. recovery, and was one of three listed by the State of Washington on its "COVID-19 risk assessment dashboard" used to determine the date the state would reopen its economy after the COVID-19 pandemic in Washington. The model's author claims it is the only one cited by CDC that is not receiving public funding.
Yann LeCun, Facebook's chief AI scientist and professor at the Courant Institute of Mathematical Sciences, stated in May 2020 that Gu's model "is the most accurate to predict deaths from COVID-19", surpassing the accuracy of the well-funded Institute for Health Metrics and Evaluation COVID model. Its superior accuracy was also noted by Silicon Valley newspaper The Mercury News and by The Economist, which called it "more accurate than forecasts from many established outfits".

Youyang Gu biography
Gu is a 2015 graduate of Massachusetts Institute of Technology. He was born in Shanghai in 1993 or 1994 and grew up in Urbana, Illinois.

See also
List of COVID-19 simulation models

References
External links
Official website (COVID-19-projections.com)
COVID-19 projections by Youyang Gu on GitHub

Title: Jeff Hammerbacher
Jeff Hammerbacher (born 1982 or 1983) is a data scientist. He was chief scientist and cofounder at Cloudera and later served on the faculty of the Icahn School of Medicine at Mount Sinai.

Early life
Hammerbacher was born in 1982 or 1983. He grew up in Fort Wayne, Indiana. His father worked at the General Motors plant and his mother was a nurse. From an early age he had an interest in numbers.

Career
Prior to co-founding Cloudera, Hammerbacher led the data team at Facebook. Hammerbacher was an entrepreneur in residence at Accel Partners immediately prior to joining Cloudera. Hammerbacher worked as a quantitative analyst on Wall Street.
Hammerbacher has been featured for his work in Forbes, Fast Company, MIT Technology Review, Harvard Business Review, NY Times, Bloomberg BusinessWeek and others.

Selected publications
Segaran, Toby; Hammerbacher, Jeff (2009). Beautiful Data: the stories behind elegant data solutions (First ed.). Sebastopol, California: O'Reilly. ISBN 9780596157111. OCLC 827947721.


== References ==

Title: Timandra Harkness
Timandra Harkness is a British writer, presenter and comedian.
She has contributed to several publications, including BBC Science Focus magazine and The Daily Telegraph, and authored the book Big Data: Does Size Matter?. Harkness has co-written and performed comedy shows related to science and mathematics, including collaborations with her mother Linda Cotterill and comedian Matt Parker. Since 2016, she has chaired the Data Debate series for the Alan Turing Institute and the British Library. She is a Fellow of the Royal Statistical Society and a visiting fellow at the University of Winchester's Centre for Information Rights.

Education
Harkness has a BA in Film and Drama with Art from Bulmershe College and a BSc in Mathematics & Statistics from the Open University, awarded in 2017. In 2021 she began postgraduate study in Philosophy at Birkbeck College, London, completing an MA in 2023.

Career
Writer
Harkness has written about technology for BBC Science Focus magazine, about statistics for Significance (a popular science magazine published by the Royal Statistical Society), and about motorcycles for The Daily Telegraph.
Harkness is the author of the book Big Data: Does Size Matter? Her second book Technology is not the Problem was published in May 2024.

Broadcaster
Harkness' work for BBC Radio 4 includes an afternoon play, documentaries, being a roving reporter for The Human Zoo and presenting FutureProofing, a series about the future potential of science.

Performer
In 1999 Harkness co-wrote a comedy with her mother Linda Cotterill, called No Future in Eternity, about an astronomer who shares a flat with two angels.  They received a grant from the Astronomer Royal for Scotland, John Campbell Brown, to perform the show at the Edinburgh Festival Fringe in 2000.  The show was subsequently broadcast as an afternoon play on BBC Radio 4.
Since 2004 Harkness has collaborated with Dr Helen Pilcher, as a comedy duo named the Comedy Research Project, writing and performing stand-up shows about science.
In 2012 Harkness and fellow comedian Matt Parker co-wrote a comedy show called Your Days are Numbered: The Maths of Death.  They performed the show in Australia, at the Adelaide Fringe and Melbourne International Comedy Festival, on tour around England and in Scotland, at the Edinburgh Festival Fringe.
Harkness returned to the Edinburgh Festival Fringe in 2019 to perform a one-woman show, called Take a Risk.

Data engagement
Since 2016, Harkness has chaired the Data Debate for the Alan Turing Institute and the British Library, a series of panel discussions about big data and its implications for society.  She has spoken at the Hay Festival, the Battle of Ideas, TEDx, The Scotsman Data Conference and the Cheltenham Science Festival.
Harkness is a Fellow of the Royal Statistical Society (RSS), a founding member of their special interest group on Data Ethics. She was elected to serve on the Council of the RSS from 2024, and took over the chair of the editorial board of their magazine Significance at that time. Harkness is also a visiting fellow at the University of Winchester's Centre for Information Rights.

Awards
In 1997, Harkness won a column-writing competition, organised by The Independent newspaper.

Publications
Big Data: Does Size Matter (2016)

Radio
No Future In Eternity (2001)
Data, Data, Everywhere (2014)
Personality Politics (2014)
FutureProofing (2014–present)
Hindsight Bias (2015)
Perfect People (2015)
Supersense Me (2017)
The Why Factor: Are You A Numbers Person? (2017)
How to Disagree: A Beginner's Guide to Having Better Arguments (2018)
The Infinite Monkey Cage: Big Data (2018)
Divided Nation (2019)
What Has Sat-Nav Done to Our Brains? (2019)
Five Knots (2020)
Steelmanning (2021)

Comedy
The Comedy Research Project (2004)
Your Days Are Numbered - The Maths of Death (2012)
Brainsex (2013)
Take A Risk (2019)


== References ==

Title: Jane Hunter (scientist)
Jane Hunter was the interim Director of the Australian Urban Research Infrastructure Network (AURIN). She has held roles such as the Director of University of Queensland's e-Research Lab and Chair of the Australian Academy of Science's National Committee for Data in Science; Vice-President of the National Executive Committee for Digital Humanities; and Member of Scientific Committee of the ICSU World Data System. E-research has emerged through the exponential expansion of information technologies.  New online tools, networks, data capture, management and visualisation techniques are needed to enhance collaboration and data sharing between researchers who need access to very large data collections, high-performance analysis and modelling particularly across disciplines.
Her research projects cover a wide range of topics and include the OzTrack Project looking at how to store, analyse and visualise animal tracking data;   The Twentieth Century in Paint a multidisciplinary project to inform the preservation of modern art;  HuNI - Humanities Networked Infrastructure applying new data tools to integrate Australia's most significant cultural datasets for humanities researchers.  In speaking of a joint project on Queensland's waterways, Hunter notes, 'There is a massive amount of monitoring data being collected, and Health-e-Waterways provides a high-tech approach to turning that data into meaningful information".

References
External links
AURIN. Australian Urban Research Infrastructure Network – Australia's Urban Intelligence Network
Information about Jane Hunter
Data Science - School of Information Technology and Electrical Engineering - The University of Queensland, Australia
Academic Programs
list of Professor Hunter's publications

Title: Jean Innes (scientist)
Jean Elizabeth Innes is a British technologist who is the chief executive officer of the Alan Turing Institute. She uses data science and artificial intelligence to solve global challenges. She previously served as director of strategy at Faculty and in HM Treasury.

Early life and education
Innes studied chemistry at Imperial College London. She moved to the University of Cambridge for her doctoral research and worked as a postdoctoral researcher on a Lindemann Fellowship.

Research and career
Innes joined HM Treasury as a policy researcher. She worked with several ministers and held various roles across the civil service. She joined Rightmove as Director of Consumer Data, where she deployed artificial intelligence at scale and developed relationships with Amazon. Innes joined Faculty, where she was made director of transformation and strategy and acted as an advisor for the World Economic Forum.
In 2023 Innes joined the Alan Turing Institute as the chief executive officer.


== References ==

Title: J. Alberto Aragon-Correa
J. Alberto Aragon-Correa (also known as Alberto Aragon and Juan Alberto Aragón Correa) is a business data scientist and the current Professor of Management and Director of the Talent Incubator at the University of Granada. Aragon-Correa serves as the founding director of three endowed chairs dedicated to providing complimentary mentoring and resources to university students, focusing on the areas of leadership and sustainability.
Previously Aragon-Correa was an Honorary Professor of Management and Professor of International Business at the University of Surrey (United Kingdom). He was also a guest visiting scholar at University of California, Los Angeles (USA), University of California, Berkeley (USA), and ETH Zurich (Switzerland).

Awards and distinctions
Aragon-Correa was awarded with the Academy of Management «ONE Distinguished Scholar Award», a distinction designed to recognize scholars who have assumed leadership roles within the field.
His research has gained recognition, featuring in the "List of Top Two Percent Scientists in the World" published by Prof. Ioannidis (Stanford University) and colleagues. Additionally, Aragon-Correa was honored with the Academy of Management “ONE Services Award” for his leadership as Chair of the Organizations and Natural Environment Division within the Academy of Management. Furthermore, he has achieved finalist status twice for the Carolyn Dexter Award, which acknowledges outstanding papers that contribute to the internationalization of management research. 

Aragon-Correa's work has also garnered attention from national and international media.  One of his works achieved a gross audience of 89.7 million people, securing coverage in 6,310 media outlets and generating an economic impact for his university exceeding  1.8 million euros in advertising terms.

Publications
Aragon-Correa's research has been published in different journals in the fields of business and management. As of December 2023, his works have received 13744 citations according to Google Scholar. Some of his most-cited publications include:

Aragón-Correa, J.A. Sharma, S. (2003): A contingent resource-based view of proactive corporate environmental strategy], Academy of Management Review, vol. 28, 1, pp. 71–88 (https://doi.org/10.5465/amr.2003.8925233).
Aragón-Correa, J.A. (1998): Strategic proactivity and firm approach to the natural environment], Academy of Management Journal, vol. 41, 5, pp. 556–567 (https://doi.org/10.5465/256942).
Aragón-Correa, J.A., García-Morales, V.J., Cordón-Pozo, E. (2007): Leadership and organizational learning's role on innovation and performance: Lessons from Spain, Industrial Marketing Management, vol. 36, 3, pp. 349–359  (https://doi.org/10.1016/j.indmarman.2005.09.006).
Aragón-Correa, J.A., Hurtado-Torres, N.E., Sharma, S., García-Morales, V.J. (2008): Environmental strategy and performance in small firms: A resource-based perspective], Journal of Environmental Management, vol. 86, 1, pp. 88–103 (https://doi.org/10.1016/j.jenvman.2006.11.022).
Aragón-Correa, J. A., Marcus, A., & Hurtado-Torres, N. (2016). The natural environmental strategies of international firms: Controversies and new evidence on performance and disclosure. Academy of Management Perspectives, 30 (1), 24–39. (https://doi.org/10.5465/amp.2014.0043).
Aragon-Correa, J.A., Marcus, A., Rivera, J., Kenworthy, A. (2017): Sustainability management teaching resources and the challenge of balancing planet, people, and profits, Academy of Management Learning & Education, 16 (3), 469–483 (https://doi.org/10.5465/amle.2017.0180).
Leyva-de la Hiz, D. I., Ferron-Vilchez, V., & Aragon-Correa, J. A. (2019). Do firms’ slack resources influence the relationship between focused environmental innovations and financial performance? More is not always better. Journal of Business Ethics, 159 (4), 1215–1227 (https://doi.org/10.1007/s10551-017-3772-3).
Aragon-Correa, J.A., Marcus, A., & Vogel, D. (2020). The institutional challenges of regulation: The heterogeneous effects of mandatory and self-regulatory pressures on firm’s environmental strategies. Academy of Management Annals, 14 (1), 339–365 (https://doi.org/10.5465/annals.2018.0014).
Aguilera, R., Aragon-Correa, J.A., Marano, V. (2021): The corporate governance of environmental sustainability. Journal of Management, 47 (6), 1468–1497 (https://doi.org/10.1177/0149206321991212).
Ellimäki, P., Aguilera, R. V., Hurtado-Torres, N. E., & Aragón-Correa, J. A. (2023). The link between foreign institutional owners and multinational enterprises’ environmental outcomes. Journal of International Business Studies, 1–18 (https://doi.org/10.1057/s41267-022-00580-0).

Research experience
Aragon-Correa's research examines firms’ business strategies, especially those related to the connections between innovation, governance, and sustainability in multinational firms. Aragon-Correa's two most cited papers have been published in the Academy of Management Review and the Academy of Management Journal respectively. Three of his most recent and popular works have been published in the Journal of Management, Academy of Management Review, and the Journal of International Business Studies. Most of his publications have used samples of business data and multivariable analytic methodologies. The previous section of this article includes a detailed list of his works.

Additional experience
Aragon-Correa has played a leadership role in various academic international initiatives. Notably, he was the founding President of the Group of Research on Organizations and the Natural Environment (GRONEN), an international network of scholars dedicated to fostering "relevant research with the highest levels of rigor". Additionally, he served as a member of the steering committee of the Organizations and Natural Environment's (ONE) Division within the Academy of Management for five years, eventually assuming the role of chair for this division.
He currently holds the position of Editor for the Cambridge University Press book series titled ″Organizations and the Natural Environment″ (in collaboration with Professor Jorge Rivera). Aragon-Correa also serves as a Consulting Editor for Organization & Environment, a journal focusing on sustainability and management published by SAGE Publishing. Previously, he held the position of Editor-in-Chief of Organization & Environment, in collaboration with Professor Mark Starik, from 2013 to 2016.

References
External links
J. Alberto Aragon-Correa's personal webpage at University of Granada
J. Alberto Aragon-Correa's official profile at University of Granada
J. Alberto Aragon-Correa at University of Surrey
J. Alberto Aragon-Correa at LinkedIn
J. Alberto Aragon-Correa at Google Scholar
J. Alberto Aragon-Correa at ResearchGate
J. Alberto Aragon-Correa at  X Corp.
J. Alberto Aragon-Correa at Cambridge University Press book series
J. Alberto Aragon-Correa at Talent Incubator

Title: Gareth M. James
Gareth Michael James is the John H. Harland Dean of Emory University's Goizueta Business School.

Early life and education
Gareth M. James is a native of New Zealand. In 1994, he earned a bachelor of science and a bachelor of commerce from the University of Auckland, New Zealand, where he majored in statistics and finance. He was awarded a Fulbright Scholarship to study in the United States and attended Stanford University, where he earned a Ph.D. in statistics in 1998.

Career
Gareth James is renowned for his visionary leadership, statistical mastery, and commitment to the future of business education. 
James joined the faculty of the Information and Operations Management department at the USC Marshall School of Business in 1998. In 2013, he became professor of data sciences and operations and was named E. Morgan Stanley Chair in Business Administration in 2014. He served as vice dean for faculty and academic affairs from 2013 to 2017 before being named interim dean in 2019.  
In July 2022, James became the John H. Harland Dean of Emory University's Goizueta Business School. 
James is a noted scholar and researcher. His extensive published works include numerous articles, conference proceedings, and book chapters focused on statistical and machine learning methodologies. His work has been cited more than 20,000 times. James is also co-author of the extremely successful textbook, An Introduction to Statistical Learning. He has led multiple National Science Foundation research grants and has served as an associate editor for five top research journals. The recipient of two Dean’s Research Awards from the Marshall School of Business, he is a life member, and elected Fellow, of the American Statistical Association and the Institute of Mathematical Statistics.
His many accolades also encompass honors for his superb teaching and mentoring. James is a recipient of the Evan C. Thompson Faculty Teaching and Learning Innovation Award and three-time winner of the Marshall School of Business’ Golden Apple Award for best instructor in the full-time MBA program. He has also been awarded Marshall and USC’s highest honors for mentoring junior colleagues and graduate students, including the Dean’s Ph.D. Advising, USC Mellon, Evan C. Thompson and Provost’s Mentoring awards.

Family
James is married and has two children. His wife is a member of the public health faculty at UCLA.


== References ==

Title: Antonino Letteriello
Antonino (Nino) Letteriello (born August 11, 1979, in Italy) is the first President of the Data Association Management Italy Chapter and European Coordinator for the International Data Association Management.
Letteriello started his career in the public transport industry in 2006, working for transport networks including Metropolitana Milanese and Transport for London before assuming the role of Head of Programme Management for the London 2012 Olympic and Paralympic Games. He was invited to the UK Olympic Expert Panel by then President of London 2012 Lord Coe.
He founded Enne Limited in 2014 and is also founder and CEO of FIT Strategy. Since 2020, Letteriello has been a member of the Massachusetts Institute of Technology's CDO Symposium Programme Committee and in 2021 was nominated High Level Facilitator Tracker for policy on building confidence and security in the use of ICTs. 
Holder of a Data Management Excellence Award, in 2021, Letteriello was nominated as one of the DataIq 100 most influential people in data. He has also written for the World Economic Forum. He lives in Modena with his wife Renata and their son.


== References ==

Title: Giorgia Lupi
Giorgia Lupi is an Italian information designer, a partner at design firm Pentagram, and co-founder of research and design firm Accurat. She is a co-author of Dear Data, a collection of hand drawn data visualizations, along with information designer Stefanie Posavec. Her work is also part of the permanent collection at the Museum of Modern Art.

Early life and education
Lupi was born 1981 in Modena, Italy. When she was a little girl she would spend a significant amount of time collecting and organizing all kinds of items into folders: colored sheets of papers, tiny stones, pieces of textiles from her grandmothers buttons, sales receipts and so much more grew in her collection. She has said she took pleasure in organizing and categorizing her treasures based on their, sizes, color and dimensions. She has said that her childhood interest in numbers, cataloguing and classifying rules and systems explains the origin of her work and her desires to play with data. These interests have also included the scales of large cities and urban mapping projects, and representing information layers underlining an architecture project.
She graduated from FAF in Ferrara, Italy, where she studied architecture. Lupi has her masters in architecture, but has not built any houses during her schooling career. An architect's job is not to build buildings, but they design representations of buildings, images of building's following a system of symbols that convey information about how to manufacture them.  After graduating in 2006 She worked with two different interaction design firms in Italy, mostly working on interactive installations and mapping projects showing off difficult systems of knowledge. In 2011 she began her PhD in design at Milan Politecnico and started Accurat. In 2012 she moved to New York City where she lives now.

Career
In 2011, Lupi co-founded research and design firm Accurat, that combines design and data to create data visualizations, interfaces, and tools. Among their clients are Google, IBM, Bill & Melinda Gates Foundation, Starbucks, United Nations, the World Economic Forum and the Museum of Modern Art. Lupi's influences for her work come from fascinations by geometrical feel and balance of abstract art compositions. Lupi's work has been influenced by data visualization and data art by Moritz Stefaner, Aaron Koblin and Jer Thorp. What drives Lupi in her career is the overlapping space between intuition and analysis, between beauty and logic, numbers and images.
In 2014 Lupi began the Dear Data Project with Stefanie Posavec. Every week for one year Lupi and Posavec exchanged a "data drawing", a hand drawn data visualization that represented a part of their daily life, through the mail. In 2016 these postcards were compiled and published in a book called Dear Data. The following year the Museum of Modern Art added the original Dear Data postcards to the Museum's collection.
In 2016, Giorgia Lupi published an article in Print Mag in which she introduced the concept of Data Humanism, which she further developed in her TED Talk. 
In 2019, Lupi joined Pentagram New York as a partner.

Long Covid
In December 2023, she published her experience with Long Covid as a visual story in The New York Times.

Awards
2022: National Design Award by Cooper Hewitt, Smithsonian Design Museum
2015: Information is Beautiful Awards - Gold Medal and Most Beautiful Project.
2013: Information is Beautiful Awards - Gold Medal, category: Data Visualization.
2013: Strata Conference - Data Journalism Award.
2013: Cannes Festival - Bronze Lion, Direct Advertising.


== References ==

Title: Nita Madhav
Nita Madhav is an epidemiologist and risk modeler who was CEO of Metabiota from 2019 - 2022.

Education
Madhav graduated from Yale University in 2002 with degrees in ecology and evolutionary biology, and received her Master's in Public health from Emory University in 2005.

Career
After time at the Centers for Disease Control and Prevention and AIR Worldwide, Madhav joined Metabiota to work on infectious disease modeling and data science. During the ensuing five years, Madhav's team worked on models to predict epidemiological preparedness and readiness of economies to absorb losses experienced in these extreme, "Black swan theory" events. These models were formalized into the Epidemic Preparedness Index.
Along with colleagues from Stanford and Metabiota, Madhav co-authored a chapter on pandemic preparedness for the World Bank.
She was promoted to CEO in 2019.
In 2020, Madhav remarked that Metabiota's AI-powered models were capable of forecasting epidemics based on both formal and informal data sources.


== References ==

Title: Mauro Martino
Mauro Martino is an Italian artist, designer and researcher. He is the founder and director of the Visual Artificial Intelligence Lab at IBM Research, and Professor of Practice at Northeastern University.

Career
He graduated from Polytechnic University of Milan, and was a research affiliate with the Senseable City Lab at MIT. Mauro was formerly an Assistant Research Professor at Northeastern University working with Albert-Laszlo Barabasi at Center for Complex Network Research and with David Lazer and Fellows at The Institute for Quantitative Social Science (IQSS) at Harvard University.

His works have been published in "The Best American Infographics" in the 2015 and 2016 editions and have been shown at international festivals and exhibitions including Ars Electronica, RIXC Art Science Festival, Global Exchange at Lincoln Center, TEDx Cambridge THRIVE, TEDx Riga, and the Serpentine Gallery. His work is in the permanent collection at Ars Electronica Center. In 2017, Martino and his team received the National Science Foundation's award for Best Scientific Video for the project Network Earth. In 2019, Martino and Luca Stornaiuolo won the 2019 Webby People's Voice Award in the category NetArt for the project AI Portraits.
The project 150 Years of Nature won multiple awards such as Fast Company - Innovation by Design Awards Best Data Design 2020, Webby Award 2020, Webby People's Voice Award 2020. This project, along with other works created in collaboration with Barabási Lab (e.g., Wonder Net, A Century of Physics, Data Sculpture in Bronze, Control, Resilience, Success in Science, Fake News), was shown at the "Barabási Lab. Hidden Patterns" exhibitions at ZKM Center for Art and Media and Ludwig Museum - Museum of Contemporary Art, Budapest.
Mauro Martino is a pioneer in the use of the artificial neural network in sculpture.

Notable works
Strolling Cities   is an interactive AI art project created in collaboration with Politecnico di Milano and exhibited in the Italian Pavilion at the 2021 Venice Biennale. Strolling Cities is the first project that "unites generative AI, human voice, poetry and urban landscape." The neural network model was trained on "millions of photos taken during the recent lockdowns (’20/’21) that show the urban space as an unfiltered landscape of walls, streets, and buildings." One of the main characteristics of this project is that Italian cities appear empty, without their population. The project makes it possible to "manipulate the urban landscape with the voice, using any kind of vocal input, from simple utterances to more complex phrases." The video installations used in the exhibit featured 9 Italian cities: Bergamo, Bologna, Catania, Como, Florence, Genoa, Milan, Palermo, Rome and Venice. The authors whose poems appeared in the installation included Alda Merini, Giulia Niccolai, Stefano Benni, Giorgio Caproni, Cesare Pavese, Goffredo Parise, and Valerio Magrelli.
150 Years of Nature is a data visualization project created in collaboration with Barabási Lab and dedicated to the 150th Anniversary of Nature.  It includes an interactive tool, "data movie" and the cover showing the co-citation network of Nature publications throughout its history. 150 Years of Nature was the Webby Award Winner in 2020; it was awarded the Best Data Design at Fast Company - Innovation by Design Awards. The project is exhibited as part of the "Barabási Lab. Hidden Patterns" exhibitions at ZKM Center for Art and Media and Ludwig Museum - Museum of Contemporary Art, Budapest.
AI Portraits  is a research project that uses artificial neural network to reconstruct a portrait of a person. The AI system was trained on a dataset that included millions of photos of actors and actresses.
Wonder Net  was developed in collaboration with Albert-László Barabási at the Center for Complex Network Research at Northeastern University. Wonder Net includes 8 data sculptures which represent 8 different "data-stories" (e.g., art network, flavor network, fake news network, etc.). It was presented at the IEEE VIS 2018 Arts Program in Berlin.
Forma Fluens  uses the world’s largest doodle data set by Google Quick Draw. This project was presented at 123 DATA design exhibition in Paris.
Charting Culture maps cultural mobility, tracking the births and deaths of notable individuals, from 600 BC to the present day. Charting Culture is one of the most viewed videos of the Nature Video channel on YouTube with over 1.3 million views. This project is part of the Places & Spaces: Mapping Science exhibit. It was also featured in "The Best American Infographics 2015".
News Explorer is a web application providing new interface for news analysis and discovery.
Network Earth won the 2017 Best Scientific Video award of the National Science Foundation. Network Earth explores nature's resilience and interconnections between all life on Earth. It accompanied a research paper published in Nature.
Rise of Partisanship shows the party polarization of the House of Representatives through time. This project was included in "The Best American Infographics 2016".
Redrawing the map of Great Britain from a network of human interactions explored a new approach to regional delineation, based on analyzing networks of billions of individual human transactions.

Awards
2020: Fast Company – Innovation by Design Awards, Best Data Design: 150 Years of Nature 
2020: Webby Award Winner: 150 Years of Nature, Webby People's Voice Award Winner: 150 Years of Nature 
2019: Webby People's Voice Award – Winner, NetArt, AI Portraits 
2017: Vizzies Visualization Challenge by National Science Foundation and Popular Science – Winner, Best Scientific Video, Network Earth
2017: Kantar Information is Beautiful Award – Honorable Mention, Unusual, Forma Fluens 
2016: Innovation by Design Award by Fast Company, Finalist for Websites & Platforms, Watson News Explorer 
2016: Kantar Information is Beautiful Award – Silver Medal, Commercial Project, IBM Watson News Explorer 
2015: Kantar Information is Beautiful Award – Gold Medal in Data visualization, Rise of Partisanship 
2015: Kantar Information is Beautiful Award – Honorable mention - Motion Infographic, Charting Culture


== References ==

Title: Veronika Megler
Veronika Margaret Megler (born 14 October 1960) is an Australian computer scientist. As of 2024, Megler is a principal data scientist at Amazon.com, and is known as the co-developer of The Hobbit, a 1982 text adventure game adapted from the novel by J. R. R. Tolkien.

Education
Megler was born in 1960, and educated in Melbourne at Mac.Robertson Girls' High School, where she was school valedictorian in science. She began studying science at the University of Melbourne, intending to major in statistics but switching to a computer science major which she found more enjoyable.

Beam Software
Megler became the first employee at video game development studio Beam Software/Melbourne House as a programmer. She recruited Philip Mitchell and the two began working on an illustrated interactive fiction game based on The Hobbit by J. R. R. Tolkien, with Megler concentrating on the game's physics system and a measure of autonomy for non-player characters.
This game was structured to be used as the basis for other games as an early adaptable game engine. The game was released in 1982 in the UK and Australia.
Megler and Mitchell also developed another Beam game, Penetrator, also released in 1982.

Computer science career
Prior to her graduation from the University of Melbourne, Megler resigned from Beam to concentrate on her studies, and Mitchell remained to complete the ZX Spectrum version. Megler worked at IBM as an information technology architect, operating system expert and consultant. In 2009, she left IBM to study for a master's degree and PhD in computer science for scientific big data at Portland State University.
As of February 2024, Megler lives in Portland, Oregon and is a principal data scientist at Amazon.com.  In her interview in MLinProduction in 2020 as part of a series with creators of top machine learning resources, she explains that this recent work focuses on managing larger and higher-impact machine learning projects.

Publications and journal articles
Megler, V.M., Maier, D. (2011). Finding Haystacks with Needles: Ranked Search for Data Using Geospatial and Temporal Characteristics. In: Bayard Cushing, J., French, J., Bowers, S. (eds) Scientific and Statistical Database Management. SSDBM 2011. Lecture Notes in Computer Science, vol 6809. Springer, Berlin, Heidelberg.
V. M. Megler and D. Maier, "Data Near Here: Bringing Relevant Data Closer to Scientists," in Computing in Science & Engineering, vol. 15, no. 3, pp. 44–53, May–June 2013.
Veronika Megler, David Banis, Heejun Chang, Spatial analysis of graffiti in San Francisco, Applied Geography, Volume 54, 2014, p 63-73, ISSN 0143-6228.

References
External links
Official website
Megler, Veronika M. (2014). "There and Back Again: A Case History of Writing The Hobbit".
Google Scholar listing

Title: Solomon Messing
Solomon Messing is a researcher and data scientist known for his work on how algorithms and social information embedded in new technologies affect the way people understand the political world. He was the founding Director of Pew Research Center's Data Labs, research scientist at Facebook and Twitter, chief scientist at Acronym,
and is now Research Associate Professor at New York University.
Messing's work quantifying media polarization and filter bubbles was published in Science and has been influential in the field of political communication and sparked media commentary on the role of networks and algorithms in the media ecosystem. His work on how people understand election forecasting was the subject of public debate about the role of election forecasting in the democratic process and was cited by FiveThirtyEight's Politics Podcast as a reason for changing the forecast from percent change of winning to odds.
He also led the technical effort at Facebook to release perhaps the largest ever social media data set for research, which relied on a controversial technology, differential privacy, to protect data from malicious actors. 
Messing earned his PhD in 2013 as well as a master's degree in Statistics from Stanford University.

Most cited peer-reviewed journal articles
Bakshy E, Messing S, Adamic LA. Exposure to ideologically diverse news and opinion on Facebook. Science. 2015 Jun 5;348(6239):1130-2. cited 2441 times in Google Scholar
Messing S, Westwood SJ. Selective exposure in the age of social media: Endorsements trump partisan source affiliation when selecting news online. Communication Research. 2014 Dec;41(8):1042-63. cited 925 times in Google Scholar 
Grimmer J, Messing S, Westwood SJ. How words and money cultivate a personal vote: The effect of legislator credit claiming on constituent credit allocation'  American Political Science Review. 2012 Nov;106(4):703-19.cited 311 times in Google Scholar 
Bond R, Messing S. Quantifying social media’s political space: Estimating ideology from publicly revealed preferences on Facebook. American Political Science Review. 2015 Feb;109(1):62-78. cited 182 times in Google Scholar 


== References ==

Title: Joachim Meyer (professor)
Joachim Meyer (Hebrew: יואכים מאיר; born: 1957) is Celia and Marcos Maus Professor for Data Sciences at the Department of Industrial Engineering at Tel-Aviv University. His work deals with human decisions in interactions with intelligent systems and he is a fellow of the Human Factors and Ergonomics Society.

Early life and education
Joachim Meyer was born in Mülheim an der Ruhr, Germany. He emigrated to Israel in 1973, completed high school at the Kanot agricultural school, did IDF service and was discharged  as a lieutenant.
Meyer began his academic studies in the Department of Behavioral Sciences at Ben-Gurion University of the Negev in 1979 and received his B.A. in 1982 (with honors), and M.Sc. in Psychology in 1986 (with honors). He then moved to the  Department of Industrial Engineering and Management at Ben-Gurion University of the Negev where he completed his Ph.D. in 1994. His dissertation on Processing of graphic displays that present quantitative information was supervised by David Shinar and David Leiser.

Academic career
Meyer was a research fellow at the Faculty of Industrial Engineering at the Technion - Israel Institute of Technology from 1993 until 1997. In 1995 he joined Ben-Gurion University of the Negev as a lecturer in the Department of Industrial Engineering and Management and the School of Management. He was promoted to senior lecturer in 1998, to associate professor in 2003, and to full professor in 2009.
In 2012, Meyer joined the Department of Industrial Engineering at Tel Aviv University as a professor and was chair of the department from 2015 until 2019.
Meyer held visiting academic positions at other universities and research institutes, including Harvard Business School and the Massachusetts Institute of Technology (MIT) where he was a Research Scientist at the Center for Transportation Studies (1999-2001) and helped to establish the MIT AgeLab. He was also a visiting professor at the Human Dynamics Group at the MIT Media Lab (2014-2015).
Meyer has supervised more than 75 graduate students and postdoctoral fellows.

Research
Meyer’s work focuses on modeling human decisions when interacting with intelligent systems. He combines methods and tools from psychology, economics, management science, and engineering, focusing on the development of models for predicting human decision making as a function of properties of the system, the situation, and the user. Applications are in various fields, including transportation, medicine, cyber security, privacy and management.
Meyer’s initial research dealt with the visualization of information, demonstrating that the relative advantage of different forms of visualization depends on multiple factors and in particular on the structure of the displayed information.
Much of Meyer’s research deals with aided decision making and specifically the effects of alarms, alerts and binary cues on decisions. He demonstrated that there are two different types of trust in these systems – compliance, and reliance. He also studies the active role people have in shaping the information they use for decision making, including the adjustment of thresholds of alerts, which tends to be incorrect and may even be practically impossible, because users don't have the necessary information.
In recent years he developed, together with his student Nir Douer, the ResQu (Responsibility Quantification) model of human responsibility, which is a method to quantify the responsibility of a human operator when using a system with advanced automation for tasks such as medical diagnostics,  the identification of cyber-attacks, or the operation of advanced weapon systems. He has also studied behavioral aspects of cyber security, demonstrating that cyber risk taking is the combination of three behaviors (the Triad of Risk Related Behaviors), and developing models of different aspects of cyber security, such as the question when users’ actions should be blocked and when they should be warned.
Prof. Meyer has authored more than 170 scientific publications.

Professional experience
Meyer was the chairperson of the Israeli Society for Human Factors Engineering and Ergonomics during 2005-2008. He was also a member of several committees of the Israeli Standards Institute and a member of the advisory committee of the National Authority for Traffic Safety at the Israel Ministry of Transportation.
Meyer is a member of the editorial board of Human Factors, and was on the editorial board of Journal of Experimental Psychology: Applied. He was also an associate editor of several  journals including the Journal of Cognitive Engineering and Decision Making; IEEE Transactions on Human-Machine Systems; IEEE Transactions on Systems, Man, and Cybernetics – Part A.

Honors and awards
In 2018 Meyer received the Jerome H. Ely Human Factors Article Award for the best paper in the Human Factors Journal. He is a senior member of the Institute of Electrical and Electronics Engineers (IEEE) since 2011, and he was elected as a fellow of the Human Factor and Ergonomics Society in 2020. Since 2021 he holds the Celia and Marcos Maus Chair for Data Sciences at Tel Aviv University.

Selected articles
Ben-Asher, N., & Meyer, J. (2018). The Triad of Risk-related Behavior (TriRB): A three-dimensional model of cyber-risk taking. Human Factors, 60(8), 1163-1178.
Bereby-Meyer, Y., Meyer, J., & Flascher, O. (2002). Prospect Theory analysis of guessing in multiple choice tests.  Journal of Behavioral Decision Making, 15, 313-327.
Botzer, A., Meyer, J., Bak, P., & Parmet, Y. (2010). Cue threshold settings for binary categorization decisions.  Journal of Experimental Psychology: Applied, 16, 1-15.
Douer, N., & Meyer, J. (2020). The Responsibility Quantification (ResQu) model of human in-teraction with automation. IEEE Transactions on Automation Science and Engineering, 17 (2), 1044-1060.
Douer, N., & Meyer, J. (2021). Theoretical, Measured and Subjective Responsibility in Aided Decision Making. ACM Transactions on Intelligent Interactive Systems, 11(1), Article 5
Meyer, J. (2004). Conceptual issues in the study of dynamic hazard warnings.  Human Factors, 46, 196-204.
Meyer, J., & Bitan, Y.  (2002). Why better operators receive worse warnings.  Human Factors, 44, 343-354.
Meyer, J., Dembinsky, O., & Raviv, T. (2020). Alerting about possible risks vs. blocking risky choices: A quantitative model and its empirical evaluation. Computers and Security, 97, 101944.
Meyer, J., & Sheridan, T. B. (2017). The intricacies of user adjustment of system properties. Human Factors, 59 (6), 901-910.
Meyer, J., Shinar, D., & Leiser, D. (1997). Multiple factors that determine performance with tables and graphs. Human Factors, 39, 268-286.
Oron-Gilad, T., Meyer, J., & Gopher, D. (2002). Monitoring dynamic processes with alphanu-meric and graphic displays.  Theoretical Issues in Ergonomic Sciences, 2, 368-389.

External links
Joachim Meyer, Tel-Aviv University
Joachim Meyer, Google Scholar
Caution: Adverse Effects of Big Data on Clinical Practice - Joachim Meyer’s  Technion lecture
On the need to understand human behavior to do analytics of behavior, Joachim Meyer’s talk at Heidelberg University


== References ==

Title: Aleksandra Mojsilovic
Aleksandra (Saška) Mojsilović (born 1968) is a Serbian-American scientist. Her research interests are artificial intelligence, data science, and signal processing. She is known for innovative applications of machine learning to diverse societal and business problems. Her current research focuses on issues of fairness, accountability, transparency, and ethics in AI. She is an IBM Fellow and IEEE Fellow.

Education and career
Mojsilović was born in Belgrade, Serbia. She received her PhD in Electrical Engineering in 1997 from the University of Belgrade, Belgrade, Serbia. From 1997 to 1998, she was an assistant professor at the University of Belgrade. From 1998 to 2000, she was a Member of Technical Staff at the Bell Laboratories, Murray Hill, New Jersey. She was at IBM Research from 2000 to 2023. She is currently employed at Google as their senior director, Responsible AI.  Prior to joining Google, she led Trustworthy AI at IBM Research and served as a co-director of IBM Science for Social Good.

Research
Mojsilović's research interests include artificial intelligence, machine learning, multi-dimensional signal processing, and data science. She has applied her expertise to diverse application areas, including computer vision, multimedia, recommender systems, medical diagnostics, healthcare, IT operations, business analytics, workforce analytics, drug discovery, disease ecology, and most recently, COVID-19 response. A substantial part of her research is focused on development of ethical, responsible, and beneficial AI systems. In 2015, with Kush Varshney, she created IBM Science for Social Good initiative as a way to promote and direct AI research and development towards applications that benefit humanity. She was among the first researchers to call for transparent reporting on the development and deployment of AI models and systems.
While at IBM Research she helped create leading open source and product capabilities in support of fair, explainable, robust, transparent, and responsible AI. Most notable contributions include: AI Fairness 360, a toolkit for mitigating bias in machine learning models, AI Explainability 360, a toolkit for supporting explanations in AI models, and AI FactSheets 360, an open research effort to foster trust in AI by increasing transparency and enabling governance.

Awards and recognition
IEEE Young Author Best Paper Award (2001)
European Conference on Computer Vision Best Paper Award (2002)
Daniel H. Wagner Prize for Excellence in the Practice of Advanced Analytics and Operations Research (2010)
IBM Fellow (2014)
IEEE Fellow (2017)
Computing Community Consortium and Schmidt Futures AI for Good Award (2019)
Belfer Center's Technology and Public Purpose Project Spotlights Outstanding Technologies for Public Good (2020)
100 Brilliant Women in AI Ethics (2020)
Tech Spotlight (2021)

Personal
Mojsilović is the creator of the award-winning food blog Three Little Halves, which blends her love of food, photography, and writing. She was nominated for the James Beard Award and received the International Association of Culinary Professionals Award.
Mojsilović serves on the Board of Directors of Neighborhood Trust Financial Partners, which provides financial literacy and economic empowerment training to low-income individuals.
She lives in New York City with her husband and daughter.


== References ==

Title: Johanna Pirker
Johanna Pirker (born June 26, 1988) is an Austrian computer scientist, educator, and game designer at Graz University of Technology and professor at Ludwig Maximilian University of Munich with a focus on games research, virtual reality and data science. Pirker was listed on the Forbes 30 under 30 Europe list in the category Science & Healthcare (2018) for her efforts in improving digital education with virtual reality environments and games. She holds a Ph.D. in Computer Science. Her dissertation was supervised by the Austrian e-learning expert Christian Gütl and MIT professor of physics John Winston Belcher. She is involved in various efforts to educate about the potential of video games. This also includes efforts to advocate the background and cultural aspect of video games. In 2020 she received the Käthe-Leichter Prize for her efforts for her initiatives in the field of diversity in engineering and in the games industry. In 2021 she received the Hedy Lamarr Prize. 
In 2022, she was nominated as a member of the founding convention of the Institute of Digital Sciences Austria (IDSA), which is involved in the establishment of the new Technical University in Linz.
Since 2024, she has been a member of the Austrian Council for Research, Science, Innovation, and Technology Development (FORWIT).

Awards and prizes
Pirker received multiple awards including a listing on the Forbes 30 under 30 Europe list for her research and development efforts in the field of virtual reality for learning applications. 

Forbes 30 under 30 Europe, Science & Healthcare (2018)
IGDA Women in Games Ambassador, GDC (2017)
Women in Tech Award by Futurezone (2019)
Käthe-Leichter-Prize (2020)
Hedy Lamarr Prize (2021)

References
External links
Personal Homepage
Johanna Pirker at MobyGames
Publications by Johanna Pirker at ResearchGate

Title: Ganna Pogrebna
Ganna Pogrebna (born 4 July 1980) is a British behavioural data scientist, decision theorist, educator, author, and academic writer. She currently serves as the Lead for Behavioral Data Science at the Alan Turing Institute, the Executive Director of the Artificial Intelligence and Cyber Futures Institute at Charles Sturt University, and an Honorary Professor of Behavioral Business Analytics and Data Science at the University of Sydney.
She is known for her work in combining data science methods with those from economics and psychology to model human behavior under risk and uncertainty.

Education
Pogrebna holds a master's degree in economics from the University of Missouri, Kansas City, and has obtained her Ph.D. in economics and social sciences from the University of Innsbruck, Austria.

Career
Pogrebna is a researcher and academic in behavioral business analytics and data science. She currently serves as Executive Director of the Artificial Intelligence and Cyber Futures Institute at Charles Sturt University in Australia and holds a research professorship position at the University of Sydney Business School. Additionally, she is a Lead of Behavioral Data Science at the Alan Turing Institute in London.
Pogrebna has taught and supervised students in data science, cyber security, behavioral data science, business analytics and artificial intelligence. She has also worked as a consultant for private and public sector companies in various industries. Pogrebna communicates her research through her YouTube blog "Data Driven" and "Inclusion AI. Blog", and is a regular contributor to various blogs and media outlets. She is methods editor at Leadership Quarterly and associate editor of Judgement and Decision Making.

Notable research works
In 2021, her team's box office predicting technology was discussed at the Stockholm Film Festival. In 2018, she co-developed a model predicting parental risk attitudes based on the characteristics of children, revealing gender inequalities in parenting. In 2019, she contributed to a Council of Europe study on the implications of advanced digital technologies for human rights. In 2020-2021, she researched handwashing patterns and protected vulnerable groups during the COVID-19 pandemic. In 2022, Pogrebna's work highlighted the threats and benefits of software products used in schools and at home.

Bibliography
Navigating New Cyber Risks: How Businesses can Plan, Build and Manage Safe Spaces in the Digital Age, co-author (2019)
Big Bad Bias Book (2021)

Awards and honours
British Academy of Management Award (2018)
Pogrebna's work in risk analytics and modeling has been recognized through the Leverhulme Research Fellowship award.
In January 2020, she was named the winner of TechWomen100, an award for leading female experts in Science, Technology, Engineering, and Mathematics in the UK.
She has also been named one of the 20+ Inspiring Data Scientists by AI Time Journal.


== References ==

Title: Bastiaan Quast
Bastiaan Quast is a Dutch-Swiss Machine learning researcher. He is the author and lead maintainer of the open-source rnn and transformer deep-learning frameworks in the R programming language, and the datasets.load GUI package, as well as R packages on Global Value Chain decomposition & WIOD and on Regression Discontinuity Design. Quast is a great-great-grandson of the Nobel Peace Prize laureate Tobias Asser.

Early life and education
Bastiaan Quast graduated from University of Groningen with a bachelor's degree in Economics and bachelor's degree in Theoretical philosophy. He holds a master's degree in Econometrics from the University of St. Gallen He obtained his Ph.D from the Graduate Institute of International and Development Studies with advisors Richard Baldwin and Jean-Louis Arcand, his work on local languages and internet usage was discussed at the 2017 G20 meeting in Germany.

Career
Quast is an functionary of the United Nations at the International Telecommunication Union, as Secretary of the ITU-WHO Focus Group on Artificial Intelligence for Health and AI for Good.
Bastiaan Quast created the popular machine learning framework rnn in R, which allows native implementations of recurrent neural network architectures, such as LSTM and GRU (>100,000 downloads). While working at UNCTAD, Quast developed the popular package datasets.load, which is part of the top 10% of most downloaded R packages (>100,000). The R packages decompr and wiod have been downloaded >20,000 times.

Bibliography
Kummritz, Victor; Quast, Bastiaan (2017). Global value chains in developing economies. London, United Kingdom: VoxEU.

References
External links
Bastiaan Quast

Title: Gabriela de Queiroz
R-Ladies is an organization that promotes gender diversity in the community of users of the R statistical programming language. It is made up of local chapters affiliated with the worldwide coordinating organization R-Ladies Global.

History
On October 1, 2012, Gabriela de Queiroz, a data scientist, founded R-Ladies in San Francisco (United States) after participating in similar free initiatives through Meetup. In the following four years, three other groups started: Taipei in 2014, Minneapolis (called “Twin Cities”) in 2015, and London in 2016. The chapters were independent until the 2016 useR! Conference, where it was agreed to create a central coordinating organization. In that year, Gabriela de Queiroz and Erin LeDell of R-Ladies San Francisco; Chiin-Rui Tan, Alice Daish, Hannah Frick, Rachel Kirkham and Claudia Vitolo of R-Ladies London; as well as Heather Turner joined to apply for a grant from the R Consortium, with which they asked for support for the global expansion of the organization.
In September 2016, with this scholarship, R-Ladies Global was founded and in 2018 it was declared as a high-level project by the R Consortium. The current leadership team consists of Averi Giudicessi, Athanasia Monika Mowinckel, Shannon Pileggi, Riva Quiroga  and Yanina Bellini Saibene. The leadership team has an overarching role in steering and directing the organization. It works closely together with more than 20 volunteers who take care of various areas such as campaigns, the directory, mentoring, public communications and social media, or the community Slack. As of 2024, the R-Ladies Global community consists of 219 groups in 63 countries.

Organization
R-Ladies meetings are organized around workshops and talks, led by people that identify as female or as gender minorities (including but not limited to cis/trans women, trans men, non-binary, genderqueer, agender, pangender, two-spirt, gender-fluid, neutrois). The organization is coordinated by the Global Team. The chapters, however, operate decentralized and new chapters can be founded by anyone using the publicly available "starter-kit".
R-Ladies groups aim to promote a culture of inclusion within their events and community. In addition, they promote gender equality and diversity in conferences, in the workplace, collaboration among gender minorities, and analysis of data about women.
The Directory is a public directory listing 1,267 profiles of R-Ladies (in 2024). R-Ladies Global also showcases blogs written and maintained by their members. 
R-Ladies also collaborates with other projects, such as NASA Datanauts or PyLadies.

Gabriela de Queiroz
Gabriela de Queiroz is the Director of AI at Microsoft and the founder of first R-Ladies chapter, AI Inclusive and co-founder of R-Ladies Global. Before, she also worked at IBM as a chief data scientist.

Early life and education
She was raised in Brazil and received her bachelor's degree in statistics from Rio de Janeiro State University. She has a master’s in epidemiology at Oswaldo Cruz Foundation and another one in statistics at California State University, East Bay. de Queiroz moved to the United States in 2012 to begin her master's degree in statistics at California State University, East Bay.

Achievements
Interested in creating an inclusive space for women learning the programming language R, she began a Meetup group in the San Francisco Bay area. Since then, the R-Ladies organization has grown to 219 groups in 63 countries.
In addition to her work with R-Ladies, de Queiroz is an expert in machine learning and led IBM's AI Strategy and Innovations team. Her team contributed to projects such as TensorFlow. de Queiroz was a finalist of the Women in Open Source Award by Red Hat in 2019, named 40 under 40 by CSUEB as well as listed among the 100 Brilliant Women in AI Ethics™ in 2023.

Notable members
Julia Stewart Lowndes – Marine ecologist


== References ==

Title: Karthik Ram
Karthik Ram is a research scientist at the Berkeley Institute for Data Science and member of the Initiative for Global Change Biology at the University of California, Berkeley. He is best known for being the co-founder of rOpenSci. Ram's work focuses on global change, data science, and open research software.

Career
Ram received his PhD in Ecology and Evolution at University of California, Davis. After his PhD, he went on to hold a post-grad position at University of California, Santa Cruz before eventually moving to UC Berkeley. Currently, Karthik Ram is a research scientist at UC University of California, Berkeley for both the Berkeley Institute for Data Science and the Berkeley Initiative for Global Change Biology. His work aims to make it easier for scientists to produce reproducible research.

Projects
Ram co-created rOpenSci in 2011, and is currently the lead of the project. rOpenSci is non-profit with the goal of making data retrieval more accessible through open source R packages. He also a member of the peer-review and editorial staff for the rOpenSci Software Review. Ram is the lead principal investigator for the URSSI (US Research Software Sustainability Institute). Karthik Ram has been the lead of this project since its initial grant in December, 2017. Ram is a founding editor of the Journal of Open Source Software, and an editorial board member of ReScience C and Research Ideas and Outcomes, which are both academic journals focused on open research sustainability.

Awards
Leamer-Rosenthal Prizes for Open Social Science (2007)
rOpenSci awarded $2.9M grant from The Leona M. and Harry B. Helmsley Charitable Trust

Notable works
Among Ram's notable work are the following:

Point of view: How open science helps researchers succeed
Git can facilitate greater reproducibility and increased transparency in science
Data carpentry: workshops to increase data literacy for researchers


== References ==

Title: Seth Rich
The murder of Seth Rich occurred on July 10, 2016, at 4:20 a.m. in the Bloomingdale neighborhood of Washington, D.C. Rich died about an hour and a half after being shot twice in the back. The perpetrators were never apprehended; police suspected he had been the victim of an attempted robbery.
The 27-year-old Rich was an employee of the Democratic National Committee (DNC), and his murder spawned several right-wing conspiracy theories, including the false claim, contradicted by the law enforcement branches that investigated the murder, that Rich had been involved with the leaked DNC emails in 2016. It was also contradicted by the July 2018 indictment of 12 Russian military intelligence agents for hacking the e-mail accounts and networks of Democratic Party officials and by the U.S. intelligence community's conclusion the leaked DNC emails were part of Russian interference in the 2016 United States elections. Fact-checking websites like PolitiFact, Snopes, and FactCheck.org stated that the theories were false and unfounded. The New York Times, the Los Angeles Times and The Washington Post wrote that the promotion of these conspiracy theories was an example of fake news.
Rich's family denounced the conspiracy theorists and said that those individuals were exploiting their son's death for political gain, and their spokesperson called the conspiracy theorists "disgusting sociopaths". They requested a retraction and apology from Fox News after the network promoted the conspiracy theory, and sent a cease and desist letter to the investigator Fox News used. The investigator stated that he had no evidence to back up the claims which Fox News attributed to him. Fox News issued a retraction, but did not apologize or publicly explain what went wrong. In response, the Rich family sued Fox News in March 2018 for having engaged in "extreme and outrageous conduct" by fabricating the story defaming their son and thereby intentionally inflicting emotional distress on them. Fox News reached a seven-figure settlement with the Rich family in October 2020.

Seth Rich
Early life and career
Rich grew up in a Jewish family in Omaha, Nebraska. He volunteered for the Nebraska Democratic Party, interned for Senator Ben Nelson, was active in Jewish outreach, and worked with the United States Census Bureau. In 2011, he graduated from Creighton University with a degree in political science. He moved to Washington, D.C., to work for pollster Greenberg Quinlan Rosner. In 2014, he began working for the Democratic National Committee (DNC) as the voter expansion data director. One of his tasks at the DNC was the development of a computer application to help voters locate polling stations.

Shooting and death
On Sunday, July 10, 2016, at 4:20 a.m., Rich was shot about a block from his apartment at the southwest corner of Flagler Place and W Street Northwest in the Bloomingdale neighborhood of Washington, D.C.
Earlier that night, he had been at Lou's City Bar, a sports pub 1.8 miles (2.9 km) from his apartment, in Columbia Heights, where he was a regular customer. He left when the bar was closing, at about 1:30 or 1:45 a.m. Police were alerted to gunfire at 4:20 a.m. by an automated gunfire locator. Less than one minute after the gunfire, police officers found Rich conscious with gunshot wounds. He was taken to a nearby hospital, where he died over 1+1⁄2 hours after being shot. That he was not shot in the head is seen as evidence against the shooting having been an "assassination". According to police, he died from two shots to the back and may have been killed in an attempted robbery. Residents noted the neighborhood had been plagued by robberies.
Rich's mother told NBC's Washington affiliate WRC-TV: "There had been a struggle. His hands were bruised, his knees are bruised, his face is bruised, and yet he had two shots to his back, and yet they never took anything ... They didn't finish robbing him, they just took his life." The police told the family they had found a surveillance recording showing a glimpse of the legs of two people who could be the killers.

Aftermath
On the day after the shooting, DNC chair Debbie Wasserman Schultz issued a statement mourning his loss and praising Rich's work to support voter rights. Two days after the shooting, Hillary Clinton spoke of his death during a speech advocating limiting the availability of guns.

In September 2016, Rich's parents and girlfriend appeared on the syndicated television show Crime Watch Daily to speak about the murder case. In October 2016, a plaque and bike rack outside the DNC headquarters were dedicated to Rich's memory. In February 2017, the Beth El Synagogue in Omaha named after Rich an existing scholarship that helps Jewish children attend summer camps.
The Rich family accepted the pro bono public relations services of Republican lobbyist Jack Burkman in September 2016. The Rich family and Burkman held a joint press conference on the murder in November 2016. In January 2017, Burkman launched an advertising campaign in Northwest D.C. searching for information regarding Seth's death. This included billboard advertisements and canvassing with flyers. In late February, Burkman told media outlets he had a lead that the Russian government was involved in Rich's death, and the Rich family then distanced itself from Burkman. On March 19, 2017, Rich's brother, Aaron, started a GoFundMe campaign to try to raise $200,000 for private investigation, public outreach activities, and a reward fund.
The Rich family was approached by Ed Butowsky (a friend of Trump advisor Steve Bannon and a frequent Fox News contributor), who recommended having Fox News contributor and former homicide detective Rod Wheeler investigate Seth's murder. Butowsky said Wheeler had been recommended to him. The family gave Wheeler permission to investigate, though they did not hire him. When questioned by CNN, Butowsky denied involvement in the case, but later admitted he was involved and had offered to pay Wheeler's fees. After Wheeler asserted links between Rich and Wikileaks in a Fox affiliate interview on May 15, 2017—an assertion he later backpedaled from—the family spokesman said that the family regretted working with Wheeler. Wheeler then sued Fox News on August 1, 2017, for mental anguish and emotional distress, alleging that he had been misquoted in a story that was then published on the urging of Trump.

Rewards
The Metropolitan Police Department of the District of Columbia (MPDC) posted its customary reward of $25,000 for information about the death.
On August 9, 2016, WikiLeaks announced a $20,000 reward for information about Rich's murder leading to a conviction. Rich's family said they were unable to verify this reward offer. WikiLeaks stated that this offer should not be taken as implying Rich had been involved in leaking information to it.
In November 2016, Republican lobbyist Jack Burkman said he was personally offering a $100,000 reward in addition to those announced by the police department and WikiLeaks, and he added another $5,000 to his offer in December and another $25,000 in January. Burkman said he hoped the money would help "get to the truth of what happened here and will either debunk the conspiracy theories or validate them."

Conspiracy theories
Origins
Beginnings on social media
Political conspiracy theories and racially charged comments started to appear on social media the day after Rich's death. Within days, right-wing conspiracy theories began circulating, including false claims that his murder was connected to the DNC email leak of 2016 or the FBI's investigation of the Clinton Foundation.
A post on Twitter before Rich's memorial service spread the idea that his killing was a political assassination. Subsequently, the conspiracy theory was spread on the subreddit /r/The Donald, and on July 29, 2016, the website Heat Street reported on these Reddit posts. Reddit users attempted to tie the homicide to the Clinton body count conspiracy theory. The conspiracy theory was later popularized by Donald Trump political adviser Roger Stone via his Twitter account.
According to British journalist Duncan Campbell, the Russian intelligence agency, GRU, tried to implicate Rich as the source of the stolen DNC emails in order to draw attention away from themselves as the real perpetrators of the theft. Datestamps on the DNC files were altered to show the data had been obtained on July 5, 2016, five days before Rich's death, and the time zone was changed to Eastern Time, within which Washington, D.C., falls. Guccifer 2.0, the alleged GRU front that provided the emails to Wikileaks, then reported that Rich had been their source. Based partly on their acceptance of the false dates, some experts then concluded that the emails had been copied in the DNC offices, and had not been hacked from outside.

WikiLeaks statements
Julian Assange, the founder of WikiLeaks, fueled the speculation in an interview with Nieuwsuur published on August 9, 2016, which touched on the topic of risks faced by WikiLeaks' sources. Unbidden, Assange brought up the case of Seth Rich. When asked directly whether Rich was a source, Assange said "we don't comment on who our sources are". Subsequent statements by WikiLeaks emphasized that the organization was not naming Rich as a source.
According to the Mueller Report, WikiLeaks had received an email containing an encrypted file named "wk dnc link I .txt.gpg" from the Guccifer 2.0 GRU persona on July 14, which was four days after Seth Rich died. In April 2018, Twitter direct messages revealed that even as Assange was suggesting publicly that WikiLeaks had obtained emails from Seth Rich, Assange was trying to obtain more emails from Guccifer 2.0, who was at the time already suspected of being linked to Russian intelligence. BuzzFeed described the messages as "the starkest proof yet that Assange knew a likely Russian government hacker had the Democrat leaks he wanted. And they reveal the deliberate bad faith with which Assange fed the groundless claims that Rich was his source, even as he knew the documents' origin." Mike Gottlieb, a lawyer for Rich's brother, noted that WikiLeaks received the file of stolen documents from the Russian hackers on July 14, four days after Rich was shot. Gottlieb described the chronology as "damning".

Julian Assange not only knew that a murdered Democratic National Committee staffer wasn't his source for thousands of hacked party emails, he was in active contact with his real sources in Russia's GRU months after Seth Rich's death. At the same time he was publicly working to shift blame onto the slain staffer “to obscure the source of the materials he was releasing,”  Special Counsel Robert Mueller asserts in his final report on Russia's role in the 2016 presidential election.

Propagation by right-wing media and venues
The conspiracy theories were promoted by Mike Cernovich, Sean Hannity, Geraldo Rivera, Kim Dotcom, Paul Joseph Watson, Newt Gingrich, Jack Posobiec, Tim Pool, and others.
The same venues that fomented the false Pizzagate conspiracy theory helped to promulgate the Seth Rich murder conspiracy theories, and each shared similar features. Both were promoted by individuals subscribing to far-right politics, and by campaign officials and individuals appointed to senior-level national security roles by Donald Trump. After prior coordination on Facebook, each theory was spread on Twitter by automated bots using a branded hashtag, with the goal of becoming a trending topic. Both the Pizzagate conspiracy theory and the Seth Rich murder conspiracy theory were spread in the subreddit forum /r/The_Donald. In both conspiracy theories, the promoters attempted to shift the burden of proof — asking others to attempt to disprove their claims, without citing substantiated evidence. Slate's Elliot Hannon called the claims about Seth Rich a "PizzaGate-like conspiracy theory surrounding Rich's death", The Huffington Post described it as "the 'alt-right' idiocy of Pizzagate all over again", NPR's David Folkenflik said Fox News coverage of it "evokes the pizza-gate terrible allegations utterly unfounded", and Margaret Sullivan wrote for The Washington Post: "The Seth Rich lie has become the new Comet Ping Pong ... Crazy, baseless and dangerous."
On July 9, 2019, a Yahoo! News article stated that an alleged Russian Foreign Intelligence Service bulletin dated July 13, 2016 was the original source of the conspiracy theory. An analysis by the Washington Post disputed the conclusion while crediting the report for highlighting the roles played by InfoWars, Fox News, and Hannity in promoting the misinformation.

Debunking
Law enforcement stated that the conspiracy theories were unfounded, while fact-checking websites like PolitiFact.com, Snopes.com, and FactCheck.org came to the conclusion that the theories were false and baseless.
The Metropolitan Police Department of the District of Columbia described the murder as related to a bungled attempted robbery, and said "the assertions put forward by Mr. Wheeler are unfounded."  Assistant Police Chief Peter Newsham said the police had no information suggesting a connection between Rich's death and data obtained by WikiLeaks, and MPD officers considered the murder to be the result of a botched robbery attempt. A spokesman for the Washington, D.C., mayor's office, who said, "All claims made by Mr. Wheeler are false and take fake news to a whole new level. The family deserves better and everyday MPD continues to work diligently to solve this case."
People who worked with Rich said he didn't have access to the emails on the DNC server and was not an expert computer hacker helping to leak information to foreigners. Andrew Therriault, a data scientist who had mentored Rich, said although he had recently been working as a programmer, he did not have a programming background; another co-worker said Rich was very upset when he heard hackers associated with Russian intelligence services had broken into the DNC computers and could be interfering with the election. 
Conspiracy theories falsely claimed that the FBI was investigating the case; in fact, the DC's MPD investigated the murder, and the FBI was not involved.
Newt Gingrich promoted the conspiracy theory of a tie between Rich and Wikileaks, claiming that Rich "apparently was assassinated" subsequent to "having given WikiLeaks something like ... 53,000 [DNC] emails and 17,000 attachments". No evidence supported Gingrich's false and baseless claim, and there was no evidence of any link between Rich and Wikileaks.
The fabrications were described as fake news and falsehoods by The New York Times. The New York Times cited the conspiracy theories as an example of the persistence of false claims, concluding: "fake news dies hard". The Los Angeles Times called the conspiracy theories "unsubstantiated rumors".
The Washington Post cited the conspiracy theories as an example of the power of fake news to spread virally online. The paper used the example as a case study of the persistence of fake news, and found that television news media can be a soft target for such false stories. The Washington Post further found that the proliferation of fake news via Facebook had decreased, but remained powerful on Twitter due to spread via online bots. They found that the conspiracy theories with the largest potential to spread on the Internet were those that held attraction for both the alt-right movements and the political left wing. The Washington Post concluded that even if a particular false story had been sufficiently debunked, such fact-checking was unable to stop the spread of the falsehoods online.

Retracted reporting
Uncorroborated Fox News story
On May 15, 2017, Fox 5 DC (WTTG) reported the uncorroborated and later largely retracted claims by Rod Wheeler, a Fox News contributor and former homicide detective, that there was evidence Seth Rich had contacted WikiLeaks and that law enforcement were covering this up, claims that were never independently verified by Fox. The next day, Fox News published a lead story on its website and provided extensive coverage on its cable news channel about what it later said were Wheeler's uncorroborated claims about the murder of Seth Rich; in the lead story Fox News removed from their website a few days later, they stated that Wheeler's claims had been "corroborated by a federal investigator who spoke to Fox News." In reporting these claims, the Fox News report re-ignited conspiracy theories about the killing. According to NPR, within a day of the original Fox report, "Google searches for Rich had overtaken searches for James Comey, even amid continuous news about the former FBI director's conversations with Trump." The Washington Post's Callum Borchers noted Fox News chose to lead with this story at a time when most other media outlets were covering Donald Trump's disclosure of classified information to Russia.
Other news organizations revealed Wheeler was a Donald Trump supporter, a paid Fox News contributor, and according to NBC News had "developed a reputation for making outlandish claims, such as one appearance on Fox News in 2007 in which he warned that underground networks of pink pistol-toting lesbian gangs were raping young women". The Washington Post noted it is "rare for a news organization to have such a close relationship with the people it is covering", as Wheeler was "playing three roles at once: as a Fox source, as a paid contributor to the network and as a supposedly independent investigator of the murder". When Wheeler appeared on Sean Hannity's Fox News shows, these multiple roles were not disclosed to viewers. After Wheeler's Fox News interview on May 15, 2017, Brad Bauman, a communications professional and spokesman for the Rich family, said the family was asking Fox News and the Fox affiliate to retract their reports and apologize for damaging their son's legacy.
The family spokesperson, the Washington, D.C., police department, the Washington, D.C., mayor's office, the FBI, and law enforcement sources familiar with the case all disputed Wheeler's claims. The family said, "We are a family who is committed to facts, not fake evidence that surfaces every few months to fill the void and distract law enforcement and the general public from finding Seth's murderers." Bauman criticized Fox News for its reporting, saying he believed that the outlet was motivated by a desire to deflect attention from the Trump-Russia story: "I think there's a very special place in hell for people that would use the memory of a murder victim in order to pursue a political agenda."
Later that day, Wheeler told CNN he had no evidence that Rich had contacted Wikileaks. Wheeler claimed that Fox had presented his quotes misleadingly and that he only learned about the possible existence of the evidence from a Fox News reporter. Despite this, Sean Hannity's show and Fox & Friends continued to promote the conspiracy theory for the remainder of the week. On May 18, 2016, Hannity's guest on the show was Jay Sekulow who said that Rich's killing "... undercuts this whole Russia argument;" neither one mentioned that Sekulow had just been hired as one of Trump's lead lawyers in the Mueller investigation into Russian interference in the 2016 United States elections. Former House Speaker Newt Gingrich and Geraldo Rivera took part in spreading the conspiracy. Hannity had on his program Tom Fitton of Judicial Watch, who said the organization filed Freedom of Information Act requests for documents from Washington, D.C., mayor Muriel E. Bowser, and from the Metropolitan Police. Sean Hannity furthermore promoted the uncorroborated claims of Kim Dotcom, a New Zealand resident sought by the United States on fraud charges who claimed without evidence that Rich had been in contact with him before his death. Fox News host Julie Roginsky was critical of the conspiracy theory peddlers, stating on Twitter and on her television show: "The exploitation of a dead man whose family has begged conspiracy theorists to stop is really egregious. Please stop." Fox News was also criticized by conservative outlets, such as the Weekly Standard, National Review, and conservative columnists, such as Jennifer Rubin, Michael Gerson, and John Podhoretz. In September 2017, NPR noted that Fox News had yet to apologize for its false story or explain what went wrong; "When a story of this scale crumbles, most news organizations feel obligated to explain what happened and why. Not so far at Fox."
By November 2020, Malia Zimmerman, the reporter behind the retracted Fox News story, was no longer working at Fox.

Cease and desist letter and Fox News retraction
On May 19, 2017, an attorney for the Rich family sent a cease and desist letter to Wheeler.
Fox News issued a retraction of the story on May 23, 2017, and removed the original article and did not apologize or specify what went wrong or how it did so. Despite this, Hannity, who pushed the theory, remained unapologetic, saying "I retracted nothing" and "I am not going to stop trying to find the truth." In their May 23 statement, Fox News said, "The article was not initially subjected to the high degree of editorial scrutiny we require for all our reporting. Upon appropriate review, the article was found not to meet those standards and has since been removed." Media ethics writer Kelly McBride criticized the retraction as "woefully inadequate", writing that it did not specify exactly what was inaccurate or provide correct information in place of the retracted story.
The same day, Hannity stated on his show that he would cease discussing the issue. Hannity said his decision to cease commenting on the matter was related to the family of the murder victim: "Out of respect for the family's wishes, for now, I am not discussing the matter at this time." In the same statement wherein he promised to cease discussion of the topic, he vowed to pursue facts in the future: "I promise you I am not going to stop trying to find the truth." Several advertisers including Crowne Plaza Hotels, Cars.com, Leesa Mattress, USAA, Peloton and Casper Sleep pulled their marketing from his program on Fox News. Crowne Plaza Hotels later said that it was not their policy to advertise on political commentary shows and that they had not been aware of their sponsorship of the show. USAA soon returned to advertising on Fox News after receiving customer input.

InfoWars retraction
In 2019, Jerome Corsi and InfoWars apologized and retracted a story promoting conspiracy theories about the murder of Seth Rich. The retraction was published on the front page of InfoWars, where Corsi said that "his allegations were not based upon any independent factual knowledge." Corsi said that he retracted the story because it relied on information that the Washington Times had retracted, but still thought that investigators should look into whether Seth Rich played a role.

Wheeler lawsuit
On August 1, 2017, Rod Wheeler, the private investigator hired by Butowsky who was the first to claim links between Seth Rich's murder and the DNC hack on Fox, but who later appeared to retract his claims, filed a lawsuit (Case 1:17-cv-05807 Southern District of New York) in which 21st Century Fox, the Fox News Channel, Fox News reporter Malia Zimmerman and Ed Butowsky were named as defendants, stating that quotes attributed to him in the original Fox News piece were fabricated. The lawsuit also alleged that the fabricated quotes were included in the Fox News story at the urging of the Trump White House.
Text messages and audio apparently supporting this assertion were included in the filing of the lawsuit. About a month before the story was aired on Fox News, Wheeler and Butowsky met at the White House with the White House press secretary, Sean Spicer, to review the planned story on Seth Rich's murder. After talking to Wheeler and Butowsky, Zimmerman sent Wheeler a draft of a story without any quotes from Wheeler on May 11. On May 14, Butowsky texted Wheeler saying "Not to add any more pressure but the president just read the article. He wants the article out immediately. It's now all up to you. But don't feel the pressure." Butowsky also left a voicemail for Wheeler which said "We have the full, uh, attention of the White House on this. And tomorrow, let's close this deal, whatever we've got to do."
Butowsky said Seymour Hersh confirmed a link between Rich and the FBI. Hersh confirmed the conversation with Butowsky but told NPR the link was "gossip" and that Butowsky exaggerated its significance.
In an email to Fox News, Butowsky also wrote about the purpose behind the Seth Rich story: "One of the big conclusions we need to draw from this is that the Russians did not hack our computer systems and ste[a]l emails and there was no collusion (between) Trump and the Russians." He also instructed Wheeler that "[T]he narrative in the interviews you might use is that you and [Zimmerman's] work prove that the Russians didn't hack into the DNC and steal the emails and impact our elections ... If you can, try to highlight this puts the Russian hacking story to rest."
When the story aired on Fox News, it included supposed quotes from Wheeler and was written as if the accusations against the DNC came from him. Wheeler alleges that the quotes were fabricated and should not have been attributed to him.
In later recordings Butowsky told Wheeler that the claims being attributed to him were false but says that "One day you're going to win an award for having said those things you didn't say." He also says "I know that's not true, if I'm under oath, I would say I never heard him say that."
The lawsuit was dismissed in August 2018, alongside Rich's family lawsuit against Fox, with the presiding judge ruling that there was no evidence that Fox has manipulated claims he had made on recordings, while other statements that Wheeler claimed were defamatory were considered opinion.  However, information learned from the discovery phase of the lawsuit on how Fox News was handling the story were subsequently used by Rich's family in its lawsuit against the network after they successfully appealed the dismissal.

Family's reactions
In May 2017, Seth Rich's brother Aaron Rich issued a statement saying, "We simply want to find his killers and grieve. Instead, we are stuck having to constantly fight against non-facts, baseless allegations, and general stupidity to defend my brother's name and legacy." The family spokesperson said "At this point, only people with transparent political agendas or sociopaths are still perpetuating Seth Rich conspiracies."
His parents authored a piece in The Washington Post on May 23, 2017, titled: "We're Seth Rich's parents. Stop politicizing our son's murder," in which they wrote:

We are asking you to please consider our feelings and words. There are people who are using our beloved Seth's memory and legacy for their own political goals, and they are using your outrage to perpetuate our nightmare. We ask those purveying falsehoods to give us peace, and to give law enforcement the time and space to do the investigation they need to solve our son's murder.
In March 2018, Aaron Rich sued Butowsky, Couch, America First Media, and The Washington Times for suggesting he had played a role in the purported theft of emails from the DNC. On October 1, 2018, as part of a settlement they had reached with Aaron Rich, The Washington Times retracted the relevant articles and apologized to Rich and his family.

Lawsuit against Fox News
In March 2018, Rich's family filed a lawsuit against Fox News, Fox reporter Malia Zimmerman, and Fox contributor Ed Butowsky, for publishing a news report about Seth Rich. The suit alleges that the report fueled conspiracy theories about Rich's death and caused the family emotional distress. Judge George B. Daniels dismissed the lawsuit in August 2018 alongside the case against Fox News from Wheeler. Judge Daniels ruled that, although it was reasonable for plaintiffs to believe their son's death was being used for political purposes, the plaintiffs failed to allege "intentional infliction" of emotional distress on the part of defendants, as that standard is determined under New York state law: "defamatory statements to news outlets 'fall well short of meeting the high standards for extreme and outrageous conduct.' They would have had to prove more extreme and persistent distress under the law."
The United States Court of Appeals for the Second Circuit overturned the dismissal in September 2019 saying, "the Riches plausibly alleged what amounted to a campaign of emotional torture." In subsequent proceedings, the Rich family used information from the failed Wheeler lawsuit to support further allegations towards Fox News. Among that information included the level of involvement that Butowsky had in preparing and coaching the Fox News hosts in the days before they broke their version of the Seth Rich story, his hiring of Wheeler, and his meeting with Spicer. On October 12, 2020, Fox News reached a settlement with the Rich family. The terms of the settlement were not disclosed, but were reported to be in the seven figures. The settlement also dismissed the actions against Zimmerman and Butowsky, which eliminated the need for Fox News hosts like Hannity and Dobbs to give testimony. The settlement contained the provision that it had to be kept secret for a month.

Documentary
In April 2018, the BBC broadcast the documentary Conspiracy Files: Murder in Washington examining the death of Rich and subsequent theories about the death.
In May 2022, an episode of the Netflix documentary series Web of Make Believe: Death, Lies and the Internet examined the murder of Seth Rich, the unfounded conspiracy theories and his family.

See also
Crime in the District of Columbia
Fake news in the United States
Fringe theory
Furtive fallacy
List of conspiracy theories
List of unsolved deaths

References
Further reading
Abraham, Rabbi Steven (July 22, 2016). "In Memoriam: Seth Conrad Rich". The Jewish Press. Omaha. Archived from the original on August 26, 2016. Retrieved August 12, 2016. Eulogy given by Rabbi Steven Abraham of Beth El Synagogue at the service on July 13, 2016.
Harris, Chris (July 13, 2016). "Friends of Slain DNC Staffer Seth Rich Remember Him as 'Honest' and 'Self-Sacrificing'". People.

External links
Seth Conrad Rich at Find a Grave—Beth El Cemetery, Ralston, Nebraska
"The Seth Rich Conspiracy Theory", Snopes.com
Wheeler v. Twenty-First Century Fox, Inc. et al.

Title: Hannah Ritchie
Hannah Ritchie is a Scottish data scientist, senior researcher at the University of Oxford in the Oxford Martin School, and deputy editor at Our World in Data. Her work focuses on sustainability, in relation to climate change, energy, food and agriculture, biodiversity, air pollution, deforestation, and public health. 
Her first book, Not the End of the World, was published in 2024 by Chatto & Windus.

Early life and education
Hannah Ritchie trained in environmental science at the University of Edinburgh. She earned her undergraduate degree in environmental geoscience and a master's degree in carbon management. 
She remained in Scotland for her Ph.D., researching malnutrition and global food systems. She created a scalable framework to understand food system pathways and identify losses, allocations and conversions. In particular, she looked to understand whether it was possible to feed a growing population without damaging the environment. Ritchie is vegan.

Research
Ritchie started her career as a lecturer in sustainability at the University of Edinburgh. She developed teaching programs focused on sustainability. She left Edinburgh to start a research position at the University of Oxford, where she developed data visualizations to communicate information.
Ritchie's early work considered food systems and how it was essential to adapt to meet the Sustainable Development Goals. For example, she has argued that for most foods, the carbon footprint is barely impacted by transport.
In 2017 Ritchie joined Our World in Data as Head of Research. Her work focuses on environmental sustainability, including topics such as climate change, energy, food and agriculture, biodiversity, air pollution, and deforestation. During the COVID-19 pandemic, she built the Our World in Data COVID-19 information dashboard. In 2023 she became Deputy Editor and Lead Researcher.
In 2024 Chatto & Windus published Ritchie's first book, Not the End of the World, which explores her optimism for large-scale problem-solving and ending climate change.

Recognition
In 2022 Ritchie was named Scotland's Youth Climate Champion at the Holyrood Green Giant Awards in recognition of her contributions to the climate-change movement. In 2024, Ritchie was recognised for lifetime achievement with honorary fellowship of the Royal Statistical Society.

Bibliography
Book
Not the End of the World, Chatto & Windus, 2024, ISBN 9781784745004.

Select papers
"Coronavirus Pandemic (COVID-19)" 
"A global database of COVID-19 vaccination"
"CO₂ and Greenhouse Gas Emissions"
"World Population Growth"
Hannah Ritchie, "What We Learned from Acid Rain: By working together, the nations of the world can solve climate change", Scientific American, vol. 330, no. 1 (January 2024), pp. 75–76. "[C]ountries will act only if they know others are willing to do the same. With acid rain, they did act collectively.... We did something similar to restore Earth's protective ozone layer.... [T]he cost of technology really matters.... In the past decade the price of solar energy has fallen by more than 90 percent and that of wind energy by more than 70 percent. Battery costs have tumbled by 98 percent since 1990, bringing the price of electric cars down with them....[T]he stance of elected officials matters more than their party affiliation.... Change can happen – but not on its own. We need to drive it." (p. 76.)


== References ==

Title: David G. Robinson (data scientist)
David G. Robinson is a data scientist at the Heap analytics company. He is a co-author of the tidytext R (programming language) package and the O’Reilly book, Text Mining with R. Robinson has previously worked as a chief data scientist at DataCamp and as a data scientist at Stack Overflow. He was also a data engineer at Flatiron Health in 2019.

Education
Robinson graduated from Harvard University with a Bachelor of Arts degree in Statistics in 2010. He received a PhD in Quantitative and Computational Biology from Princeton University.

Career
Robinson previously worked at Flatiron Health, where he used data science in the fight against cancer on the Data Insights Engineering team. He has three courses on DataCamp published, which assist people with learning R and data science. He also co-authored Text Mining with R: A Tidy Approach with Julia Silge. The book was published by O'Reilly in July 2017 and is a guide to drawing insights from text using the tidytext package in R. Another book authored by Robinson is Introduction to Empirical Bayes: Examples from Baseball Statistics, an e-book demonstrating the statistical method of empirical Bayes, based on the example of estimating baseball batting averages.
Robinson is known for his author profiling and sentiment analysis of Donald Trump's tweets in 2016, when he found that posts from Trump's official account came from multiple sources.

Publications
Robinson has numerous publications including, "Widespread changes in mRNA stability contribute to quiescence-specific gene expression patterns in a fibroblast model of quiescence", "broom: An R package for converting statistical analysis objects into tidy data frames", "A nested parallel experiment demonstrates differences in intensity-dependence between RNA-seq and microarrays", "subSeq: Determining appropriate sequencing depth through efficient read subsampling", "Design and Analysis of Bar-seq Experiments", and "OASIS: an automated program for global investigation of bacterial and archaeal insertion sequences".
As mentioned, his book Introduction to Empirical Bayes helps readers understand Bayesian methods for estimating binomial proportions, through a series of examples drawn from baseball statistics.


== References ==

Title: Monica Rogati
Monica Rogati is a data scientist, computer scientist and the former Vice President of Data of Jawbone. Before that, she was a senior data scientist at LinkedIn.

Early life and education
Rogati was born in Romania, where she attended the Tudor Vianu National College of Computer Science. Rogati has a B.S. in Computer Science from the University of New Mexico, and an MS and PhD in Computer Science from Carnegie Mellon University.

Career
Rogati was Vice President of Data of Jawbone from 2013 to 2015. Previously, she was a senior data scientist at LinkedIn for five years, where she built the initial version of LinkedIn's job matching system and the first machine learning model for LinkedIn’s "People You May Know" feature. She is currently an equity partner at Data Collective (DCVC) and a scientific advisor to CrowdFlower.

Media
Rogati has been interviewed and featured by the New York Times, Recode, and others. In 2014, she was named a "Big Data All-Star" by Fortune and was named one of the "100 Most Creative People in Business" by Fast Company. In 2013, she was named an "Enterprise Superstar" by VentureBeat.

References
External links
Monica Rogati on LinkedIn

Title: Susanna-Assunta Sansone
Susanna-Assunta Sansone is a British-Italian data scientist who is professor of data readiness at the University of Oxford where she leads the data readiness group and serves as associate director of the Oxford e-Research Centre. Her research investigates techniques for improving the interoperability, reproducibility and integrity of data.

Early life and education
Sansone is from Italy. She was an undergraduate student at the University of Naples Federico II. She earned her bachelor's degree in molecular biology and a PhD in microbiology at Imperial College London, where she worked in St Mary's Hospital, London. Her thesis investigated the role of the cofactored enzyme superoxide dismutase in the virulence of Salmonella.

Research and career
After earning her doctorate, she moved to Microscience Ltd, where she characterised vaccine strains. In 2001, Sansone joined the European Bioinformatics Institute (EBI), part of the European Molecular Biology Laboratory (EMBL) where she worked in research data management. Sansone joined the University of Oxford in 2010. She became concerned that whilst there were vast amounts of data in the public domain, the majority of it was not reusable. To make data reusable, Sansone encourages researchers to combine their data with metadata: a description of what the data means. Sansone has described data reproducibility as “the foundation of every scientific field,”.
Sansone's research investigates strategies to enable the creation of research objects that are Findable, Accessible, Interoperable and Reusable (FAIR). She co-founded the peer-reviewed journal Scientific Data in 2013, and serves as chair of the Research Data Alliance. She co-authored the FAIR data principles in 2016, a set of guidelines for the scientific ecosystem. FAIR principles have since been adopted by funding bodies, scientific publishers and the private sector. Sansone works with partners to deliver data stewardship and data governance training and to develop guidelines to make data more accessible. She is one of the co-creators the FAIR Cookbook, an online resource for life scientists to enable them to keep FAIR data. Her research has been funded by the Biotechnology and Biological Sciences Research Council (BBSRC) and the European Union.

Selected publications
Her publications include

The FAIR Guiding Principles for scientific data management and stewardship
ArrayExpress--a public database of microarray experiments and gene expression profiles
The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration
The minimum information about a genome sequence (MIGS) specification
MetaboLights—an open-access general-purpose repository for metabolomics studies and associated meta-data
COVID-19 pandemic reveals the peril of ignoring metadata standards
ISA software suite: supporting standards-compliant experimental annotation and enabling curation at the community level
Toward interoperable bioscience data
Modeling biomedical experimental processes with OBI


== References ==

Title: Suchi Saria
Suchi Saria is an Associate Professor of Machine Learning and Healthcare at Johns Hopkins University, where she uses big data to improve patient outcomes. She is a World Economic Forum Young Global Leader. From 2022 to 2023, she was an investment partner at AIX Ventures. AIX Ventures is a venture capital fund that invests in artificial intelligence startups.

Early life and education
Saria is from Darjeeling. She earned her bachelor's degree at Mount Holyoke College. She was awarded a full scholarship from Microsoft. In 2004 she joined Stanford University as a Rambus Corporation Fellow. She earned her Master of Science and Doctor of Philosophy degrees at Stanford University, supervised by Daphne Koller and advised by Anna Asher Penn and Sebastian Thrun. At Stanford University, Saria developed a statistical model that could predict premature baby outcomes with a 90% accuracy. The model used data from monitors, birth weight and length of time spent in the womb to predict whether a preemie would develop an illness.  She worked in the startup Aster Data Systems.

Career and research
Saria believes that big data can be used to personalise healthcare. She is considered an expert in computational statistics and their applications to the real world. She uses Bayesian and probabilistic modelling. In 2014 Saria was funded by a $1.5 million Gordon and Betty Moore Foundation project that looked to make intensive care units safer. The project used data collected at patients' bedsides along with noninvasive 3D sensors that monitor care in patient's hospital rooms. The sensors collect information on steps that might have been missed by doctors; like washing hands.
Saria uses big data to manage chronic diseases. She is part of a National Science Foundation (NSF) award that looks at scleroderma. She uses machine learning to analyse medical records and identify similar patterns of disease progression. The system works out which treatments have been effectively used for various symptoms to aid doctors in choosing treatment plans for specific patients.  She has developed another algorithm that can be used to predict and treat Septic shock. The algorithm used 16,000 items of patient health records and generates a targeted real-time warning (TREWS) score. She collaborated with David N. Hager to use the algorithm in clinics, and it was correct 86% of the time. Saria modified the algorithm to avoid missing high risk patients- for example, those who have suffered from septic shock previously and who have sought successful treatment. She was described by XRDS magazine as being a Pioneer in transforming healthcare. In 2016 Saria spoke at about using machine learning for medicine at TEDxBoston. The talk has been viewed over 100,170 times.

Awards and honours
Her awards and honors include:

2018 Sloan Research Fellowship
2018 World Economic Forum Young Global Leader
2017 MIT Technology Review 35 Innovators Under 35
2017 Defense Advanced Projects Research Agency (DARPA) Young Faculty Fellowship
2016 Brilliant 10 award by Popular Science
2015 IEEE Intelligent Systems Young Star in Artificial Intelligence
2015 Johns Hopkins Discovery Award
2014 National Science Foundation (NSF) Smart and Connected Health Research Grant
2014 Google Research Award
2014 Society of Critical Care Medicine Annual Scientific Award
2013 Gordon and Betty Moore Foundation Research Award


== References ==

Title: Ervin Sejdic
Ervin Sejdic is North York General Hospital's Research Chair in Artificial Intelligence for Health Outcomes.  He focuses on biomedical signal processing, gait analysis, swallowing difficulties, advanced information systems in medicine, rehabilitation engineering, assistive technologies and anticipatory medical devices. He was previously a researcher at the Swanson School of Engineering, University of Pittsburgh, where he directs a research laboratory focused on engineering developments in medicine. His research has focused on creating computational biomarkers indicative of age- and disease-related changes in functional outcomes such as swallowing, gait and handwriting. In particular, he aims to develop clinically relevant solutions by fostering innovation in mechatronic systems (computational data-centric approaches and instrumentation) that can be translated to bedside care. Due to his contributions in signal processing and biomedical engineering, Sejdic has been named to editorial positions of IEEE Signal Processing Magazine, BioMedical Engineering Online and IEEE Transactions on Biomedical Engineering.

Education
Sejdic has received his Bachelor of Engineering Science in electrical and computer engineering from the University of Western Ontario in 2002. He continued to a graduate program in electrical and computer engineering at the same university while being advised by Professor Jin Jiang, where Sejdic obtained his PhD in electrical and computer engineering in January 2008. Next, Sejdic joined Dr. Tom Chau's group at the University of Toronto and Holland Bloorview Kids Rehabilitation Hospital, where he was a postdoctoral fellow specializing in pediatric rehabilitation engineering and biomedical instrumentation. In 2011, Sejdic joined Harvard Medical School and Beth Israel Deaconess Medical Center as a research fellow in medicine, where he specialized in geriatrics (cardiovascular and cerebrovascular monitoring of older diabetic adults)

Research
Sejdic's early research revolved around signal processing, specifically the area of time–frequency analysis. In more recent years, he has focused on modeling of human functions such as swallowing and gait. Sejdic has also made contributions in computational medicine, implantable medical devices, and biomedical engineering, including a novel brain-machine interface modality based on transcranial Doppler sonography

Awards
2018 University of Pittsburgh, Chancellor's Distinguished Research Award
2017 National Science Foundation CAREER Award
2013 Presidential Early Career Award for Scientists and Engineers
2010 Institute for Aging Research, Hebrew Senior Life, Melvin First Young Investigator's Award
2005 Natural Sciences and Engineering Research Council of Canada, Postgraduate Scholarship
2003 Natural Sciences and Engineering Research Council of Canada, Postgraduate Scholarship

References
External links

"President Obama Honors Extraordinary Early-Career Scientists". whitehouse.gov. 2016-02-18. Retrieved April 9, 2017.
"Obama awards four Pitt professors - The Pitt News". The Pitt News. 2016-02-21. Retrieved April 9, 2017.
"Honoring Federally-Funded Scientists and Engineers at the Forefront of Research and Discovery". whitehouse.gov. 2016-05-05. Retrieved April 9, 2017.
"NSF Award Search: Award#1652203 - CAREER: Advanced data analytics and high-resolution cervical auscultation can accurately predict dysphagia". NSF.gov. Retrieved April 9, 2017.
"Five Pitt engineering faculty set university and school record by receiving competitive NSF CAREER awards in first months of 2017". engineering.pitt.edu. Retrieved May 7, 2017.
"A Better Way to Swallow". engineering.pitt.edu. Retrieved May 7, 2017.
"Five things to know today, and more donations for Point Park's Pittsburgh Playhouse". bizjournals.com. Retrieved May 7, 2017.
"Pitt researcher receives NSF CAREER Award to develop improved screening method for dysphagia". News-Medical.net. 2017-02-14. Retrieved May 7, 2017.
"A better way to swallow". EurekAlert!. Retrieved May 7, 2017.

Title: Roberta Sinatra
Roberta Sinatra is an Italian scientist and associate professor at the IT University of Copenhagen. She is known for her work in network science and conducts research on quantifying success in science.

Early life and education
Sinatra received her undergraduate degree from the University of Catania in 2005. She received her M.Sc. in theoretical physics from the same university in 2007 and, in 2008, an M.Sc. (Eccellenza Specialistica) from the Scuola superiore di Catania. She went on to earn a Ph.D in physics from the University of Catania in 2012, working with Vito Latora and Jesús Gómez-Gardeñes. She then held postdoctoral and associate research scientist positions  at Northeastern University and Dana Farber Cancer Institute. She joined  IT University of Copenhagen in 2019.

Research and career
Sinatra is known for her research on the social determinants of success, using large-scale data sets and methods from statistical physics, machine learning and network science. Her early work was on cooperation in games. She has subsequently quantified performance and success in scientific and creative careers, as well as in art and culture. Through her research, she addressed gender inequality in academic publishing and the importance of luck in success.

Selected publications
Fortunato, Santo; Bergstrom, Carl T.; Börner, Katy; Evans, James A.; Helbing, Dirk; Milojević, Staša; Petersen, Alexander M.; Radicchi, Filippo; Sinatra, Roberta; Uzzi, Brian; Vespignani, Alessandro (March 2, 2018). "Science of science". Science. 359 (6379): eaao0185. doi:10.1126/science.aao0185. PMC 5949209. PMID 29496846.
Huang, Junming; Gates, Alexander J.; Sinatra, Roberta; Barabási, Albert-László (March 3, 2020). "Historical comparison of gender inequality in scientific careers across countries and disciplines". Proceedings of the National Academy of Sciences. 117 (9): 4609–4616. Bibcode:2020PNAS..117.4609H. doi:10.1073/pnas.1914221117. ISSN 0027-8424. PMC 7060730. PMID 32071248.
Sinatra, Roberta; Wang, Dashun; Deville, Pierre; Song, Chaoming; Barabási, Albert-László (November 4, 2016). "Quantifying the evolution of individual scientific impact". Science. 354 (6312): aaf5239. doi:10.1126/science.aaf5239. PMID 27811240. S2CID 24015860.
Sinatra, R; Iranzo, J; Gómez-Gardeñes, J; Floría, L M; Latora, V; Moreno, Y (September 21, 2009). "The Ultimatum Game in complex networks". Journal of Statistical Mechanics: Theory and Experiment. 2009 (9): P09012. arXiv:0807.0750. Bibcode:2009JSMTE..09..012S. doi:10.1088/1742-5468/2009/09/p09012. ISSN 1742-5468. S2CID 15069048.
Sinatra, Roberta; Deville, Pierre; Szell, Michael; Wang, Dashun; Barabási, Albert-László (2015). "A century of physics". Nature Physics. 11 (10): 791–796. Bibcode:2015NatPh..11..791S. doi:10.1038/nphys3494. ISSN 1745-2473. S2CID 10078155.

Awards and honors
In 2017 she was named a fellow of the Institute for Scientific Interchange. In 2020 Sinatra received a Junior Scientific Award from the Complex System Society, for "pioneer contributions to the science of science and success, having had an impact in multiple fields, from network science to computational social science and scientometrics".

References
External links
Roberta Sinatra publications indexed by Google Scholar
Roberta Sinatra - Quantifying the biases of scientific success on YouTube, December 13, 2021

Title: Samuel Sinyangwe
Samuel Sinyangwe (born May 12, 1990) is an American policy analyst and racial justice activist. Sinyangwe is a member of the Movement for Black Lives, the founder of Mapping Police Violence, a database of police killings in the United States and the Police Scorecard, a website with data on police use of force and accountability metrics on US police and sheriff's departments. Sinyangwe is also a co-founder of We the Protestors, a group of digital tools that include Campaign Zero, a policy platform to end police violence and a co-host of the Pod Save the People podcast, where he discusses the week's news with a panel of other activists.

Early life
Sinyangwe was born May 12, 1990, to a Tanzanian father and a European Jewish mother who met while studying at Cornell University. He grew up in the College Park neighborhood of Orlando, Florida and attended Winter Park High School in the International Baccalaureate program. He has discussed the influence of his upbringing in Florida, where he was a black child often surrounded by white peers, on his eventual career trajectory; he was shaken and moved to action after the 2013 acquittal of George Zimmerman in the shooting death of Trayvon Martin in Sanford, Florida, where Sinyangwe had regularly attended soccer practice: "I was that kid. I could have been Trayvon. That’s why it hit me so personally and that’s why I realized that needed to be something that took the priority in terms of my focus."
Sinyangwe graduated from Stanford University, where he studied how race intersects with American politics, economics, and class.

Career
Sinyangwe started his career at PolicyLink with the Promise Neighborhoods Institute. As protests emerged in the wake of the 2014 shooting of Michael Brown in Ferguson, Missouri, he connected with Ferguson activists online. With DeRay Mckesson, Brittney Packnett and Johnetta Elzie, he began working to develop policy solutions to address police violence in America. Sinyangwe particularly noticed the absence of official government statistics on police violence and began compiling them from other sources like Fatal Encounters and KilledbythePolice.net, in order to challenge claims about police shootings being rare events or only resulting from resisting arrest.
With other activists, Sinyangwe founded We the Protestors, an organization aimed at developing a set of digital tools to support Black Lives Matter activism. Sinyangwe built projects including a database of police killings, Mapping Police Violence, and a platform of policy solutions to end police violence called Campaign Zero. Sinyangwe also serves as a data scientist for OurStates.org, a project focused on state legislatures and with Mckesson and Brittney Packnett founded the Resistance Manual, an open-source project aimed at connecting anti-racist activists with activists focused on intersecting issues. He has also been responsible for a number of CPRA requests for RIPA-formatted police stops data through the non-profit organization MuckRock.
During the 2016 U.S. Presidential campaign, Sinyangwe and colleagues met with Democratic candidates Bernie Sanders and Hillary Clinton on these policy issues. He has been a vocal critic of the "Ferguson Effect", using data to refute the theory that policing had diminished and crime increased in face of activist scrutiny of police use of force. Melissa Harris-Perry has compared Sinyangwe to journalist and anti-lynching activist Ida B. Wells, noting that Wells began her work by "compil[ing] the data, the social science and research about how, when and where lynchings were happening to begin to make it stop."
Sinyangwe is a co-host of Mckesson's podcast Pod Save the People, which discusses the week's news with a panel of other activists including Mckesson, Packnett and Clint Smith. The podcast particularly focuses on race, grassroots activism, discrimination and other forms of inequality; recommending Pod Save The People in GQ, June Diane Raphael of How Did This Get Made? wrote, "The stories they uplift and think critically about are the ones I'm now wondering why I've never been exposed to/exposed myself to." Sinyangwe has also been featured on CNN, MSNBC, BBC News, FiveThirtyEight, The Los Angeles Times, and other publications. He has written for the Huffington Post and The Guardian.

Awards and fellowships
In 2017, Sinyangwe was named to the Forbes 30 Under 30 list for law and policy. He was also a 2017 Echoing Green Black Male Achievement Fellow.

Personal life
Sinyangwe lives in New York City.

Selected writings
"Cities That Reduced Arrests For Minor Offenses Also Saw Fewer Police Shootings". FiveThirtyEight. July 26, 2021. Retrieved 1 April 2022.
"Stop Pretending the 'Ferguson Effect' is Real". Huffington Post. May 13, 2016. Retrieved 2 December 2016.
"Giving the 'Ferguson effect' a new name won't make it truer". The Guardian. October 28, 2015. Retrieved 2 December 2016.
"Examining the Role of Use of Force Policies in Ending Police Violence", Police Use of Force Project, September 20, 2016.


== References ==

Title: Victoria Stodden
Victoria Stodden is a statistician, associate professor of information sciences, and affiliate professor of statistics at the University of Illinois at Urbana–Champaign.  She earned a B.A. in economics from the University of Ottawa, an MS in economics from the University of British Columbia, and both her law degree and a Ph.D. in statistics from Stanford University.

Work
Stodden's work focuses on facilitating the reproducibility of research, specifically in computational sciences. She is the founder of "Reproducible Research Standard" and the website ResearchCompendia.org, which was announced in 2015 to enable public verification of research and methods but went defunct in 2016. In 2020, Stodden proposed a set of guidelines for researchers working in data science, including the role of reproducible research.
Her current focus lies in scientific research incentives, having said: "A big part of the work I am doing concerns the scientific reward structure. For example, my work on the Reproducible Research Standard is an effort to realign the intellectual property rules scientists are subject to, to be closer to our scientific norms."

Board membership
Stodden is a co-chair for the National Science Foundation's Advisory Committee for CyberInfrastructure. She also serves on the Committee on Electronic Information and Communication of the International Mathematical Union and the Advisory Board for Project TIER (Teaching Integrity in Empirical Research).

Publications
"Four Simple Recommendations to Encourage Best Practices in Research Software," Jiminez et al., F1000Research 2017, 6:876 (doi: 10.12688/f1000research.11407.1), June 13, 2017.
"Fostering Integrity in Research" Committee Members, National Academies Report, April 11, 2017.
Privacy, Big Data, and the Public Good: Frameworks for Engagement, Lane, J., Stodden, V., Bender, S., and Nissenbaum, H. (eds). 2014.
"Reproducibility and replicability of rodent phenotyping in preclinical studies," Kafkafi et al, bioarxiv, 2016.
"Facilitating Reproducibility in Scientific Computing: Principles and Practice," David H. Bailey, Jonathan M. Borwein and Victoria Stodden, in Harald Atmanspacher and Sabine Maasen, eds, Reproducibility: Principles, Problems, Practices, John Wiley and Sons, New York, 2016.

References
External links
official page

Title: Kate Stohr
Kate Stohr is an American journalist, data scientist and civic activist based near San Francisco, CA. She was the director of a Data Science initiative at Simon & Schuster. She founded 99 Antennas. In 2016 she covered the U.S. Presidential Elections as a data journalist with Fusion  and noted for her coverage of the online dominance of the Trump candidacy and for her reporting on the racial inequality in US prosecutor elections  
In 1999 she co-founded Architecture for Humanity with Cameron Sinclair, a humanitarian architecture and design organization that focused on designing and building housing for people suffering from environmental disasters, refugee camps and other people in need. She was managing director of the organization until May 2013.
In 2006, Sinclair and Stohr published a compendium on socially conscious design, titled Design Like You Give A Damn: Architectural Responses to Humanitarian Crises (May 2006, Metropolis Books). In 2012 they released the follow-up, titled Design Like You Give A Damn [2]: Building Change From The Ground Up (May 2012, Abrams Books).
As a result of the 2006 TED Prize along with Sinclair, Stohr developed and launched the Open Architecture Network, the world's first open source community dedicated to improving living conditions through innovative and sustainable design. In 2012 the Open Architecture Network merged with Worldchanging to expand its work to both the built and natural environment.
In August 2008 Stohr was named as joint recipient of the Design Patron Award for the 2008 National Design Awards. In 2009 Stohr was awarded the Bicentenary Medal of the Royal Society of Arts for increasing people's resourcefulness.


== References ==

Title: Melissa Terras
Melissa Mhairi Terras  (born 1975) is a British scholar of Digital Humanities. Since 2017, she has been Professor of Digital Cultural Heritage at the University of Edinburgh, and director of its Centre for Digital Scholarship. She previously taught at University College London, where she was Professor of Digital Humanities and served as director of its Centre for Digital Humanities from 2012 to 2017: she remains an honorary professor. She has a wide ranging academic background: she has an undergraduate degree in art history and English literature, then took a Master of Science (MSc) degree in computer science, before undertaking a Doctor of Philosophy (DPhil) degree at the University of Oxford in engineering.

Early life and education
Terras was born in 1975 in Kirkcaldy, Scotland. She studied classical art history and English literature at the University of Glasgow. She was given the opportunity to hand in her dissertation as a website, and, after learning to code, did so in 1996. She graduated with an undergraduate Master of Arts (MA Hons) degree in 1997. She was then awarded a Scottish Government scholarship to undertake a master's degree in computer science. She studied software and systems at the University of Glasgow, graduating with a Master of Science (MSc) degree. Her master's thesis was supervised by Seamus Ross, and was a virtual reality model (in VRML 2.0) of the tomb of Sennedjem.
Terras then moved to the University of Oxford, where she had funding via the Engineering and Physical Sciences Research Council (EPSRC) to undertake a doctorate in engineering: she was based jointly in Oxford's Department of Engineering Science and the Centre for the Study of Ancient Documents. The project was concerned with using "image processing and artificial intelligence to help read Ancient documents" (specifically the Vindolanda tablets). Her thesis was titled "Image to interpretation: towards an intelligent system to aid historians in the reading of the Vindolanda texts", and was successfully submitted for her Doctor of Philosophy (DPhil) degree in 2002.

Academic career
In August 2003, Terras joined University College London (UCL) as a lecturer in electronic communication and publishing and was based in the School of Library, Archive, and Information Studies. In 2010, she was promoted to Reader in Electronic Communication. She co-founded the UCL Centre for Digital Humanities and served as its director from 2012 to 2017. Since leaving UCL, she has maintained her links with the university as an honorary professor.
In October 2017, Terras moved to the University of Edinburgh, having been appointed Professor of Digital Cultural Heritage in its School of History, Classics and Archaeology. She was also a Fellow of the Alan Turing Institute between 2018 and 2020.
Terras's research is focused on the intersection between computing and the humanities, particularly the use of computational techniques in arts and humanities research. Among the collaborations that she has been involved in is Transcribe Bentham, a crowdsourcing project in which volunteers help to transcribe the writings of Jeremy Bentham: the wiki went live in 2010. From 2015 to 2017, she was co-leader of a project analysing Egyptian mummy cases by non-destructive means (via digital imaging technology): the aim was to be able to read the papyrus that makes up a mummy's cartonnage.
On 20 July 2023, she was elected a Fellow of the Society of Antiquaries of London (FSA).
In 2024 she was elected a fellow of the Royal Academy of Engineering and she was appointed MBE in the 2025 New Year Honours for services to digital humanities.

Selected works
Terras, Melissa M. (2006). Image to interpretation: an intelligent system to aid historians in reading the Vindolanda texts. Oxford: Oxford University Press. ISBN 9780199204557.
Terras, Melissa M. (2008). Digital images for the information professional. Aldershot, England: Ashgate. ISBN 9780754648604.
Ross, C.; Terras, M.; Warwick, C.; Welsh, A. (8 March 2011). "Enabled backchannel: conference Twitter use by digital humanists" (PDF). Journal of Documentation. 67 (2): 214–237. doi:10.1108/00220411111109449.
Warwick, Claire; Terras, Melissa; Nyhan, Julianne, eds. (2012). Digital humanities in practice. London: Facet Publishing. ISBN 9781856047661.
Williams, Shirley A.; Terras, Melissa M.; Warwick, Claire (10 May 2013). "What do people study when they study Twitter? Classifying Twitter related academic papers" (PDF). Journal of Documentation. 69 (3): 384–410. doi:10.1108/JD-03-2012-0027. ISSN 0022-0418. S2CID 18581291.
Terras, Melissa; Nyhan, Julianne; Vanhoutte, Edward, eds. (2016). Defining digital humanities: a reader. Farnham, Surrey: Routledge. ISBN 9781317153580.


== References ==

Title: Joy A. Thomas
Joy Aloysius Thomas (1 January 1963 – 28 September 2020) was an Indian-born American information theorist, author and a senior data scientist at Google. He was known for his contributions to information theory and was the co-author of Elements of Information Theory, a popular text book which he co-authored with Thomas M. Cover. He also held a number of patents and was the founder of startups such as 'Insights One'.

Biography
Joy Thomas, born on January 1, 1963, in the south Indian state of Kerala, did his schooling at St Joseph's Boys' High School, Bangalore. He stood first in the IIT Joint Entrance Examination. After graduating from the Indian Institute of Technology, Madras, he migrated to the US where he continued his studies to secure a PhD from Stanford University in 1984. It was here that he met Thomas M. Cover, the renowned information theorist, and together they wrote a book in 1991, Elements of Information Theory, which is considered by many as a benchmark text book on the subject. In 1990, he joined the IBM Research at the Thomas J. Watson Research Center as a research staff member where he worked until he became involved in the founding of  Stratify, a startup founded in 1991, which was later rebranded as Iron Mountain Digital. Later, he founded InsightsOne, another startup which was subsequently acquired by Apigee in 2014.
Joy Thomas was an adjunct professor at Columbia University and Stanford University and held a number of patents. He died on September 28, 2020, at Mountain View, California, at the age of 57, survived by his wife, Priya, and children, Joshua and Leah.

Patents
"US Patent – API specification generation (Patent # 10,747,505 issued August 18, 2020) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.
"US Patent – Automatically extracting profile feature attribute data from event data (Patent # 10,255,300 issued April 9, 2019) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.
"US Patent – Method and apparatus for prediction of computer system performance based on types and numbers of active devices (Patent # 8,234,229 issued July 31, 2012) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.
"US Patent – Techniques for organizing data to support efficient review and analysis (Patent # 7,945,600 issued May 17, 2011) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.
"US Patent Application – Method and apparatus for prediction of computer system performance based on types and numbers of active devices (Application #20030023719 issued January 30, 2003) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.
"US Patent – Adaptive multiple dictionary data compression (Patent # 5,870,036 issued February 9, 1999) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.
"US Patent – Parallel compression and decompression using a cooperative dictionary (Patent # 5,729,228 issued March 17, 1998) – Justia Patents Search". patents.justia.com. Retrieved 5 October 2020.

Selected bibliography
Books
Cover, Thomas M.; Thomas, Joy A. (11 July 2006). Elements of Information Theory. Wiley. ISBN 978-0-471-74881-6.

Articles
Cover, Thomas M.; Thomas, A. (1988). "Determinant Inequalities via Information Theory". SIAM Journal on Matrix Analysis and Applications. 9 (3): 384–392. doi:10.1137/0609033. ISSN 0895-4798.
Dembo, A.; Cover, T.M.; Thomas, J.A. (1991). "Information theoretic inequalities". IEEE Transactions on Information Theory. 37 (6): 1501–1518. doi:10.1109/18.104312.

References
External links
"dblp: Joy A. Thomas". dblp.org. Retrieved 5 October 2020.
"On Amazon". www.amazon.in. Retrieved 5 October 2020.
"Joy Thomas's Tribute to Thomas Cover". ISL Stanford. YouTube. 10 July 2012. Retrieved 5 October 2020.

Title: Sherry Towers
Sherry Towers is an American and Canadian statistician and data scientist working as an independent consultant and an affiliate scholar with the Institute for Advanced Sustainability Studies in Potsdam, Germany following a seven year position as a faculty research associate at Arizona State University. Towers is perhaps best known for her study of the contagion effect of mass shootings. She is also the founder and owner of Towers Consulting LLC, a consulting company that provides visual analytics, data mining, applied statistics, and computational modeling services to industry, academia, and the public sectors.

Education
Towers earned a B.Sc in Physics at Simon Fraser University in Burnaby, Canada. She then earned a PhD in Physics at Carleton University in 2000. Her doctoral dissertation, A Study of decays of the tau lepton with charged kaons, was supervised by Dean Karlen. In 2010, Towers earned an M.S. in Applied Statistics from Purdue University in West Lafayette, IN.

Career and research
Towers worked as a research scientist at the State University of New York at Stony Brook, developing advanced machine learning techniques from 2000-2005. Following this position she started Towers Consulting LLC (2007) and worked as a postdoctoral research associate at Purdue University, modeling the spread of pandemic influenza. From 2012-2017 she served as a faculty research associate at the Simon A. Levin Mathematical, Computational and Modeling Sciences Center, at Arizona State University. She modeled the dynamics of social systems, including modeling the spread of panic in a population, developed predictive crime analytics, and studied attitudes toward gun control. She is best known for her work showing the contagious effects of mass shootings. She currently is an independent consultant and an affiliate scholar at the Institute for Advanced Sustainability Studies, and works on a variety of research topics, including election violence, the spread of political and partisan sentiments in a society, social media analytics, and the COVID-19 pandemic.

Selected publications
"Beyond Ebola: Lessons to mitigate future pandemics, 2015"
"Contagion in mass killings and school shootings, 2015"
"Estimate of the reproduction number of the 2015 Zika virus outbreak in Barranquilla, Colombia, and estimation of the relative role of sexual transmission, 2016"
"Quantifying the relative effects of environmental and direct transmission of norovirus, 2018"


== References ==

Title: Ben Warner
Ben Warner is a British data scientist.

Education
Warner earned a PhD at University College London for research investigating single molecule spintronics. The research was supervised by Cyrus Hirjibehedin and was awarded the Marshall Stoneham prize.

Career
Warner was a postdoctoral research fellow in quantum physics at the centre for nanotechnology at University College London. He left to join ASI Data Science (now Faculty), a company founded by his brother Marc Warner in 2014, where he is now a commercial principal.
Warner was a key figure in the computer modelling used by Vote Leave's successful 2016 referendum campaign. He was brought in by Dominic Cummings to run the Conservative Party's private computer model for the 2019 general election, which predicted that the Conservative Party would win 364 seats (they won 365).
Warner was a member of the Scientific Advisory Group for Emergencies (SAGE) during the COVID-19 pandemic, with Cummings and 21 others.


== References ==

Title: Amy Watt
Amy Watt (born December 16, 1997) is a Canadian-American former Paralympic athlete who mainly competed in the long jump in international level events. She was born missing her left arm just below the elbow due to amniotic band syndrome. Watt competed for the United States at the 2016 Paralympic Games in Rio de Janeiro. She competed in the 2020 Summer Paralympics in Long jump T47 for Canada, placing fifth. Watt decided to retire from international Paralympic competitions after Tokyo 2020.

Career
Watt holds United States and Canadian dual citizenship. She was born and raised in Palo Alto, California; and her father, Jeff, is from Canada. Watt competed at the 2015 Parapan American Games in Toronto, 2015 IPC World Championships in Doha, Qatar, and 2016 Summer Paralympics in Rio de Janeiro for the United States.
Watt competed at the 2019 Parapan American Games in Lima, Peru and 2019 IPC World Championships in Dubai for Canada.
Watt's first foray into elite paralympics competitions was at the 2015 US Paralympics Track and Field National Championships, where she placed first in long jump and 400m, second in 100m and 200m.
Watt attended Pomona College in Claremont, California. She took a couple of years break from international competitions to focus on her education before switching to compete for Canada in 2019. Watt graduated cum laude with a double major in mathematics and molecular biology and a minor in computer science in 2020. After graduation, Watt embarked on her professional career as a data scientist working for Meta. In 2023, she decided to pursue graduate studies full time at the University of Washington department of biostatistics.

References
External links
Amy Watt at Team USA (archived) 
Amy Watt at the Canadian Paralympic Committee
Amy Watt at the International Paralympic Committee

Title: Hadley Wickham
Hadley Alexander Wickham (born 14 October 1979) is a New Zealand statistician known for his work on open-source software for the R statistical programming environment. He is the chief scientist at Posit PBC and an adjunct professor of statistics at the University of Auckland, Stanford University, and Rice University. His work includes the data visualisation system ggplot2 and the tidyverse, a collection of R packages for data science based on the concept of tidy data.

Education and career
Wickham was born in Hamilton, New Zealand. He received a Bachelors degree in Human Biology and a master's degree in statistics at the University of Auckland in 1999–2004 and his PhD at Iowa State University in 2008 supervised by Di Cook and Heike Hofmann. He is the chief scientist at Posit PBC (formerly RStudio PBC) and an adjunct professor of statistics at the University of Auckland, Stanford University, and Rice University.
Wickham is a prominent and active member of the R user community and has developed several notable and widely used packages including ggplot2, plyr, dplyr and reshape2. Wickham's data analysis packages for R are collectively known as the tidyverse. According to Wickham's tidy data approach, each variable should be a column, each observation should be a row, and each type of observational unit should be a table.

Honors and awards
In 2006 he was awarded the John Chambers Award for Statistical Computing for his work developing tools for data reshaping and visualisation. Wickham was named a Fellow by the American Statistical Association in 2015 for "pivotal contributions to statistical practice through innovative and pioneering research in statistical graphics and computing". Wickham was awarded the international COPSS Presidents' Award in 2019 for "influential work in statistical computing, visualisation, graphics, and data analysis" including "making statistical thinking and computing accessible to a large audience".

Personal life
Wickham's sister Charlotte Wickham is also a statistician.

Publications
Wickham's publications include:

Wickham, Hadley; Grolemund, Garrett (2017). R for Data Science : Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, CA: O'Reilly Media. ISBN 978-1491910399. OCLC 968213225.
Wickham, Hadley (2015). R Packages. Sebastopol, CA: O'Reilly Media, Inc. ISBN 978-1491910597.
Wickham, Hadley (2014). Advanced R. New York: Chapman & Hall/CRC The R Series. ISBN 978-1466586963.
Wickham, Hadley (2011). "The split-apply-combine strategy for data analysis". Journal of Statistical Software. 40 (1): 1–29. doi:10.18637/jss.v040.i01.
Wickham, Hadley (2010). "A layered grammar of graphics". Journal of Computational and Graphical Statistics. 19 (1): 3–28. doi:10.1198/jcgs.2009.07098. S2CID 58971746.
Wickham, Hadley (2010). "stringr: modern, consistent string processing". The R Journal. 2 (2): 3–28. doi:10.32614/RJ-2010-012.
Wickham, Hadley (2009). ggplot2: Elegant Graphics for Data Analysis (Use R!). New York: Springer. ISBN 978-0387981406.
Wickham, Hadley (2007). "Reshaping data with the reshape package". Journal of Statistical Software. 21 (12): 1–20. doi:10.18637/jss.v021.i12.

References
External links
On the web
Bluesky
Twitter/X
GitHub
Interviews
Interview Archived 2023-02-01 at the Wayback Machine by Datascience.LA at UseR! 2014
Interview by Yixuan Qiu (2013)
Interview by Models are
Talks and presentations
Speaker Hadley Wickham Strata 2014 - O'Reilly Conferences, February 11 - 13, 2014, Santa Clara, CA Archived 2014-02-23 at the Wayback Machine
Interview at Strata 2014 Illuminating and Wrong
Ihaka Lecture Series 2017: Expressing yourself with R

Title: Roy David Williams
Roy David Williams is a physicist and data scientist. He is a professor at Caltech and is most known for his work with the LIGO, and VOTable and VOEvent standards. He is a proponent of open data.

Selected research
Fox, Geoffrey C., Roy D. Williams, and Paul C. Messina. Parallel computing works!. Elsevier, 2014.
Giavalisco, M., et al. "The great observatories origins deep survey: initial results from optical and near-infrared imaging." The Astrophysical Journal Letters 600.2 (2004): L93.
Williams, Roy D. "Performance of dynamic load balancing algorithms for unstructured mesh calculations." Concurrency: Practice and experience 3.5 (1991): 457-481.

References
External links
Roy David Williams publications indexed by Google Scholar

Title: Wong Ho-wa
Wong Ho-wa (Chinese: 黃浩華; Jyutping: Wong4 Hou6 Waa4, also known as Howa Wong) is a Hong Kong data scientist and pro-democracy activist. He led the open government data community g0vhk from 2016 to 2021. He was an Election Committee member representing the information technology (IT) industry from 2016 to 2021 and ran for the Information Technology constituency in the 2020 Hong Kong legislative election as part of the pro-democratic caucus.

Early life
Wong grew up in North District, Hong Kong and lived in a public housing estate. He attended Tung Wah Group of Hospitals Kap Yan Directors' College and Saint Francis of Assisi's College, and was a three-time medallist in the Hong Kong Olympiad in Informatics. His personal interest in democratic activism began when with the 500,000-strong demonstration against the National Security Bill 2003, in which he participated as a secondary school student.
He studied computer engineering at the Hong Kong University of Science and Technology and began his professional career in software engineering. He worked in Beijing in 2013-14. He relocated back to Hong Kong around the time of the Umbrella Movement protests and began taking part internet freedom advocacy alongside his data scientist career.

Open data activism
In 2016, Wong founded g0vhk on the model of g0v Taiwan, a technology advocacy group for open government data. He led the creation of a g0vhk open political data platform collating attendance, speech, and voting records of incumbent Hong Kong legislative councillors, and information about candidates in the 2016 general election. Later that year, Wong ran for election to the Election Committee as a member of the IT Vision slate, part of the Democracy 300+ caucus to increase the pro-democratic camp's influence in the 2017 Hong Kong Chief Executive election. IT Vision won all 30 seats in the Information Technology subsector due to the multiple non-transferable vote system.
Wong advocates that public bodies not only need to make content available to the general public, but also make them available in machine-readable formats. He criticized the complacency of civil servants who think they have complied with open data regulations but only post scan images of documents. He supports legislating a Public Records Act to codify the requirements for public bodies to disclose information, because the existing Code on Access to Information lacks enforceability.
Wong believes in an approach to open data that balances the right to know and the right to privacy. He criticized the Hong Kong public sector for using privacy concerns as an excuse to withhold data, exemplifying this issue with the abundance of rumours that stemmed from the Hong Kong Police Force's reluctance to publish data about tear gas use and about arrestees at protests. On the other hand, he also campaigned against the introduction of smart lamp-posts that have the hardware capability to record pedestrians' faces. Wong opined that, even if the Hong Kong government promises not to use facial recognition technology on the lamp-post footage, such footage would be a target of cyberattacks by malicious actors with access to facial recognition. During the 2019 Hong Kong protests, Wong represented the IT industry in a Citizen's Press Conference and responded to a question about vandalism of smart lamp-posts by protesters, saying that the general public was skeptical about the true purposes of the lamp-posts. He opined that technology innovation depends on trust, thus technology policy would be meaningless unless the government attempted to regain public trust by compromising on the Five Demands.
Ahead of the 2019 Hong Kong local elections, Wong led the g0vhk project Vote4.hk, which collated public data about candidates and compiled voter guides. The pro-democratic camp won a landslide victory in the elections, but Wong criticized that the district councils continued to lag behind the trend in terms of open data, citing the difficulty for the general public to obtain data about council budgets, and even where data were available often only scans of paper documents would be provided, making it difficult for citizens to monitor their council's performance.
In January 2020, Wong Ho Wa and Vote4.hk colleagues Brian Leung and Nandi Wong saw that public information about the Covid-19 pandemic in Hong Kong was disorganized, so they created the COVID-19 in HK dashboard to collate information about confirmed cases, disease transmission hotspots, and surgical mask market prices. The dashboard attracted 400,000 page views per day during the peak of the pandemic and was maintained by a team of some 20 volunteers assisted by automatic web crawlers. Wong Ho Wa said that the hardest part of maintaining the dashboard was finding committed volunteers to fact-check reports of unscrupulous mask merchants. Wong criticized the Centre for Health Protection's (CHP) practice of withholding data about new Covid-19 cases from their website for many hours after their daily press conference, which forced data reusers to watch CHP's press conferences in order to obtain the latest data. Nevertheless, Wong commended the CHP for providing an API for its coronavirus case data and that the Hong Kong government's open data practices had improved significantly in the late 2010s, even though Hong Kong still fell short of the Open Knowledge Foundation's standards, lagging behind nearby regions like Shanghai and Taiwan.

LegCo campaign
In July 2020, Charles Mok, the incumbent Information Technology functional constituency representative in Hong Kong Legislative Council (LegCo), announced that he would not seek re-election. In an interview, Mok said that he had been searching for a "Plan B" candidate in preparation for possible widespread disqualification of incumbent pro-democratic legislators, but in the process decided that he should retire from LegCo and endorse a younger candidate as his "Plan A" instead.
On 19 July, Wong Ho Wa declared his candidacy for the Information Technology constituency in the 2020 general election. In his manifesto, he said that he had been a longstanding supporter of liberal democracy. He stated that the Hong Kong national security law threatened internet freedom in Hong Kong, which was the lifeline of Hong Kong's IT industry, and that "the Great Firewall of China is now at our doorstep". He focused his campaign on using IT expertise to advocate for the freedom of information and for civic participation in government.
Commentators generally classify Wong as a moderate democrat. Wong and several other pro-democratic candidates for the functional constituencies signed the "confirmation form" pledging allegiance to the Hong Kong Basic Law, despite the form being a source of disagreement within the pro-democratic camp. Wong said he did not want to define himself in terms of any specific political faction, as he wanted to represent the broad range of political opinions within the IT industry. The 2020 general election was eventually postponed by the Hong Kong government, ostensibly due to the pandemic, and eventually replaced by the revamped 2021 Hong Kong legislative election and the 2021 Hong Kong Election Committee Subsector elections.
In January 2021, police raided Wong's home and office in Hong Kong amidst the mass arrests of participants of the pro-democracy legislative election primaries, but Wong was not arrested. The pro-democracy primaries did not include the Information Technology constituency in which Wong sought election to LegCo. In June, Wong announced that g0vhk was to be disbanded due to changes in the political climate.
The Information Technology functional constituency was abolished in the 2021 elections and replaced by a "Technology and Innovation" functional constituency whose franchise was limited to representatives of about 100 industry groups vetted by the government. Wong declined to run in the revamped elections, citing the restriction of voter franchise, and accordingly retired as an Election Committee member.

References
External links
GitHub: howawong
g0vhk
Wong Ho Wa 2020 official website

Title: Shirley Wu
Shirley Wu is a data scientist specialized in data art and data visualizations. She is a freelancer based out of San Francisco, California. With Nadieh Bremer, Wu is the author of Data Sketches.

Education
Shirley Wu graduated from Newbury Park High School in Newbury Park, California in 2008. Wu received her Business Administration degree at the University of California, Berkeley in 2012. In Fall 2021, she began a master's degree at the New York University Tisch School of the Arts's Interactive Telecommunications Program.

Career
In 2012, Wu worked as a software engineer at Splunk. Wu then worked as a software engineer on a frontend team at Illumio in 2015.
Since 2016, Wu has freelanced as a data visualization contractor and consultant. Wu writes, teaches, and speaks at conferences about her data art and visualization expertise, as well as providing courses on front-end web development, particularly D3. As a data visualization expert, she is a frequent speaker at conferences and guest on data visualization podcasts.
Wu has had an ongoing collaboration with Nadieh Bremer since they met in 2016 at the OpenVisConf in Boston. In late 2017, Wu and Bremer collaborated with The Guardian to enrich the field of journalism research in the project "Bussed Out: How America Moves its Homeless". This article's cartographic and visual works seamlessly accompanying its storytelling received various accolades and awards. The pair have collaborated with Google and Alberto Cairo on visualizations about popular travel locations and searches. Wu's work focused on the search terms entered from one country that were related to other countries. Bremer and Wu have also co-authored the book Data Sketches together. The book was first suggested by Tamara Munzner, who wanted the book to be part of her data visualization series. Munzner joined the project as its editor.
As a result of her work, Wu was featured in GitHub's ReadME Project, which "amplifies the voices of the open source community."

Notable works
Data Sketches (2021), coauthored with Nadieh Bremer, is a collection of 24 data visualizations.
An Interactive Visualization of Every Line in Hamilton, The Pudding (part of the Data Sketches series). An interactive data visualization of the dialogue in the musical Hamilton.
Explore Adventure, Google News Lab (part of the Data Sketches series). The visualization explores how Google searches differ between countries. In 2017, it received the Science & Technology award given by Information is Beautiful Awards.
People of the Pandemic: an interactive simulation game. This piece allows readers to localize a simulation of the COVID-19 pandemic, to make the impacts of decisions more intuitive.
Bussed Out: How America Moves its Homeless, The Guardian This piece won best data visualization from the North American Digital Media Awards, as well as Silver from the Malofiej Awards.

Awards
Best Data Visualization – North American Digital Media Awards (2018) for "Bussed Out: How America Moves its Homeless".
Silver in Features – Malofiej 26 (2018)  for "Bussed Out: How America Moves its Homeless".
Bronze in Art, Entertainment & Pop Culture – Information is Beautiful Awards (2017) for "An Interactive Visualization of Every Line in Hamilton".
Gold in Unusual – Information is Beautiful Awards (2017) for "Data Sketches in Twelve Installments".


== References ==

Title: Christopher Wylie
Christopher Wylie (born 19 June 1989) is a British-Canadian data consultant. He is noted as the whistleblower who released a cache of documents to The Guardian he obtained while he worked at Cambridge Analytica. This prompted the Facebook–Cambridge Analytica data scandal, which triggered multiple government investigations and raised wider concerns about privacy, the unchecked power of Big Tech, and Western democracy's vulnerability to disinformation. Wylie was included in Time magazine's 100 Most Influential People of 2018. He appeared in the 2019 documentary The Great Hack. He is the head of insight and emerging technologies at H&M.

Early life and education
Wylie was born to parents Kevin Wylie and Joan Carruthers, both physicians. He was raised in Victoria, British Columbia. As a child he was diagnosed with dyslexia and ADHD. After being abused at the age of 6 at the British Columbia Ministry of Education, which the school had tried to conceal, he sued. After a six-year legal battle, winning a settlement of $290,000 at the age of 14. The agency was also forced to overhaul its policies on inclusion and bullying. He left school in 2005 at the age of 16 without a qualification, and when asked about his "probable destiny" on his school leaver's yearbook page, he stated "just another dissociative smear merchant peddling backroom hackery in its purest Machiavellian form".
He taught himself to code at age 19.
In 2010, at the age of 20, he began studying law at the London School of Economics, graduating with a Bachelor of Laws in 2013, specialising in technology, media and IP law, and being awarded the Dechert Prize for Property Law.
Wylie has a PhD in predicting fashions trends from the University of the Arts London.

Career
2005–2012
After leaving school, Wylie moved to Ottawa, where he began volunteering for "a short stint" in the parliamentary office of his Member of Parliament, Keith Martin. During his time in Martin's office, he overlapped with Martin's executive assistant Jeff Silvester, who was later commissioned by Wylie to set up AggregateIQ. The following year, he got a job as a contractor in the office of the Canadian opposition leader, Michael Ignatieff, at the age of 19. During his contract, Wylie begun developing strategies on how to capitalize data harvested through social media for political gain. The party officials did not renew Wylie's contract in 2009, and a senior insider said it was largely because his ideas were seen as "too invasive." Of Wylie, the colleague said, "Let's say he had boundary issues on data even back then. He effectively pitched an earlier version of [the Cambridge Analytica data-harvesting operation] to us back in 2009 and we said, 'No.'"
In 2008, he volunteered on the presidential campaign of Barack Obama, learning about microtargeting from Obama campaign adviser Ken Strasma. There has been some dispute over whether his volunteer role was a senior or a junior-level data entry role.
In 2012, Wylie worked for the Liberal Democrats in the UK on voter targeting.

SCL Elections and Cambridge Analytica, 2013–14
In 2013, Wylie discovered some research on psychological profiling using social data funded by DARPA and used that knowledge when he began working for SCL Elections, formerly Strategic Communication Laboratories, and its offshoot for American elections (later renamed Cambridge Analytica), an international consultancy specialising in data-driven psychographic targeting in elections. Alexander Nix recruited Wylie for his small team in SCL and Wylie assembled the core of what would later become Cambridge Analytica, made up of psychologists and data scientists. His role at Cambridge Analytica was reported by the Parliament of the United Kingdom as its "director of research" and by Time as a "founder," but a Queen's Counsel (QC) report by Julian Malins disputed those titles, documenting that his employment contract stated he was hired as a part-time "intern" on a student visa, limited to 19 hours of work a week.
Wylie's role at SCL was first revealed in May 2017 by The Observer journalist Carole Cadwalladr, who wrote that "He’s the one who brought data and micro-targeting [individualised political messages] to Cambridge Analytica". He said that traditional analytics around voter profiling used voting records and purchase histories to predict voter behavior, but that it was useless for learning if a voter was "a neurotic introvert, a religious extrovert, a fair-minded liberal or a fan of the occult," which were among the traits Wylie and his team determined were uniquely susceptible to political messaging. Wylie found research done at Cambridge University which mapped Facebook likes to personality traits, research that was done by paying users to take a quiz and download an app that scraped private information from the profiles of the participants and their friends. Academic Aleksandr Kogan was commissioned by Wylie's team to build a similar application, which illegally scraped the personal data of 87 million people from their Facebook profiles, and the data was used to develop new forms of psychographic microtargeting. Facebook denied any knowledge of Kogan's program, citing that he had said he was "collecting information for academic purposes," and agreed that it would not be used for "commercial purposes." Kogan declined to comment on the matter, citing Non-disclosure agreements with both Facebook and Cambridge Analytica, beyond the statements that his application was "a very standard vanilla Facebook app," that Wylie's team assured him that their usage was legal, and that he did not personally profit from the work. Cambridge Analytica has repeatedly denied obtaining or using Facebook data, and further denied they influenced the outcome of the 2016 United States presidential election.
Wylie worked for American Republican candidates affiliated with the party's "Tea Party" wing in the 2014 United States elections; and on disinformation campaigns for political parties in Nigeria, Kenya, Ghana and the Caribbean.
Prior to his departure, Wylie had shared his profiling tool with Robert Mercer, the billionaire who funded Cambridge Analytica and later became one of Donald Trump's mega-donors, and with Steve Bannon, who effectively ran the company from 2014 onward and later went on to become Trump's campaign manager. Wylie's research work included message-testing work for Steve Bannon on building a wall on the American-Mexican border. He later recounted, "My ears perked up when I [later] started hearing some of these things like 'drain the swamp' or 'build the wall' or 'the deep state' because these were all narratives that had come out from the research that we were doing," and that the wall "is not really about stopping immigrants. It's to embody separation. If you can embody that separation and you can further distance in the minds of Americans us here in America and them elsewhere, even if it is just across a river, or just across a desert, then you have won that culture war."
Wylie has said he did not realize the "potential misuse" of his research at the time, referring to it as his "real failure," and to the work he did as, "political hackery." He said that if he had taken a job offer with Deloitte, Cambridge Analytica would not exist. He resigned in 2014.

Eunoia Technologies, 2014–17
In 2014, Wylie co-founded Eunoia Technologies along with former SCL/Cambridge Analytica senior staff Brent Clickard, Mark Gettleson and Tadas Jucikas. In describing his ambitions for developing Eunoia, Wylie stated "I want to build the NSA’s wet dream". 
Eunoia Technologies has been criticized for the similar psychographic profiling tactics used by Cambridge Analytica, using the same dataset shared by Kogan.
In December 2014, Wylie registered Eunoia Technologies Inc in the tax haven of Delaware. In May 2015, a wholly owned UK subsidiary of Eunoia was registered in the UK as Eunoia Technologies Ltd. The name "Eunoia" meant "beautiful thinking" in ancient Greek, and the company offered election-related consultancy services including "psychographic microtargeting", "multi-agent system voter behaviour simulation", and "data & communications management".
Wylie's lawyer subsequently assured journalists that Eunoia had no data, but parliamentary testimony from Kogan later revealed that Eunoia had possessed Kogan's full data set of 87 million Facebook users; and that SCL/Cambridge Analytica had only ever had access to some 4% of the scraped data, "in contrast with the contract with Mr. Wylie’s entity
Eunoia, where Eunoia received all of the page like data as well as dyads", and that unlike SCL/Cambridge Analytica, Wylie's company had been the only organization Kogan granted complete access to the dataset.
During the Easter of 2015, two of Wylie's Eunoia colleagues who had joined him from SCL, Tadas Jucikas and Mark Gettleson, flew to New York to meet Donald Trump's then-campaign manager on his 2016 presidential bid, Corey Lewandowski, for a meeting in a Central Park hotel. They pitched for Eunoia to work on the 2016 Trump presidential campaign, but were ultimately unsuccessful. The approach to the Trump campaign was made with Wylie's knowledge as CEO of Eunoia, and reportedly had his blessing.
In November 2015, Eunoia Technologies pitched Facebook data-mining techniques to the Liberal Party of Canada, securing a $100,000 contract in January 2016 for "a short-lived pilot project" with the Liberal Caucus Research Bureau. However, the contract was not renewed beyond the pilot.
From 2015, Wylie and Gettleson became embroiled in litigation with SCL, alleging that Eunoia had infringed SCL's intellectual property, had misappropriated SCL's data, had attempted to 'poach' other SCL contractors, and had attempted to 'poach' SCL's clients. Wylie elaborated on the Eunoia allegations when denying them: "They tried to sue me over their claims that I was somehow trying to steal their clients, or to somehow try to interfere with their contractual relations with other employees, or what have you." SCL later claimed that Eunoia had been "the subject of restraining undertakings to prevent the misuse of the company's intellectual property". A QC's report noted:

"On 21 May 2015, SCL discovered that Eunoia Technologies Limited had approached at least one of SCL’s existing clients in the USA, following confirmation from a US political client that they had received a proposal from Eunoia Technologies, which purported to deliver exactly the same services as SCL. Consequently, SCL’s lawyers wrote to Wylie and others at Eunoia Technologies Limited regarding suspected breaches of covenants on intellectual property, client solicitation, staff solicitation and non-competition."
Eunoia Technologies Ltd was voluntarily wound up in October 2017, with accounts running up to 31 May 2016.

2018–present
On 1 December 2018, Wylie was hired by H&M as its consulting director of research, eventually being promoted to its head of insight and emerging technologies.

EU Referendum campaign
Wylie has repeatedly denied having had any involvement in the 2016 United Kingdom European Union referendum, describing himself as  "A Eurosceptic, but I wouldn’t call myself a Brexiteer", and pointing to his having been abroad at the time of the referendum. He has conceded that most of the key personnel involved in the Vote Leave campaign's digital and campaign financing controversies were all friends or colleagues of his, including BeLeave founder Darren Grimes, BeLeave Treasurer Shahmir Sanni, AggregateIQ founders Zack Massingham and Jeff Silvester (a company set up at Wylie's instigation when he was SCL's director of research), and Vote Leave HQ staffer Mark Gettleson (who recruited AggregateIQ to work for Vote Leave), with Sanni and Gettleson later becoming witnesses in criminal investigations of the Vote Leave campaign.
It subsequently emerged that in January 2016, Wylie and Gettleson wrote a joint proposal, on behalf of Eunoia, to Vote Leave campaign Director Dominic Cummings, to pitch for a pilot providing microtargeting services to the Leave campaign in the 2016 United Kingdom European Union membership referendum. The pitch was ultimately unsuccessful, with Cummings later describing them as "charlatans". Gettleson subsequently admitted that they had only made the pitch to Vote Leave after first making a pitch to the opposing Remain campaign in November 2015.
It was subsequently reported that on the morning of the referendum result, Wylie posted the "Brexit butterfly" graphic to his social media in support of Brexit, along with the caption "We did it", tagging four individuals: Conservative MP Nigel Evans of the Vote Leave campaign board; BeLeave founder Darren Grimes, later found guilty by the Electoral Commission of using BeLeave as a vehicle for illegally breaking campaign spending limits; and Leave campaigners Luis Lopez and Shahmir Sanni, with the latter later becoming a "whistleblower" on Vote Leave's campaign finance breaches.

Whistleblowing
In March 2018, Wylie released a cache of documents to The Guardian centered around Cambridge Analytica's alleged unauthorized possession of personal private data from up to 87 million Facebook user accounts, which was obtained for the purpose of creating targeted digital advertising campaigns.  The campaigns were based on psychological and personality profiles mined from the Facebook data which Wylie had commissioned in a mass-data scraping exercise.
On the 18 March 2018, Wylie gave a series of detailed interviews to The Observer with revelations about his time at SCL/Cambridge Analytica, presenting himself as a "whistleblower". He subsequently provided testimony and materials to a range of inquiries and legislatures around the world, and his revelations were instrumental in the May 2018 collapse of Cambridge Analytica. Wylie admitted to having been the principal anonymous source for a May 2017 The Observer article by Carole Cadwalladr, which first drew attention to Cambridge Analytica. Cadwalladr subsequently related how she had tracked Wylie down via LinkedIn in early 2017, and after finding him "fascinating, funny and brilliant", had spent a year persuading him to go public with his allegations.
On March 27, 2018, Wylie provided evidence to the Digital, Culture, Media and Sport Committee of the UK Parliament that contained further revelations about the practices at Cambridge Analytica and its associated companies.
Wylie said that as a result of his whistleblowing, Facebook suspended his account, along with his accounts on the platforms its subsidiaries, Instagram and WhatsApp, although WhatsApp denied that any action was taken on Wylie's account.
Wylie authored a book about the subject matter: Wylie, Christopher (October 2019). Mindf*ck: inside Cambridge Analytica's plot to break the world. London, United Kingdom: Profile Books. ISBN 978-178816-506-8.

Personal life
Wylie is self-described as gay and vegan. He lives in London, England.
Wylie became a British citizen on 24 January 2024.

See also
Sophie Zhang
Frances Haugen
Brittany Kaiser

References
External links

Christopher Wylie on Twitter
Video interview with The Guardian, published 17 March 2018

Title: Yihui Xie
Yihui Xie (谢益辉) is a Chinese software developer who previously worked for Posit PBC. He is the principal author of the open-source software package Knitr for data analysis in the R programming language, and has also written the book Dynamic Documents with R and knitr.

Early life and education
Xie is a native of Yichang, Hubei, China.
Xie received a Bachelor of Economics in 2006 and a Master of Economics in 2009, both in statistics and from the Renmin University of China. He received a Doctor of Philosophy in statistics from Iowa State University in 2013. His doctoral advisors at Iowa State University were Di Cook and Heike Hofmann.

Career
Yihui Xie created the animation package in R which allows animation in graphics through R. He then authored the knitr package which makes reproducible research available from R. Between 2013 and 2023, he has been working with RStudio, the makers of an Integrated development environment (IDE) for the R programming language.
Xie was awarded the John M. Chambers statistical software award by American Statistical Association (ASA) in 2009 for the R package animation.

Publications
His publications include:

R Markdown Cookbook
R Markdown: The Definitive Guide
blogdown: Creating Websites with R Markdown
bookdown: Authoring Books and Technical Documents with R Markdown
Dynamic Documents with R and knitr
animation: An R Package for Creating Animations and Demonstrating Statistical Methods


== References ==

Title: Takeharu Yamanaka
Takeharu Yamanaka (山中 竹春, Yamanaka Takeharu, born 27 September 1972) is a Japanese politician and current mayor of Yokohama, the capital of Kanagawa Prefecture. He defeated incumbent mayor Fumiko Hayashi in the 2021 Yokohama mayoral election. His independent campaign was supported by the Constitutional Democratic Party, Social Democratic Party, and the Communist Party of Japan. Yamanaka's campaign focused on the opposition for a planned integrated resort development and casino for the city which was to be built on Yamashita Pier, criticism against the government's response to the COVID-19 pandemic, and additional sister city relationship with San Francisco, California.
Yamanaka formerly worked as a professor at Yokohama City University and as a data scientist.


== References ==

Title: Nathan Yau
Nathan Yau is an American statistician and data visualization expert.

Early life
Nathan Chun-Yin Yau grew up in Fresno, California.
He received a Bachelor of Science in electrical engineering and computer science from the University of California, Berkeley. He graduated in 2007 with a Master of Science and in 2013 with a PhD in statistics from the University of California, Los Angeles.
His dissertation was titled "An Online Tool for Personal Data Collection and Exploration" and focused on self-surveillance techniques. Yau's earlier self-surveillance work on the "Personal Environmental Impact Report" was featured in Yau's chapter of the book Beautiful Data, published in 2009.

Career
Yau is known for his blog FlowingData in which he publishes writing and tutorials on information design and analytics, as well as visualizations and data science-related projects created by other professionals.
He is the author of books "Visualize This: The FlowingData Guide to Design, Visualization, and Statistics" (2011) and "Data Points: Visualization That Means Something" (2013).
Since 2014, Yau has worked at the U.S. Census as a research mathematical statistician.

Selected publications
Yau, Nathan (2011). Visualize This (First ed.). John Wiley and Sons. ISBN 9780470944882. OCLC 729943780.
Yau, Nathan (2013). Data points : visualization that means something (First ed.). John Wiley and Sons. ISBN 9781118462195. OCLC 871319880.


== References ==

Title: Oliver Zahn
Oliver Zahn is US/German theoretical astrophysicist, data scientist, and entrepreneur, best known for developing algorithms for astrophysical data analysis and widely cited discoveries of phenomena in the history of the Universe. He is also known for his more recent work as founder and CEO of Climax Foods, a California-based biotechnology company modeling dairy and other animal products directly from plant ingredients. Prior to becoming an entrepreneur, Zahn directed UC Berkeley's Center for Cosmological Physics alongside George Smoot and Saul Perlmutter and was Head of Data Science at Google

Early life and education
Zahn was born in Munich and studied physics and philosophy at Ludwig Maximilian University of Munich, doing his Diploma thesis in theoretical astrophysics jointly at Max Planck Institute for Astrophysics and New York University, graduating summa cum laude. He went on to do his dissertation work in cosmology at Harvard University, before winning the inaugural prize fellowship at UC Berkeley's Center for Cosmological Physics, funded directly by the 2006 Nobel Prize in Physics.

Career
Industry career
In 2019, Oliver founded Climax Foods to replace animal foods with dairy and meat directly produced from plants, circumventing the animals' complex metabolisms and thereby reducing greenhouse gases and water use caused by animal agriculture. Climax aims to outcompete animal products by offering zero-compromise alternatives that are purely plant-based, yet indistinguishable in terms of taste and texture, and better than their animal-based competitors in terms of nutrition and price.

Academic career
Zahn has worked on a broad range of topics in theoretical, computational, and observational astrophysics and cosmology. Working with multiple multi-national collaborations, he has co-authored more than 100 peer-reviewed journal articles with more than 14,000 citations and a h-index of 68.
As an undergraduate at Max Planck Institute for Astrophysics, Zahn studied the early Universe and constrained deviations from the laws of gravity and electro-magnetism during the Big Bang.
While a doctoral student at Harvard University, Zahn and co-authors Smith and Dore detected, for the first time, gravitational lensing in the cosmic microwave background. The finding has since been confirmed by teams analyzing data from the Planck, Polarbear, and SPT telescopes 
In a separate series of papers  Zahn introduced statistical measures to use redshifted 21 cm radiation to study otherwise inaccessible periods of the Universe's structure formation. He invented a novel simulation framework to study galaxy formation in the cosmic web, yielding orders of magnitude performance gains compared to previous ray tracing frameworks, enabling exploration of much larger parameter spaces.
While leading analyses for the University of Chicago and University of California, Berkeley, based South Pole Telescope collaboration, Zahn and his team showed that the first galaxies formed more explosively than previously thought.


== References ==

Title: Sophie Zhang (whistleblower)
Sophie Zhang is an American data scientist and whistleblower who formerly worked at the Facebook Site Integrity fake engagement team, created to deal with bot accounts, often controlled by authoritarian governments' entities.
From 2018 to 2020, while investigating those fake engagements, Zhang uncovered abusive political manipulation and opposition harassment networks in 25 countries. She testified that Facebook negligence allowed those authoritarian regimes to manipulate public discourse. Zhang reported that most of these subversive networks use Facebook's organization pages, configured with human names and photographs to mimic human accounts in order to successfully evade Facebook's emerging efforts to counter fake users.
The British newspaper The Guardian dedicated a series, The Facebook Loophole, based on Zhang's resources and accounts, to report on these Facebook-based political manipulations. Following later whistleblower Frances Haugen's testimony on Facebook's impact on children, interest toward Zhang's testimony increased with investigation from European and American legislative bodies ongoing.

Career
Zhang worked at Facebook for two years as a data scientist until September 2020. She was in the "Fake Engagement" team, a sub-division of the "Spam team" assigned to look for abuses of the platform. Zhang investigated “fake engagement” such as inauthentic likes, comments, shares, and reactions. There was no Facebook team dedicated to investigating and rooting out these fake or abusive organization pages.

Unveiled government abuses
She found series of "multiple blatant attempts by foreign national governments to abuse our platform on vast scales to mislead their own citizenry, [which] caused international news on multiple occasions". Most notably, these included:

Honduras, 2018: President Juan Orlando Hernández - From June to July 2018, 78% of Hernández’s Facebook posts received likes that were not from real people, artificially boosting his apparent popular support by a factor of five. The social manager of Hernández's official Facebook pages, for both Hernández and his late sister who had served as communications minister, was directly controlling several hundreds of fake entities. This campaign used Facebook's Organization Pages, configured with human names and photographs, to add support and to lure unaware readers.
Azerbaijan, 2019: Zhang found the ruling party to be using thousands of Organization Pages to harass opposition parties. The network of pages was still active as of June 2021.
India, USA, Dominican Republic, Mexico, Honduras, El Salvador, Ecuador, Bolivia, Paraguay, Argentina, Italy, Poland, Ukraine, Albania, Bulgaria, Turkey, Iraq, Tunisia, Afghanistan, Mongolia, India, Myanmar, Indonesia, Philippines, South Korea and more.

Departure from Facebook
Zhang was fired from Facebook in September 2020. She declined a $64,000 severance package attached to a non-disparagement agreement restricting her ability to speak publicly about Facebook issues. On her departure day, she posted a 7,800-word departure message to Facebook's internal message board outlining Facebook’s failure to combat political manipulation campaigns similar to the Russian interference in the 2016 United States elections. Anticipating Facebook's deletion of the post, she created a personal, password-protected website with a copy of the post, then distributed its web address and password to Facebook co-workers.
Facebook suppressed the message on the internal board, then contacted Zhang's web hosting service and domain registrar to request and force her private website offline.

Facebook criticisms
Zhang argued that Facebook was not acting out of malice, but rather in slapdash, haphazard ways, concerned with self preservation and public relations. She pointed out several shortcomings in Facebook's management of such unauthentic political engagement on its services.

Facebook priority assessment. Structure and effort investments focuses on “large-scale” issues (ex: spam) rather that specific uncivic and political cases. Issues are prioritized by volume, so outright political manipulations in smaller countries are discounted despite their real impact. Zhang reported that 99% of resources are dedicated to fight spams. Expansion to fight political and election manipulations was rejected due to limitation of human resources.
Facebook has an "enforcement gap", with a dedicated team fighting fake accounts, but no team fighting fake organizations that can work like and mimic user accounts.
Facebook minimized bottom-up alerts by on-the-ground data analysts.
Facebook investment priorities and associated internal workload makes it impossible to properly manage political manipulations.
Facebook decision processes are generally haphazard, despite its effort to project itself as a fully competent entity.
Facebook has a Western-centric focus. Western occurrences of political manipulations are dealt with rapidly, while responses to occurrences in other countries are delayed or not responded to at all.
Facebook waits for and reacts to media coverage in a public relationship management approach. Facebook management has used negative media coverage to assess priorities, arguing that no previous media coverage means no relative importance of an issue.
Facebook acts with no public oversight. Low-level data analysts have control and censorship power over world leaders and public discourses, with the capacity to let political manipulation continue or to stop it. While at Facebook, Zhang had monitored and personally taken or not taken action in countless countries, some of which were later shaken by civil unrest. Zhang declared she therefore has "blood on [her] hands".

Reaction to Frances Haugen
Zhang has expressed support for another Facebook whistleblower, Frances Haugen, who shared internal company documents with The Wall Street Journal and on October 5, 2021, testified before the United States Senate Commerce Committee's Sub-Committee on Consumer Protection, Product Safety, and Data Security. On October 12, 2021, Zhang told CNN she has shared documents about Facebook with a United States law enforcement agency, and that she is available to testify before Congress and is encouraged by the bipartisan support for congressional action after Haugen's testimony.

British Parliament testimony
On October 14, 2021, it was announced Zhang would testify before the British Parliament on October 18, 2021, about "her work as a data scientist for the Facebook Site Integrity fake engagement team, dealing with bot accounts, often operated by government backed agencies in countries such as Russia," according to Damian Collins, the chair of the British Parliament Online Safety Bill committee.
On October 18, Zhang testified to the British Parliament. The hearing was via video conference and was mostly filled with members working on a bill to tackle harmful online contents. Zhang stated that:

Facebook observably neglects and allows disinformation campaigns on Facebook, to prioritize profits. This is consequential to Facebook, a company, structural mission to first and foremost make profit and protect itself.
Facebook observably allows authoritarian governments to manipulate political discourse.
Facebook observably was more reluctant to remove fake accounts if those accounts were connected to political leaders. This preferential treatment encourages major politicians to continue the violation and to do so openly.
Facebook observably let fake engagement run among developing countries' online communities. Facebook’s vice-President of Integrity Guy Rosen, stated that Facebook would focus its limited human resources on Western democracies and monitoring hostile actions from their immediate online threats such as Russia and Iran. Human resources dedicated to fighting Facebook-based manipulations differs considerably.
Facebook senior management observably shown indifference and deflection when facing the issue of political manipulations on Facebook.
Facebook senior management has conflict of interest, having to both keep good relations with political leaders and to set the rules for unacceptable violations on the platform.
Facebook-based political manipulations issues could be countered if adequate staffing and funding was allocated.
Zhang testimony was connected to Frances Haugen's recent testimony to the US congress, which mentioned Facebook-supported ethnic polarization and violence in Ethiopia.

Whistleblower advocacy
Zhang authored a piece "How to blow the whistle on Facebook – from someone who already did" in The Guardian, where she gives feedback and recommendations to potential Facebook's whistleblowers.

Personal life
Zhang is a transgender woman; she said she was "tired of being in the closet as a transgender woman" and that was a core aspect of her identity that informed her actions at Facebook and after she left.
In August 2022, Zhang filed a complaint to the University of Michigan alleging that her father, Youxue Zhang, a professor at the College of Literature, Science, and the Arts, engaged in abusive contact towards her on account of her gender identity. She stated that he was active in abusive conduct in two separate instances, once in 2004 as an adolescent, and once in 2010 while she was a student at the University of Michigan. She argued that his actions violated university policy on sexual and gender-based misconduct. The complaint was dismissed by the university, with the case coordinator reasoning that the conduct alleged in the complaint did not occur in a university program or activity.

See also
Christopher Wylie
Criticism of Facebook
Facebook Files
Frances Haugen
Instagram's impact on people
Russian interference in the 2016 United States elections
Edward Snowden

References
External links
"Facebook accusé par une ancienne employée d'avoir ignoré des tentatives de manipulations politiques dans plusieurs pays". Le Monde.fr (in French). 2020-09-15. Retrieved 2021-04-15.
""I Have Blood On My Hands": A Whistleblower Says Facebook Ignored Global Political Manipulation". BuzzFeed News. 14 September 2020. Retrieved 2021-04-15.

